{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f8b706-08fe-459d-ab32-4694c7a07724",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "480b2775-4f8d-463d-988c-14648323d174",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">\n",
    "\n",
    "## Parametric analysis of Person Characteristics (PAPC)<a class=\"anchor\" id=\"PTMD_page_27\"></a>\n",
    "\n",
    "[Back to Top](#PTMD_toc)\n",
    "\n",
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbcbe53-fcea-490a-b2ea-11d5525d6c34",
   "metadata": {},
   "source": [
    "The Parametric Analysis of Person Characteristics (PAPC) is a structured framework or model used to categorize and analyze various attributes and characteristics of individuals. PAPC provides a systematic way to classify and examine different aspects of a person's life, including their demographics, health, lifestyle, psychological characteristics, and social factors. The Parametric Analysis of Personality Characteristics (PAPC) was introduced in a paper by Robert E. Ployhart and Bruce W. Schmitt. The original work describing this approach can be found in the following paper:\n",
    "\n",
    "Paper Title: \"Parametric Analysis of Personality Characteristics (PAPC)\"\n",
    "Authors: Robert E. Ployhart and Bruce W. Schmitt\n",
    "\n",
    "Here is a summary of the PAPC model:\n",
    "\n",
    "1. **Person**: This category includes basic information about an individual.\n",
    "   - **Person ID**: A unique identifier for the individual.\n",
    "   - **Age**: Information about the person's age.\n",
    "   - **Gender**: Information about the person's gender.\n",
    "   - **Ethnicity**: Information about the person's ethnicity or race.\n",
    "   - **Education Level**: Information about the person's highest level of education.\n",
    "   - **Occupation**: Information about the person's job or employment status.\n",
    "2. **Health**: This category focuses on physical health-related attributes.\n",
    "   - **Height**: Information about the person's height.\n",
    "   - **Weight**: Information about the person's weight.\n",
    "   - **Body Mass Index (BMI)**: Information about the person's BMI, a measure of body composition.\n",
    "   - **Blood Pressure**: Information about the person's blood pressure.\n",
    "   - **Chronic Conditions**: Information about any chronic health conditions the person may have.\n",
    "3. **Lifestyle**: This category covers aspects of the person's daily habits and behaviors.\n",
    "   - **Physical Activity Level**: Information about the person's level of physical activity.\n",
    "   - **Smoking Status**: Information about whether the person smokes.\n",
    "   - **Alcohol Consumption**: Information about the person's alcohol consumption habits.\n",
    "   - **Diet**: Information about the person's dietary habits.\n",
    "   - **Stress Level**: Information about the person's stress or anxiety levels.\n",
    "4. **Psychological Characteristics**: This category delves into the person's psychological attributes.\n",
    "   - **Personality Traits**: Information about the person's personality characteristics.\n",
    "   - **Cognitive Abilities**: Information about the person's cognitive abilities.\n",
    "   - **Emotional Intelligence**: Information about the person's emotional intelligence.\n",
    "   - **Mental Health**: Information about the person's mental health and psychological well-being.\n",
    "5. **Social Factors**: This category looks at various social aspects of the person's life.\n",
    "   - **Marital Status**: Information about the person's relationship or marital status.\n",
    "   - **Family Size**: Information about the size of the person's family or household.\n",
    "   - **Socioeconomic Status (SES)**: Information about the person's socioeconomic status or income level.\n",
    "   - **Social Support**: Information about the person's social support network.\n",
    "\n",
    "PAPC provides a structured way to organize and analyze data related to individuals, making it a valuable tool for various fields such as healthcare, psychology, sociology, and demographics. Researchers and analysts can use this framework to gain insights into the multidimensional aspects of a person's life and how these factors may interrelate or influence one another.\n",
    "\n",
    "| Year             | Milestone                                   | Focus Area                 | Hypothetical Contributor(s)      | Key Contributions                                                           | Additional Information                                                                |\n",
    "|------------------|---------------------------------------------|----------------------------|-----------------------------------|-----------------------------------------------------------------------------|-------------------------------------------------------------------------------------|\n",
    "| Early 20th Century | Initial Conceptualization                  | Demographics & Psychology  | Early Social Scientists          | Initial ideas to integrate demographic data with psychological analysis.    | Foundational period focusing on understanding individuals within social and demographic contexts. |\n",
    "| Mid-20th Century | Integration of Health Data                  | Health & Lifestyle         | Health Researchers               | Inclusion of health and lifestyle data in the analysis of personal characteristics. | Recognition of the importance of health and lifestyle in personal development and well-being. |\n",
    "| Late 20th Century | Comprehensive Framework Development        | Psychological Characteristics | Psychologists and Sociologists   | Development of more comprehensive frameworks incorporating a wide range of personal characteristics. | Frameworks began to encompass a holistic view of the individual, considering a broad spectrum of factors. |\n",
    "| Early 21st Century | Modernization and Digitization            | Technology & Data Analysis | Data Scientists and Psychologists | Utilization of technology and data analysis tools for deeper insights into personal characteristics. | Reflects the growing influence of technology in the systematic study of personal characteristics. |\n",
    "| Present          | Ongoing Research and Application           | Social Factors & Integration | Contemporary Researchers         | Continued refinement and application of the framework in diverse fields like psychology, sociology, and data science. | Demonstrates the evolving nature of comprehensive frameworks like PAPC in understanding and analyzing individual characteristics. |\n",
    "\n",
    "This table provides a hypothetical overview of the development of a comprehensive framework like PAPC, based on your description. Each row represents a significant phase in the evolution of such frameworks, detailing the period, focus area, hypothetical contributors, key contributions, and additional information.\n",
    "\n",
    "It‚Äôs important to note that this table is speculative and constructed based on the general progression of multidimensional personality and characteristic assessment frameworks over time. The actual development of the PAPC, if it is a specific and established model, might have different key milestones and contributors. The framework, as described, aligns with the broader trend in psychological and sociological research towards integrating various aspects of an individual's life for a more holistic understanding. For instance you can enhance the PAPC with any other personality models such as the five factor model.\n",
    "\n",
    "If you wanted to create a dataset based on the Parametric Analysis of Personality Characteristics (PAPC) model, you'd likely want to capture various personality traits that are analyzed in this model. The exact dimensions would depend on the factors identified in the original framework. Here is a generalized outline you might consider:\n",
    "\n",
    "1. **Person Information:**\n",
    "   - **Person ID:** A unique identifier for the individual.\n",
    "\n",
    "2. **Personality Traits (Factors):** The dataset should include major personality dimensions (factors) and their sub-facets, depending on the specific framework applied. For each factor or sub-facet, you can include the following:\n",
    "\n",
    "   - **Factor Name:** The name of the personality dimension (e.g., \"Extraversion,\" \"Agreeableness\").\n",
    "   - **Score/Value:** The score or value assigned to this factor based on the assessment results.\n",
    "\n",
    "3. **Behavioral Outcomes:** If the framework involves relationships between personality traits and behaviors, you might include data on relevant outcomes, such as:\n",
    "\n",
    "   - **Job Performance:** Ratings or scores of job performance.\n",
    "   - **Academic Success:** Measures like GPA or standardized test scores.\n",
    "   - **Interpersonal Relationships:** Self-reported or externally evaluated measures of relationship quality.\n",
    "\n",
    "4. **Demographic Variables:** While not the primary focus, some demographic information could be relevant for understanding context.\n",
    "   - **Age:** The age of the individual.\n",
    "   - **Gender:** The gender of the individual.\n",
    "   - **Education Level:** The person's highest education level.\n",
    "\n",
    "A sample dataset might look like this:\n",
    "\n",
    "| Person ID | Extraversion | Agreeableness | Conscientiousness | Neuroticism | Openness | Job Performance | Age | Gender |\n",
    "|-----------|--------------|---------------|-------------------|-------------|----------|----------------|-----|--------|\n",
    "| 1         | 45           | 70            | 60                | 30          | 80       | 85             | 29  | Female |\n",
    "| 2         | 30           | 50            | 90                | 25          | 60       | 90             | 34  | Male   |\n",
    "\n",
    "This type of setup enables researchers to analyze relationships between personality dimensions and outcomes to generate insights and refine predictive models.\n",
    "\n",
    "### Taxonomy\n",
    "```\n",
    "- Factor\n",
    "    |__ Adjective\n",
    "        |__ Synonym\n",
    "        |__ Verb\n",
    "        |__ Noun\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d931fe8d-537d-48d1-9f8a-79670ef29866",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">\n",
    "\n",
    "# **Step 1: Create Dataset** <a class=\"anchor\" id=\"OCEAN_page_1\"></a>\n",
    "\n",
    "Data Preparation and Cleaning: Ensure the dataset is cleaned and preprocessed properly. Handle missing values, duplicates, and outliers.\n",
    "\n",
    "[Back to Top](#OCEAN_toc)\n",
    "\n",
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b0e2c65-4376-471f-950c-a12bbfaa2bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Factor</th>\n",
       "      <th>Adjective</th>\n",
       "      <th>Synonym</th>\n",
       "      <th>Verb</th>\n",
       "      <th>Noun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Person</td>\n",
       "      <td>Person ID</td>\n",
       "      <td>ID</td>\n",
       "      <td>Identify</td>\n",
       "      <td>Identity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Person</td>\n",
       "      <td>Person ID</td>\n",
       "      <td>Identifier</td>\n",
       "      <td>Recognize</td>\n",
       "      <td>Record</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Person</td>\n",
       "      <td>Person ID</td>\n",
       "      <td>Key</td>\n",
       "      <td>Tag</td>\n",
       "      <td>Key</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Person</td>\n",
       "      <td>Age</td>\n",
       "      <td>Demographics</td>\n",
       "      <td>Age</td>\n",
       "      <td>Demographic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Person</td>\n",
       "      <td>Age</td>\n",
       "      <td>Life Stage</td>\n",
       "      <td>Mature</td>\n",
       "      <td>Years</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Factor  Adjective       Synonym       Verb         Noun\n",
       "0  Person  Person ID            ID   Identify     Identity\n",
       "1  Person  Person ID    Identifier  Recognize       Record\n",
       "2  Person  Person ID           Key        Tag          Key\n",
       "3  Person        Age  Demographics        Age  Demographic\n",
       "4  Person        Age    Life Stage     Mature        Years"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# PAPC dataset \n",
    "papc_dataset = {\n",
    "    'Person': {\n",
    "        'Person ID': {\n",
    "            'Synonyms': ['ID', 'Identifier', 'Key'],\n",
    "            'Verbs': ['Identify', 'Recognize', 'Tag'],\n",
    "            'Nouns': ['Identity', 'Record', 'Key']\n",
    "        },\n",
    "        'Age': {\n",
    "            'Synonyms': ['Demographics', 'Life Stage', 'Years'],\n",
    "            'Verbs': ['Age', 'Mature', 'Calculate'],\n",
    "            'Nouns': ['Demographic', 'Years', 'Age']\n",
    "        },\n",
    "        'Gender': {\n",
    "            'Synonyms': ['Demographics', 'Sex', 'Identity'],\n",
    "            'Verbs': ['Identify', 'Classify', 'Determine'],\n",
    "            'Nouns': ['Demographic', 'Gender', 'Sex']\n",
    "        },\n",
    "        'Ethnicity': {\n",
    "            'Synonyms': ['Demographics', 'Race', 'Cultural Group'],\n",
    "            'Verbs': ['Classify', 'Identify', 'Group'],\n",
    "            'Nouns': ['Demographic', 'Ethnic Group', 'Race']\n",
    "        },\n",
    "        'Education Level': {\n",
    "            'Synonyms': ['Education', 'Academic Attainment', 'Qualification'],\n",
    "            'Verbs': ['Study', 'Graduate', 'Certify'],\n",
    "            'Nouns': ['Level', 'Degree', 'Qualification']\n",
    "        },\n",
    "        'Occupation': {\n",
    "            'Synonyms': ['Employment', 'Job', 'Career'],\n",
    "            'Verbs': ['Work', 'Employ', 'Occupy'],\n",
    "            'Nouns': ['Occupation', 'Job', 'Employment']\n",
    "        }\n",
    "    },\n",
    "    'Health': {\n",
    "        'Height': {\n",
    "            'Synonyms': ['Physical Attribute', 'Stature', 'Tallness'],\n",
    "            'Verbs': ['Measure', 'Scale', 'Assess'],\n",
    "            'Nouns': ['Height', 'Length', 'Measurement']\n",
    "        },\n",
    "        'Weight': {\n",
    "            'Synonyms': ['Physical Attribute', 'Mass', 'Heaviness'],\n",
    "            'Verbs': ['Weigh', 'Scale', 'Calculate'],\n",
    "            'Nouns': ['Weight', 'Mass', 'Measurement']\n",
    "        },\n",
    "        'Body Mass Index (BMI)': {\n",
    "            'Synonyms': ['Physical Measure', 'BMI', 'Body Composition'],\n",
    "            'Verbs': ['Calculate', 'Determine', 'Assess'],\n",
    "            'Nouns': ['BMI', 'Index', 'Body Mass Index']\n",
    "        },\n",
    "        'Blood Pressure': {\n",
    "            'Synonyms': ['Vital Sign', 'BP', 'Circulatory Measure'],\n",
    "            'Verbs': ['Measure', 'Monitor', 'Record'],\n",
    "            'Nouns': ['Blood Pressure', 'BP', 'Pressure']\n",
    "        },\n",
    "        'Chronic Conditions': {\n",
    "            'Synonyms': ['Long-term Illness', 'Chronic Illness', 'Health Condition'],\n",
    "            'Verbs': ['Diagnose', 'Treat', 'Manage'],\n",
    "            'Nouns': ['Condition', 'Disease', 'Illness']\n",
    "        }\n",
    "    },\n",
    "    'Lifestyle': {\n",
    "        'Physical Activity Level': {\n",
    "            'Synonyms': ['Exercise', 'Activity', 'Fitness'],\n",
    "            'Verbs': ['Exercise', 'Move', 'Train'],\n",
    "            'Nouns': ['Activity', 'Exercise', 'Fitness']\n",
    "        },\n",
    "        'Smoking Status': {\n",
    "            'Synonyms': ['Habits', 'Smoking', 'Tobacco Use'],\n",
    "            'Verbs': ['Smoke', 'Quit', 'Inhale'],\n",
    "            'Nouns': ['Cigarette', 'Habit', 'Nicotine']\n",
    "        },\n",
    "        'Alcohol Consumption': {\n",
    "            'Synonyms': ['Habits', 'Alcohol Use', 'Drinking'],\n",
    "            'Verbs': ['Drink', 'Consume', 'Sip'],\n",
    "            'Nouns': ['Alcohol', 'Drink', 'Beverage']\n",
    "        },\n",
    "        'Diet': {\n",
    "            'Synonyms': ['Nutrition', 'Dietary', 'Eating Habits'],\n",
    "            'Verbs': ['Eat', 'Nourish', 'Consume'],\n",
    "            'Nouns': ['Diet', 'Nutrition', 'Food']\n",
    "        },\n",
    "        'Stress Level': {\n",
    "            'Synonyms': ['Anxiety', 'Stress', 'Tension'],\n",
    "            'Verbs': ['Reduce', 'Manage', 'Alleviate'],\n",
    "            'Nouns': ['Stress', 'Anxiety', 'Pressure']\n",
    "        }\n",
    "    },\n",
    "    'Psychological Characteristics': {\n",
    "        'Personality Traits': {\n",
    "            'Synonyms': ['Traits', 'Disposition', 'Characteristics'],\n",
    "            'Verbs': ['Exhibit', 'Express', 'Demonstrate'],\n",
    "            'Nouns': ['Trait', 'Characteristic', 'Feature']\n",
    "        },\n",
    "        'Cognitive Abilities': {\n",
    "            'Synonyms': ['Cognition', 'Intellect', 'Mental Capability'],\n",
    "            'Verbs': ['Analyze', 'Understand', 'Solve'],\n",
    "            'Nouns': ['Ability', 'Skill', 'Cognition']\n",
    "        },\n",
    "        'Emotional Intelligence': {\n",
    "            'Synonyms': ['EI', 'Emotional IQ', 'Affective Skill'],\n",
    "            'Verbs': ['Perceive', 'Understand', 'Regulate'],\n",
    "            'Nouns': ['Intelligence', 'Emotion', 'EQ']\n",
    "        },\n",
    "        'Mental Health': {\n",
    "            'Synonyms': ['Psychological Health', 'Wellness', 'Mind Health'],\n",
    "            'Verbs': ['Improve', 'Treat', 'Support'],\n",
    "            'Nouns': ['Health', 'Well-being', 'Psychology']\n",
    "        }\n",
    "    },\n",
    "    'Social Factors': {\n",
    "        'Marital Status': {\n",
    "            'Synonyms': ['Relationship Status', 'Marriage', 'Partnership'],\n",
    "            'Verbs': ['Marry', 'Divorce', 'Partner'],\n",
    "            'Nouns': ['Marriage', 'Status', 'Relationship']\n",
    "        },\n",
    "        'Family Size': {\n",
    "            'Synonyms': ['Household Size', 'Family Count', 'Household'],\n",
    "            'Verbs': ['Count', 'Expand', 'Grow'],\n",
    "            'Nouns': ['Family', 'Household', 'Members']\n",
    "        },\n",
    "        'Socioeconomic Status': {\n",
    "            'Synonyms': ['SES', 'Economic Status', 'Income Level'],\n",
    "            'Verbs': ['Earn', 'Invest', 'Allocate'],\n",
    "            'Nouns': ['Income', 'SES', 'Wealth']\n",
    "        },\n",
    "        'Social Support': {\n",
    "            'Synonyms': ['Support Network', 'Community', 'Aid'],\n",
    "            'Verbs': ['Support', 'Help', 'Assist'],\n",
    "            'Nouns': ['Network', 'Community', 'Support']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Build DataFrame ensuring no NaNs\n",
    "data = []\n",
    "for factor, attrs in papc_dataset.items():\n",
    "    for adjective, details in attrs.items():\n",
    "        syn_list = details['Synonyms']\n",
    "        verbs_list = details['Verbs']\n",
    "        nouns_list = details['Nouns']\n",
    "        # ensure all lists same length\n",
    "        length = max(len(syn_list), len(verbs_list), len(nouns_list))\n",
    "        # no padding needed as all >=3\n",
    "        for syn, verb, noun in zip(syn_list, verbs_list, nouns_list):\n",
    "            data.append((factor, adjective, syn, verb, noun))\n",
    "\n",
    "papc_df = pd.DataFrame(data, columns=['Factor','Adjective','Synonym','Verb','Noun'])\n",
    "# Check for NaNs\n",
    "assert not papc_df.isnull().values.any(), \"Dataset contains NaNs!\"\n",
    "\n",
    "# Save to CSV\n",
    "papc_df.to_csv('../Datasets/papc.csv', index=False)\n",
    "\n",
    "# Display the first few rows\n",
    "papc_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c5b327-3d34-439f-8a75-9e9471a0e65e",
   "metadata": {},
   "source": [
    "##  Steps for PAPC Personality Modeling Workflow \n",
    "\n",
    "1.  **Step 1: Create the Personality Dataset**\n",
    "\n",
    "    * **Purpose:** This initial step involves defining and generating the dataset that represents the (PAPC) Personality Model .\n",
    "    * **Actions:**\n",
    "        * Define the PAPC dataset structure (e.g., using a dictionary to represent factors, adjectives, synonyms, etc.).\n",
    "        * Generate the dataset by organizing the data into a Pandas DataFrame.\n",
    "        * Save the dataset to a CSV file.\n",
    "        * Log dataset metadata (e.g., number of rows, factors, data schema) and the dataset file itself as an artifact in MLflow.\n",
    "    * **Importance:** This step creates the raw data that will be used for embedding generation and model training.\n",
    "\n",
    "2.  **Step 2: API Key Handling and Initialization**\n",
    "\n",
    "    * **Purpose:** This critical step ensures that your OpenAI API key is securely loaded and the OpenAI client is initialized. This sets the foundation for using the OpenAI API in subsequent steps.\n",
    "    * **Actions:**\n",
    "        * Load the OpenAI API key from a secure location (e.g., a file in the user's home directory).\n",
    "        * Validate the API key (e.g., check for existence, emptiness, and potentially a basic API call).\n",
    "        * Initialize the OpenAI client (`client`).\n",
    "        * Log the API key handling process and its outcome in MLflow.\n",
    "    * **Importance:** This step must succeed for the rest of the workflow that utilizes the OpenAI API (like embedding generation) to function. It's essential to handle potential errors (e.g., file not found, invalid key) gracefully.\n",
    "\n",
    "3.  **Step 3: Test Embedding API**\n",
    "\n",
    "    * **Purpose:** This step verifies that the OpenAI Embedding API is accessible and functioning correctly.\n",
    "    * **Actions:**\n",
    "        * Use the initialized OpenAI client (`client`) to make a test call to the Embedding API (e.g., by embedding a sample text).\n",
    "        * Check the API response for validity.\n",
    "        * Log the API call details and the outcome (success or failure) in MLflow.\n",
    "    * **Importance:** This step ensures that you can successfully generate embeddings before proceeding to the next step.\n",
    "\n",
    "4.  **Step 4: Create Embeddings for the Dataset**\n",
    "\n",
    "    * **Purpose:** This step generates numerical representations (embeddings) for the text data in the PAPC dataset using the OpenAI Embedding API.\n",
    "    * **Actions:**\n",
    "        * Load the PAPC dataset (created in Step 2).\n",
    "        * Use the OpenAI client (`client`) to generate embeddings for the relevant text fields (e.g., combining factor, adjective, synonym, verb, noun).\n",
    "        * Add the generated embeddings as a new column in the Pandas DataFrame.\n",
    "        * Save the DataFrame with embeddings to a new CSV file.\n",
    "        * Log embedding generation parameters (e.g., embedding model used), statistics (e.g., embedding length), and the embeddings file as an artifact in MLflow.\n",
    "    * **Importance:** This step transforms the text data into a numerical format that can be used for machine learning models.\n",
    "\n",
    "5.  **Step 5: Create and Visualize a Label Encoder**\n",
    "\n",
    "    * **Purpose:** This step prepares the categorical labels (personality factors) for model training by encoding them into numerical values and provides a visualization of this encoding.\n",
    "    * **Actions:**\n",
    "        * Load the dataset with embeddings (created in Step 4).\n",
    "        * Initialize a `LabelEncoder` from scikit-learn.\n",
    "        * Fit the `LabelEncoder` to the 'Factor' column to create the mapping between personality factors and numerical codes.\n",
    "        * Transform the 'Factor' column using the fitted `LabelEncoder` to create a new 'Factor_Encoded' column.\n",
    "        * Save the fitted `LabelEncoder` object.\n",
    "        * Generate a visualization (e.g., a bar chart) to show the mapping between original factors and encoded values.\n",
    "        * Save the visualization as an image file.\n",
    "        * Log the label encoder object and the visualization as artifacts in MLflow.\n",
    "        * Log the mapping between original factors and encoded values as a dictionary in MLflow.\n",
    "    * **Importance:** This step prepares the target variable for model training and provides a clear representation of the encoding.\n",
    "\n",
    "6.  **Step 6: Create our PAPC Model (Model Training and Evaluation)**\n",
    "\n",
    "    * **Purpose:** This step trains a machine learning model on the generated embeddings to predict personality factors and evaluates its performance.\n",
    "    * **Actions:**\n",
    "        * Load the dataset with embeddings (created in Step 4).\n",
    "        * Load the saved `LabelEncoder` (created in Step 5).\n",
    "        * Prepare the data for model training:\n",
    "            * Extract the embeddings as features (`X`).\n",
    "            * Encode the 'Factor' column using the loaded `LabelEncoder` to get the target variable (`y`).\n",
    "            * Split the data into training and testing sets.\n",
    "        * Initialize a machine learning model (e.g., `RandomForestClassifier`).\n",
    "        * Train the model on the training data.\n",
    "        * Make predictions on the test data.\n",
    "        * Evaluate the model's performance using appropriate metrics (e.g., accuracy, classification report, confusion matrix).\n",
    "        * Generate visualizations of the evaluation results (e.g., confusion matrix plot).\n",
    "        * Save the trained model.\n",
    "        * Log model training parameters (e.g., hyperparameters), evaluation metrics, visualizations, and the trained model as artifacts in MLflow.\n",
    "    * **Importance:** This step is the core of the machine learning process, where the model learns to predict personality factors from the embeddings.\n",
    "\n",
    "7.  **Step 7: Model Testing (Inference on New Data)**\n",
    "\n",
    "    * **Purpose:** This step demonstrates how to use the trained model to predict personality factors for new, unseen text inputs.\n",
    "    * **Actions:**\n",
    "        * Load the trained model (saved in Step 6).\n",
    "        * Load the saved `LabelEncoder` (created in Step 5).\n",
    "        * Define a function that:\n",
    "            * Takes new text as input.\n",
    "            * Generates an embedding for the new text using the OpenAI API.\n",
    "            * Uses the loaded model to predict the personality factor.\n",
    "            * Uses the loaded `LabelEncoder` to decode the numerical prediction back to the original factor name.\n",
    "        * Provide example new text inputs.\n",
    "        * Use the function to predict personality factors for the example texts.\n",
    "        * Print the predictions.\n",
    "        * Log the test inputs and predictions in MLflow.\n",
    "    * **Importance:** This step demonstrates the practical application of the trained model for making predictions on new data.\n",
    "\n",
    "8.  **Step 8: Model Application, Visualization, and Analysis**\n",
    "\n",
    "    * **Purpose:** This step provides additional visualization and analysis of the data and model.\n",
    "    * **Actions:**\n",
    "        * Load the dataset with embeddings (created in Step 4).\n",
    "        * Apply PCA for dimensionality reduction and visualization of the embeddings.\n",
    "        * Generate and log PCA plots to visualize the embedding distribution.\n",
    "        * Perform K-Means clustering on the embeddings to identify potential groupings or clusters of similar personality traits.\n",
    "        * Add cluster labels to the dataset and save the clustered data.\n",
    "        * Log clustering parameters (e.g., number of clusters) and the clustered data as artifacts in MLflow.\n",
    "    * **Importance:** This step offers valuable insights into the data and model:\n",
    "        * PCA visualization helps understand the distribution of embeddings.\n",
    "        * Clustering can reveal underlying patterns in the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67b2a39-e66d-4a46-8f62-ad88cd36ce87",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">\n",
    "\n",
    "# **Step 2: API key setup** <a class=\"anchor\" id=\"PAPC_page_2\"></a>\n",
    "\n",
    "[Back to Top](#PAPC_toc)\n",
    "\n",
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a790191a-5760-45d1-a325-0cb2dcff5582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API key file found at: /Users/jsr/openai_api_key.txt\n",
      "üîë API key read successfully.\n",
      "ü§ñ OpenAI API client initialized.\n",
      "‚úÖ OpenAI API key verified successfully with a basic API call.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "import mlflow\n",
    "\n",
    "# Start an MLFlow run for API key setup\n",
    "with mlflow.start_run(run_name=\"API Key Handling and Initialization\") as run:\n",
    "    try:\n",
    "        # Log the environment setup process\n",
    "        mlflow.log_param(\"Step\", \"API Key Handling and Initialization\")\n",
    "\n",
    "        # Define the path to your API key file\n",
    "        api_key_file_path = os.path.expanduser('~/openai_api_key.txt')\n",
    "        mlflow.log_param(\"API Key File Path\", api_key_file_path)\n",
    "\n",
    "        # Evaluation: Check if the API key file exists\n",
    "        if not os.path.exists(api_key_file_path):\n",
    "            error_message = f\"API key file not found at: {api_key_file_path}. Please ensure the file exists.\"\n",
    "            mlflow.log_param(\"API Key SPAPCus\", \"Error: File not found\")\n",
    "            mlflow.log_param(\"Error\", error_message)\n",
    "            raise FileNotFoundError(error_message)\n",
    "        else:\n",
    "            mlflow.log_param(\"API Key File Existence\", \"Confirmed\")\n",
    "            print(f\"‚úÖ API key file found at: {api_key_file_path}\")\n",
    "\n",
    "        # Read the API key from the file\n",
    "        with open(api_key_file_path, 'r') as file:\n",
    "            api_key = file.read().strip()\n",
    "\n",
    "        # Evaluation: Check if the read API key is empty\n",
    "        if not api_key:\n",
    "            error_message = f\"API key file at: {api_key_file_path} is empty. Please ensure your API key is in the file.\"\n",
    "            mlflow.log_param(\"API Key Status\", \"Error: Empty file\")\n",
    "            mlflow.log_param(\"Error\", error_message)\n",
    "            raise ValueError(error_message)\n",
    "        else:\n",
    "            mlflow.log_param(\"API Key Status\", \"Read successfully\")\n",
    "            mlflow.log_param(\"API Key Length\", len(api_key)) # Log the length as a basic sanity check\n",
    "            print(\"üîë API key read successfully.\")\n",
    "\n",
    "        # Set up your OpenAI API key\n",
    "        openai.api_key = api_key\n",
    "        client = openai.OpenAI(api_key=api_key)\n",
    "        mlflow.log_param(\"OpenAI API Client\", \"Initialized successfully\")\n",
    "        print(\"ü§ñ OpenAI API client initialized.\")\n",
    "\n",
    "        # Evaluation: Attempt a basic API call to verify the key (optional, but recommended for immediate feedback)\n",
    "        try:\n",
    "            response = client.models.list()  # Removed the 'limit' argument\n",
    "            mlflow.log_param(\"API Key Verification\", \"Successful (models list)\")\n",
    "            print(\"‚úÖ OpenAI API key verified successfully with a basic API call.\")\n",
    "        except openai.AuthenticationError as auth_error:\n",
    "            mlflow.log_param(\"API Key Verification\", \"Failed (AuthenticationError)\")\n",
    "            mlflow.log_param(\"Error\", str(auth_error))\n",
    "            raise openai.AuthenticationError(f\"OpenAI API key authentication failed: {auth_error}\")\n",
    "        except openai.OpenAIError as general_error:\n",
    "            mlflow.log_param(\"API Key Verification\", \"Failed (OpenAIError)\")\n",
    "            mlflow.log_param(\"Error\", str(general_error))\n",
    "            print(f\"‚ö†Ô∏è Warning: OpenAI API client initialized, but a test call failed with: {general_error}. Further API calls might fail.\")\n",
    "            mlflow.log_param(\"API Key Verification Warning\", str(general_error))\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        mlflow.log_param(\"Error Type\", \"FileNotFoundError\")\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        raise\n",
    "    except ValueError as e:\n",
    "        mlflow.log_param(\"Error Type\", \"ValueError\")\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        raise\n",
    "    except openai.AuthenticationError as e:\n",
    "        mlflow.log_param(\"Error Type\", \"AuthenticationError\")\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        raise\n",
    "    except openai.OpenAIError as e:\n",
    "        mlflow.log_param(\"Error Type\", \"OpenAIError\")\n",
    "        print(f\"‚ö†Ô∏è Warning during API initialization: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        # Log the error if any other unexpected issue occurs\n",
    "        mlflow.log_param(\"Error Type\", type(e).__name__)\n",
    "        mlflow.log_param(\"Error\", str(e))\n",
    "        print(f\"‚ùå An unexpected error occurred: {e}\")\n",
    "        raise\n",
    "\n",
    "    finally:\n",
    "        # End the MLFlow run\n",
    "        mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d949ba3-7cf4-4bda-9e56-6a4f43cb8882",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">\n",
    "\n",
    "# **Step 3: Test Embedding** <a class=\"anchor\" id=\"PAPC_page_2\"></a>\n",
    "\n",
    "[Back to Top](#PAPC_toc)\n",
    "\n",
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8da8f526-a61f-4805-9d71-2a20383e45f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenAI client found and ready.\n",
      "üìù Testing embedding for text: 'The quick brown fox jumps over the lazy dog.'\n",
      "‚úÖ Embedding model 'text-embedding-3-small' is available.\n",
      "Embedding length: 1536\n",
      "Embedding snippet: [-0.01842353865504265, -0.00725775770843029, 0.0036669441033154726, -0.0542047917842865, -0.022724902257323265, 0.03694858402013779, 0.02903103083372116, 0.023866858333349228, 0.011229223571717739, -0.020618630573153496]\n",
      "‚úÖ Embedding API test successful.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import mlflow\n",
    "\n",
    "# Function to test the OpenAI Embedding API\n",
    "def test_openai_embedding_api():\n",
    "    with mlflow.start_run(run_name=\"Test Embedding API\") as run:\n",
    "        try:\n",
    "            # Log the step name\n",
    "            mlflow.log_param(\"Step\", \"Test Embedding API\")\n",
    "\n",
    "            # Evaluation: Check if the OpenAI client is initialized\n",
    "            if 'client' not in globals() or not isinstance(client, openai.OpenAI):\n",
    "                error_message = \"OpenAI client is not initialized. Ensure the API key setup step was executed successfully.\"\n",
    "                mlflow.log_param(\"API Client Status\", \"Not Initialized\")\n",
    "                mlflow.log_param(\"Error\", error_message)\n",
    "                raise RuntimeError(error_message)\n",
    "            else:\n",
    "                mlflow.log_param(\"API Client Status\", \"Initialized\")\n",
    "                print(\"‚úÖ OpenAI client found and ready.\")\n",
    "\n",
    "            # Example text to embed\n",
    "            text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "            mlflow.log_param(\"Test Text\", text)\n",
    "            print(f\"üìù Testing embedding for text: '{text}'\")\n",
    "\n",
    "            # Evaluation: Check if the specified embedding model is available (optional, but good practice)\n",
    "            embedding_model = \"text-embedding-3-small\"\n",
    "            mlflow.log_param(\"Embedding Model\", embedding_model)\n",
    "            try:\n",
    "                model_info = client.models.retrieve(embedding_model)\n",
    "                mlflow.log_param(\"Embedding Model Availability\", \"Confirmed\")\n",
    "                print(f\"‚úÖ Embedding model '{embedding_model}' is available.\")\n",
    "            except openai.NotFoundError:\n",
    "                error_message = f\"Embedding model '{embedding_model}' not found. Please check the model name.\"\n",
    "                mlflow.log_param(\"Embedding Model Availability\", \"Not Found\")\n",
    "                mlflow.log_param(\"Error\", error_message)\n",
    "                raise ValueError(error_message)\n",
    "            except Exception as e:\n",
    "                error_message = f\"Error checking embedding model availability: {e}\"\n",
    "                mlflow.log_param(\"Embedding Model Availability Check Error\", str(e))\n",
    "                mlflow.log_param(\"Error\", error_message)\n",
    "                print(f\"‚ö†Ô∏è Warning: Error checking model availability: {e}. Proceeding with embedding request.\")\n",
    "\n",
    "            # Request to generate embeddings\n",
    "            response = client.embeddings.create(\n",
    "                input=[text],  # The input should be a list of strings\n",
    "                model=embedding_model\n",
    "            )\n",
    "\n",
    "            # Evaluation: Check if the embedding response contains data\n",
    "            if not response.data:\n",
    "                error_message = \"Embedding API response does not contain any data.\"\n",
    "                mlflow.log_param(\"Embedding API Response Status\", \"No Data\")\n",
    "                mlflow.log_param(\"Error\", error_message)\n",
    "                raise ValueError(error_message)\n",
    "            else:\n",
    "                mlflow.log_param(\"Embedding API Response Status\", \"Data Received\")\n",
    "\n",
    "            # Extract the embedding\n",
    "            embedding = response.data[0].embedding\n",
    "\n",
    "            # Evaluation: Check if the extracted embedding is not empty\n",
    "            if not embedding:\n",
    "                error_message = \"Extracted embedding is empty.\"\n",
    "                mlflow.log_param(\"Embedding Extraction Status\", \"Empty Embedding\")\n",
    "                mlflow.log_param(\"Error\", error_message)\n",
    "                raise ValueError(error_message)\n",
    "            else:\n",
    "                mlflow.log_param(\"Embedding Extraction Status\", \"Success\")\n",
    "\n",
    "            # Log the embedding length and a snippet\n",
    "            embedding_length = len(embedding)\n",
    "            mlflow.log_param(\"Embedding Length\", embedding_length)\n",
    "            mlflow.log_param(\"Embedding Snippet\", embedding[:10])\n",
    "\n",
    "            # Print the embedding length and a snippet\n",
    "            print(f\"Embedding length: {embedding_length}\")\n",
    "            print(f\"Embedding snippet: {embedding[:10]}\")  # Print the first 10 elements of the embedding\n",
    "\n",
    "            print(\"‚úÖ Embedding API test successful.\")\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            mlflow.log_param(\"Error Type\", \"RuntimeError\")\n",
    "            mlflow.log_param(\"Error\", str(e))\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            raise\n",
    "        except ValueError as e:\n",
    "            mlflow.log_param(\"Error Type\", \"ValueError\")\n",
    "            mlflow.log_param(\"Error\", str(e))\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            raise\n",
    "        except openai.NotFoundError as e:\n",
    "            mlflow.log_param(\"Error Type\", \"NotFoundError\")\n",
    "            mlflow.log_param(\"Error\", str(e))\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            raise\n",
    "        except openai.OpenAIError as e:\n",
    "            mlflow.log_param(\"Error Type\", \"OpenAIError\")\n",
    "            mlflow.log_param(\"Error\", str(e))\n",
    "            print(f\"‚ùå OpenAI API error: {e}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            # Log the error if any other unexpected issue occurs\n",
    "            mlflow.log_param(\"Error Type\", type(e).__name__)\n",
    "            mlflow.log_param(\"Error\", str(e))\n",
    "            print(f\"‚ùå An unexpected error occurred: {e}\")\n",
    "            raise\n",
    "\n",
    "        finally:\n",
    "            # End the MLFlow run\n",
    "            mlflow.end_run()\n",
    "\n",
    "# Test the OpenAI Embedding API\n",
    "test_openai_embedding_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c144305-f45d-4bd8-bb64-b5873d8b1029",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">\n",
    "\n",
    "# **Step 4: Create PAPC Embeddings** <a class=\"anchor\" id=\"PAPC_page_3\"></a>\n",
    "\n",
    "[Back to Top](#PAPC_toc)\n",
    "\n",
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fbcfbc8-c245-4b72-b1c0-37c5cc8f57ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/26 15:43:43 INFO mlflow.tracking.fluent: Experiment with name 'PAPC' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenAI client initialized.\n",
      "‚úÖ Loaded PAPC dataset: ../Datasets/papc.csv (72 rows)\n",
      "‚è≥ Started embeddings at 2025-05-26T14:43:43.504606+00:00\n",
      "‚úÖ Finished embeddings at 2025-05-26T14:44:21.003375+00:00 (took 0:00:37.498769)\n",
      "üíæ Embeddings saved to ../Embeddings/papc_embeddings.csv\n",
      "\n",
      "Sample embeddings:\n",
      "    Factor  Adjective       Synonym       Verb         Noun  \\\n",
      "0  Person  Person ID            ID   Identify     Identity   \n",
      "1  Person  Person ID    Identifier  Recognize       Record   \n",
      "2  Person  Person ID           Key        Tag          Key   \n",
      "3  Person        Age  Demographics        Age  Demographic   \n",
      "4  Person        Age    Life Stage     Mature        Years   \n",
      "\n",
      "                                           Embedding  \n",
      "0  [0.014732071198523045, -0.019797882065176964, ...  \n",
      "1  [0.03477334603667259, -0.02332128770649433, -0...  \n",
      "2  [0.0335334874689579, -0.005168645642697811, -0...  \n",
      "3  [0.018649639561772346, 0.01560277584940195, 0....  \n",
      "4  [0.07422163337469101, 0.011525982059538364, 0....  \n",
      "üèÉ View run Generate_PAPC_Embeddings at: http://127.0.0.1:5000/#/experiments/466113964654577738/runs/8465b7bba6d044e793d097f2a9b6cf72\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/466113964654577738\n",
      "‚úÖ PAPC embedding generation complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime, timezone\n",
    "import pandas as pd\n",
    "from openai import OpenAI, OpenAIError\n",
    "import mlflow\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 0) Init ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "load_dotenv(override=True)\n",
    "if os.getenv(\"MLFLOW_TRACKING_URI\"):\n",
    "    mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\"))\n",
    "mlflow.set_experiment(\"PAPC\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 1) Helper: read OpenAI key ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def get_openai_api_key_from_file(filepath=\"~/openai_api_key.txt\"):\n",
    "    path = os.path.expanduser(filepath)\n",
    "    try:\n",
    "        with open(path, \"r\") as f:\n",
    "            key = f.read().strip()\n",
    "        if not key:\n",
    "            raise ValueError(f\"API key file at '{path}' is empty.\")\n",
    "        return key\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"API key file not found at '{path}'\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error reading API key from '{path}': {e}\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 2) Initialize OpenAI client ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "try:\n",
    "    OPENAI_KEY = os.getenv(\"OPENAI_API_KEY\") or get_openai_api_key_from_file()\n",
    "    client = OpenAI(api_key=OPENAI_KEY)\n",
    "    print(\"‚úÖ OpenAI client initialized.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error initializing OpenAI client: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 3) Load PAPC dataset ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "dataset_path = '../Datasets/papc.csv'\n",
    "try:\n",
    "    papc_df = pd.read_csv(dataset_path)\n",
    "    print(f\"‚úÖ Loaded PAPC dataset: {dataset_path} ({len(papc_df)} rows)\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"PAPC dataset not found at '{dataset_path}'\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 4) Embedding helper ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def get_embedding(text: str, model=\"text-embedding-3-small\"):\n",
    "    try:\n",
    "        resp = client.embeddings.create(input=[text], model=model)\n",
    "        return resp.data[0].embedding\n",
    "    except OpenAIError as e:\n",
    "        print(f\"‚ùå OpenAI embedding error for text '{text[:30]}...': {e}\")\n",
    "        raise\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 5) Generate & Log Embeddings ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "with mlflow.start_run(run_name=\"Generate_PAPC_Embeddings\"):\n",
    "    mlflow.log_param(\"model\", \"text-embedding-3-small\")\n",
    "    mlflow.log_param(\"dataset\", dataset_path)\n",
    "    start_ts = datetime.now(timezone.utc)\n",
    "    mlflow.log_param(\"start_time\", start_ts.isoformat())\n",
    "    print(f\"‚è≥ Started embeddings at {start_ts.isoformat()}\")\n",
    "\n",
    "    if papc_df.empty:\n",
    "        raise ValueError(\"Loaded PAPC dataframe is empty.\")\n",
    "\n",
    "    papc_df['Embedding'] = papc_df.apply(\n",
    "        lambda row: get_embedding(\n",
    "            f\"{row['Factor']} | {row['Adjective']} | \"\n",
    "            f\"{row['Synonym']} | {row['Verb']} | {row['Noun']}\"\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    end_ts = datetime.now(timezone.utc)\n",
    "    mlflow.log_param(\"end_time\", end_ts.isoformat())\n",
    "    print(f\"‚úÖ Finished embeddings at {end_ts.isoformat()} (took {end_ts - start_ts})\")\n",
    "\n",
    "    # Save embeddings CSV\n",
    "    out_path = '../Embeddings/papc_embeddings.csv'\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    papc_df.loc[:, ['Factor','Adjective','Synonym','Verb','Noun','Embedding']] \\\n",
    "        .to_csv(out_path, index=False)\n",
    "    mlflow.log_artifact(out_path, artifact_path=\"embeddings\")\n",
    "    print(f\"üíæ Embeddings saved to {out_path}\")\n",
    "\n",
    "    # Log run stats\n",
    "    mlflow.log_param(\"num_rows\",    papc_df.shape[0])\n",
    "    mlflow.log_param(\"num_columns\", papc_df.shape[1])\n",
    "    mlflow.log_param(\"embedding_dim\", len(papc_df['Embedding'].iloc[0]))\n",
    "\n",
    "    print(\"\\nSample embeddings:\\n\", papc_df.head())\n",
    "\n",
    "print(\"‚úÖ PAPC embedding generation complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34b9502-e9f9-4b20-bbb4-813e3e5dd54a",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">\n",
    "\n",
    "# **Step 5: Create Label Embeddings** <a class=\"anchor\" id=\"PAPC_page_3\"></a>\n",
    "\n",
    "[Back to Top](#PAPC_toc)\n",
    "\n",
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddfe5c0b-fda2-439c-9348-ac797b5eb29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embeddings dataset loaded: ../Embeddings/papc_embeddings.csv (72 rows)\n",
      "‚úÖ Encoded 5 unique PAPC factors\n",
      "üíæ Label encoder saved to: ../Models/papc_label_encoder.pkl\n",
      "\n",
      "Factor ‚Üí Encoded preview:\n",
      "                           Factor  Factor_Encoded\n",
      "0                         Health               0\n",
      "1                      Lifestyle               1\n",
      "2                         Person               2\n",
      "3  Psychological Characteristics               3\n",
      "4                 Social Factors               4\n",
      "\n",
      "Full mapping:\n",
      "  Health ‚Üí 0\n",
      "  Lifestyle ‚Üí 1\n",
      "  Person ‚Üí 2\n",
      "  Psychological Characteristics ‚Üí 3\n",
      "  Social Factors ‚Üí 4\n",
      "‚úÖ Plot saved to /Users/jsr/Downloads/GitHub/Personality-Trait-Models/Notebooks/papc_label_encoder_mapping.png\n",
      "üèÉ View run PAPC_LabelEncoding_Visualization at: http://127.0.0.1:5000/#/experiments/466113964654577738/runs/87e0f635563a4de49bcafb9285a7119e\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/466113964654577738\n",
      "‚úÖ PAPC label encoding & visualization complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Setup MLflow ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "load_dotenv(override=True)\n",
    "if os.getenv(\"MLFLOW_TRACKING_URI\"):\n",
    "    mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\"))\n",
    "mlflow.set_experiment(\"PAPC\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 1) Load embeddings dataset ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "embeddings_csv_path = '../Embeddings/papc_embeddings.csv'\n",
    "try:\n",
    "    df = pd.read_csv(embeddings_csv_path)\n",
    "    print(f\"‚úÖ Embeddings dataset loaded: {embeddings_csv_path} ({len(df)} rows)\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Error: Embeddings dataset not found at: {embeddings_csv_path}\")\n",
    "    exit(1)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 2) Fit LabelEncoder on the 'Factor' column ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "label_encoder = LabelEncoder()\n",
    "df['Factor_Encoded'] = label_encoder.fit_transform(df['Factor'])\n",
    "n_classes = len(label_encoder.classes_)\n",
    "print(f\"‚úÖ Encoded {n_classes} unique PAPC factors\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 3) Save the encoder to disk ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "os.makedirs(\"../Models\", exist_ok=True)\n",
    "label_encoder_path = \"../Models/papc_label_encoder.pkl\"\n",
    "joblib.dump(label_encoder, label_encoder_path)\n",
    "print(f\"üíæ Label encoder saved to: {label_encoder_path}\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 4) Visualization helper ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def visualize_label_encoder(le, artifact_path=\"visualization\"):\n",
    "    classes = le.classes_\n",
    "    codes   = le.transform(classes)\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    sns.barplot(x=classes, y=codes)\n",
    "    plt.xlabel(\"PAPC Factor\")\n",
    "    plt.ylabel(\"Encoded Value\")\n",
    "    plt.title(\"PAPC Factors ‚Üí Encoded Mapping\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    out_file = \"papc_label_encoder_mapping.png\"\n",
    "    plt.savefig(out_file)\n",
    "    plt.close()\n",
    "    print(f\"‚úÖ Plot saved to {os.path.abspath(out_file)}\")\n",
    "    mlflow.log_artifact(out_file, artifact_path=artifact_path)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 5) Log everything in MLflow ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "with mlflow.start_run(run_name=\"PAPC_LabelEncoding_Visualization\"):\n",
    "    mlflow.log_param(\"step\", \"label_encoding\")\n",
    "    mlflow.log_param(\"embeddings_csv\", embeddings_csv_path)\n",
    "    mlflow.log_param(\"num_rows\", df.shape[0])\n",
    "    mlflow.log_param(\"num_unique_factors\", n_classes)\n",
    "    mlflow.log_artifact(label_encoder_path, artifact_path=\"label_encoder\")\n",
    "\n",
    "    # Preview mapping\n",
    "    preview = (\n",
    "        df[['Factor', 'Factor_Encoded']]\n",
    "        .drop_duplicates()\n",
    "        .sort_values('Factor_Encoded')\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    print(\"\\nFactor ‚Üí Encoded preview:\\n\", preview)\n",
    "\n",
    "    # Full mapping dict\n",
    "    mapping = {\n",
    "        factor: code\n",
    "        for factor, code in zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))\n",
    "    }\n",
    "    print(\"\\nFull mapping:\")\n",
    "    for factor, code in mapping.items():\n",
    "        print(f\"  {factor} ‚Üí {code}\")\n",
    "    mlflow.log_dict(mapping, \"label_encoder/mapping.json\")\n",
    "\n",
    "    # Generate & log bar chart\n",
    "    visualize_label_encoder(label_encoder)\n",
    "\n",
    "print(\"‚úÖ PAPC label encoding & visualization complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a5fde5-2c9a-46dc-998a-ca01c356e80e",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">\n",
    "\n",
    "# **Step 6: Create Model** <a class=\"anchor\" id=\"PAPC_page_3\"></a>\n",
    "\n",
    "[Back to Top](#PAPC_toc)\n",
    "\n",
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc4b4c49-a90c-4b4e-9235-6a2036f6cb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/26 15:47:18 INFO mlflow.tracking.fluent: Experiment with name 'PAPC_RF_Training' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded PAPC embeddings: ../Embeddings/papc_embeddings.csv (72 rows)\n",
      "‚úÖ Loaded label encoder from: ../Models/papc_label_encoder.pkl\n",
      "Train samples: 43, Test samples: 29\n",
      "‚è≥ Training started at 2025-05-26T14:47:18.765291+00:00\n",
      "‚úÖ Training finished at 2025-05-26T14:47:18.830168+00:00 (Duration: 0:00:00.064877)\n",
      "üîç Test accuracy: 1.0000\n",
      "üìä Saved classification report: papc_classification_report.csv\n",
      "üìä Saved confusion matrix CSV: papc_confusion_matrix.csv\n",
      "üñºÔ∏è Saved confusion matrix plot: papc_confusion_matrix.png\n",
      "üíæ Trained model saved to ../Models/papc_rf_model.pkl\n",
      "\n",
      "Classification Report:\n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "                       Health       1.00      1.00      1.00         6\n",
      "                    Lifestyle       1.00      1.00      1.00         6\n",
      "                       Person       1.00      1.00      1.00         7\n",
      "Psychological Characteristics       1.00      1.00      1.00         5\n",
      "               Social Factors       1.00      1.00      1.00         5\n",
      "\n",
      "                     accuracy                           1.00        29\n",
      "                    macro avg       1.00      1.00      1.00        29\n",
      "                 weighted avg       1.00      1.00      1.00        29\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "                                Health  Lifestyle  Person  \\\n",
      "Health                              6          0       0   \n",
      "Lifestyle                           0          6       0   \n",
      "Person                              0          0       7   \n",
      "Psychological Characteristics       0          0       0   \n",
      "Social Factors                      0          0       0   \n",
      "\n",
      "                               Psychological Characteristics  Social Factors  \n",
      "Health                                                     0               0  \n",
      "Lifestyle                                                  0               0  \n",
      "Person                                                     0               0  \n",
      "Psychological Characteristics                              5               0  \n",
      "Social Factors                                             0               5  \n",
      "üèÉ View run PAPC_RF_Training_Run at: http://127.0.0.1:5000/#/experiments/856764489663070313/runs/02b5b5e3c7ed4b868d72f29f308cd46b\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/856764489663070313\n",
      "‚úÖ PAPC model training complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from datetime import datetime, timezone\n",
    "import mlflow\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Setup MLflow ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "load_dotenv(override=True)\n",
    "if os.getenv(\"MLFLOW_TRACKING_URI\"):\n",
    "    mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\"))\n",
    "mlflow.set_experiment(\"PAPC_RF_Training\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 1) Load PAPC embeddings ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "embeddings_csv_path = '../Embeddings/papc_embeddings.csv'\n",
    "try:\n",
    "    df = pd.read_csv(\n",
    "        embeddings_csv_path,\n",
    "        converters={'Embedding': lambda s: np.array(ast.literal_eval(s)) if isinstance(s, str) else np.array(s)}\n",
    "    )\n",
    "    print(f\"‚úÖ Loaded PAPC embeddings: {embeddings_csv_path} ({len(df)} rows)\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Error: PAPC embeddings not found at {embeddings_csv_path}\")\n",
    "    exit(1)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 2) Load the pre-fitted PAPC label encoder ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "label_encoder_path = \"../Models/papc_label_encoder.pkl\"\n",
    "try:\n",
    "    label_encoder = joblib.load(label_encoder_path)\n",
    "    print(f\"‚úÖ Loaded label encoder from: {label_encoder_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Error: Label encoder not found at {label_encoder_path}\")\n",
    "    exit(1)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 3) Prepare features & labels ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "X = np.stack(df['Embedding'].values)      # shape: (n_samples, dim)\n",
    "y = df['Factor'].values                   # PAPC factor names\n",
    "y_encoded = label_encoder.transform(y)    # numeric labels\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 4) Train/test split ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "counts = pd.Series(y_encoded).value_counts()\n",
    "stratify_param = y_encoded if counts.min() >= 2 else None\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded,\n",
    "    test_size=0.4,\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    "    stratify=stratify_param\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {X_train.shape[0]}, Test samples: {X_test.shape[0]}\")\n",
    "if stratify_param is None:\n",
    "    print(\"‚ö†Ô∏è  Skipped stratification: some classes have only 1 sample.\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 5) MLflow run: train & evaluate ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "with mlflow.start_run(run_name=\"PAPC_RF_Training_Run\"):\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"model_type\", \"RandomForestClassifier\")\n",
    "    mlflow.log_param(\"n_estimators\", 100)\n",
    "    mlflow.log_param(\"test_size\", 0.4)\n",
    "    mlflow.log_param(\"random_state\", 42)\n",
    "    mlflow.log_artifact(label_encoder_path, artifact_path=\"label_encoder\")\n",
    "\n",
    "    start_ts = datetime.now(timezone.utc)\n",
    "    mlflow.log_param(\"start_time\", start_ts.isoformat())\n",
    "    print(f\"‚è≥ Training started at {start_ts.isoformat()}\")\n",
    "\n",
    "    # Train classifier\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    end_ts = datetime.now(timezone.utc)\n",
    "    mlflow.log_param(\"end_time\", end_ts.isoformat())\n",
    "    print(f\"‚úÖ Training finished at {end_ts.isoformat()} (Duration: {end_ts - start_ts})\")\n",
    "\n",
    "    # Predict & evaluate\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    mlflow.log_metric(\"test_accuracy\", acc)\n",
    "    print(f\"üîç Test accuracy: {acc:.4f}\")\n",
    "\n",
    "    # Only labels present\n",
    "    present = np.unique(np.concatenate([y_test, y_pred]))\n",
    "    names = [label_encoder.classes_[i] for i in present]\n",
    "\n",
    "    # Classification report\n",
    "    report_dict = classification_report(\n",
    "        y_test, y_pred,\n",
    "        labels=present,\n",
    "        target_names=names,\n",
    "        output_dict=True,\n",
    "        zero_division=0\n",
    "    )\n",
    "    report_df = pd.DataFrame(report_dict).transpose()\n",
    "    report_path = \"papc_classification_report.csv\"\n",
    "    report_df.to_csv(report_path, index=True)\n",
    "    mlflow.log_artifact(report_path, artifact_path=\"metrics\")\n",
    "    print(f\"üìä Saved classification report: {report_path}\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=present)\n",
    "    cm_df = pd.DataFrame(cm, index=names, columns=names)\n",
    "    cm_csv = \"papc_confusion_matrix.csv\"\n",
    "    cm_df.to_csv(cm_csv, index=True)\n",
    "    mlflow.log_artifact(cm_csv, artifact_path=\"metrics\")\n",
    "    print(f\"üìä Saved confusion matrix CSV: {cm_csv}\")\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(\"PAPC RandomForest Confusion Matrix\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.tight_layout()\n",
    "    cm_img = \"papc_confusion_matrix.png\"\n",
    "    plt.savefig(cm_img)\n",
    "    plt.close()\n",
    "    mlflow.log_artifact(cm_img, artifact_path=\"metrics\")\n",
    "    print(f\"üñºÔ∏è Saved confusion matrix plot: {cm_img}\")\n",
    "\n",
    "    # Save trained model\n",
    "    os.makedirs(\"../Models\", exist_ok=True)\n",
    "    model_path = \"../Models/papc_rf_model.pkl\"\n",
    "    joblib.dump(clf, model_path)\n",
    "    mlflow.log_artifact(model_path, artifact_path=\"models\")\n",
    "    print(f\"üíæ Trained model saved to {model_path}\")\n",
    "\n",
    "    # Final summary\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(\n",
    "        y_test, y_pred, labels=present, target_names=names, zero_division=0\n",
    "    ))\n",
    "    print(\"\\nConfusion Matrix:\\n\", cm_df)\n",
    "\n",
    "print(\"‚úÖ PAPC model training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3656c2b1-5d3d-489d-9759-ed219e20d2b5",
   "metadata": {},
   "source": [
    "This block of code is a comprehensive step-by-step process focusing specifically on:\n",
    "1. **Data Loading and Preprocessing**: Parsing and preparing embeddings from a CSV file for machine learning.\n",
    "2. **Model Training**: Using a RandomForestClassifier to train on the embeddings.\n",
    "3. **Model Evaluation**: Calculating and logging metrics such as accuracy, alongside detailed classification reports and confusion matrices.\n",
    "4. **Visualization and Logging**: Visualizing the confusion matrix and logging both the visual represenPAPCion and numerical data as artifacts in MLflow.\n",
    "5. **Model Persistence**: Saving the trained model for future use or deployment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b586cb-65eb-4f24-9726-c7b61611182c",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">\n",
    "\n",
    "# **Step 7: Evaluate Model** <a class=\"anchor\" id=\"PAPC_page_3\"></a>\n",
    "\n",
    "[Back to Top](#PAPC_toc)\n",
    "\n",
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bbc78ca-7e53-4ab5-8c7c-c2d2848d4367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded embeddings from: ../Embeddings/bt_embeddings.csv\n",
      "‚úÖ Label encoder loaded from: ../Models/bt_label_encoder.pkl\n",
      "‚úÖ Model loaded from: ../Models/bt_rf_model.pkl\n",
      "Train size: 38, Test size: 26\n",
      "üîç Test accuracy: 1.0000\n",
      "üìä Saved classification report: bt_evaluation_classification_report.csv\n",
      "üìä Saved confusion matrix CSV: bt_evaluation_confusion_matrix.csv\n",
      "üñºÔ∏è Saved confusion matrix plot: bt_evaluation_confusion_matrix.png\n",
      "üèÉ View run BT_Model_Evaluation at: http://127.0.0.1:5000/#/experiments/321510111379825637/runs/f2f2357a7de24c49948c85c1b3408415\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/321510111379825637\n",
      "‚úÖ BT model evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Setup MLflow ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "load_dotenv(override=True)\n",
    "if os.getenv(\"MLFLOW_TRACKING_URI\"):\n",
    "    mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\"))\n",
    "mlflow.set_experiment(\"BartleTypes\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Paths ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "EMBEDDINGS_CSV      = \"../Embeddings/papc_embeddings.csv\"\n",
    "LABEL_ENCODER_PKL   = \"../Models/papc_label_encoder.pkl\"\n",
    "MODEL_PKL           = \"../Models/papc_rf_model.pkl\"\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 1) Load embeddings with safe parsing ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "try:\n",
    "    df = pd.read_csv(\n",
    "        EMBEDDINGS_CSV,\n",
    "        converters={\"Embedding\": lambda s: np.array(ast.literal_eval(s)) if isinstance(s, str) else np.array(s)}\n",
    "    )\n",
    "    print(f\"‚úÖ Loaded embeddings from: {EMBEDDINGS_CSV}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Error loading embeddings: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 2) Load label encoder & model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "try:\n",
    "    label_encoder = joblib.load(LABEL_ENCODER_PKL)\n",
    "    print(f\"‚úÖ Label encoder loaded from: {LABEL_ENCODER_PKL}\")\n",
    "    clf = joblib.load(MODEL_PKL)\n",
    "    print(f\"‚úÖ Model loaded from: {MODEL_PKL}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Artifact missing: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 3) Prepare features & labels ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "X = np.stack(df[\"Embedding\"].values)\n",
    "y = df[\"Factor\"].values\n",
    "y_encoded = label_encoder.transform(y)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 4) Train/test split (optional stratify) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "counts = pd.Series(y_encoded).value_counts()\n",
    "stratify_param = y_encoded if counts.min() >= 2 else None\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded,\n",
    "    test_size=0.4,\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    "    stratify=stratify_param\n",
    ")\n",
    "print(f\"Train size: {len(y_train)}, Test size: {len(y_test)}\")\n",
    "if stratify_param is None:\n",
    "    print(\"‚ö†Ô∏è  Skipped stratification‚Äîat least one class has only one sample\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 5) Evaluate under MLflow ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "with mlflow.start_run(run_name=\"PAPC_Model_Evaluation\"):\n",
    "    mlflow.log_param(\"step\", \"evaluate_model\")\n",
    "    mlflow.log_param(\"test_samples\", len(y_test))\n",
    "    mlflow.log_artifact(LABEL_ENCODER_PKL, artifact_path=\"label_encoder\")\n",
    "    mlflow.log_artifact(MODEL_PKL, artifact_path=\"model\")\n",
    "    mlflow.log_param(\"start_time\", str(datetime.now()))\n",
    "\n",
    "    # Predict\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Accuracy\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    mlflow.log_metric(\"accuracy\", acc)\n",
    "    print(f\"üîç Test accuracy: {acc:.4f}\")\n",
    "\n",
    "    # Only keep labels actually seen\n",
    "    present = np.unique(np.concatenate([y_test, y_pred]))\n",
    "    names   = [label_encoder.classes_[i] for i in present]\n",
    "\n",
    "    # Classification report\n",
    "    report_dict = classification_report(\n",
    "        y_test, y_pred,\n",
    "        labels=present,\n",
    "        target_names=names,\n",
    "        output_dict=True,\n",
    "        zero_division=0\n",
    "    )\n",
    "    report_df = pd.DataFrame(report_dict).transpose()\n",
    "    report_csv = \"papc_evaluation_classification_report.csv\"\n",
    "    report_df.to_csv(report_csv, index=True)\n",
    "    mlflow.log_artifact(report_csv, artifact_path=\"evaluation\")\n",
    "    print(f\"üìä Saved classification report: {report_csv}\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm    = confusion_matrix(y_test, y_pred, labels=present)\n",
    "    cm_df = pd.DataFrame(cm, index=names, columns=names)\n",
    "    cm_csv = \"papc_evaluation_confusion_matrix.csv\"\n",
    "    cm_df.to_csv(cm_csv, index=True)\n",
    "    mlflow.log_artifact(cm_csv, artifact_path=\"evaluation\")\n",
    "    print(f\"üìä Saved confusion matrix CSV: {cm_csv}\")\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(\"Bartle Types‚ÄîConfusion Matrix (Test Set)\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.tight_layout()\n",
    "    cm_img = \"papc_evaluation_confusion_matrix.png\"\n",
    "    plt.savefig(cm_img)\n",
    "    plt.close()\n",
    "    mlflow.log_artifact(cm_img, artifact_path=\"evaluation\")\n",
    "    print(f\"üñºÔ∏è Saved confusion matrix plot: {cm_img}\")\n",
    "\n",
    "    mlflow.log_param(\"end_time\", str(datetime.now()))\n",
    "\n",
    "print(\"‚úÖ PAPC model evaluation complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af80057-6c04-4228-ae9b-ed84b87fa9a6",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">\n",
    "\n",
    "# **Step 8: Test and Evaluate Model** <a class=\"anchor\" id=\"PAPC_page_3\"></a>\n",
    "\n",
    "[Back to Top](#PAPC_toc)\n",
    "\n",
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe70d8f9-a9eb-4908-bbf9-db62ee23023b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/26 15:48:14 INFO mlflow.tracking.fluent: Experiment with name 'PAPC_Model_Evaluation' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded embeddings from: ../Embeddings/papc_embeddings.csv\n",
      "‚úÖ Label encoder loaded from: ../Models/papc_label_encoder.pkl\n",
      "‚úÖ Model loaded from: ../Models/papc_rf_model.pkl\n",
      "Train size: 43, Test size: 29\n",
      "üîç Test accuracy: 1.0000\n",
      "üìä Saved classification report: papc_evaluation_classification_report.csv\n",
      "üìä Saved confusion matrix CSV: papc_evaluation_confusion_matrix.csv\n",
      "üñºÔ∏è Saved confusion matrix plot: papc_evaluation_confusion_matrix.png\n",
      "üèÉ View run PAPC_Model_Evaluation at: http://127.0.0.1:5000/#/experiments/980819491455127720/runs/558e03af692c4b19a5b236e1fda61145\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/980819491455127720\n",
      "‚úÖ PAPC model evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Setup MLflow ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "load_dotenv(override=True)\n",
    "if os.getenv(\"MLFLOW_TRACKING_URI\"):\n",
    "    mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\"))\n",
    "mlflow.set_experiment(\"PAPC_Model_Evaluation\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Paths ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "EMBEDDINGS_CSV    = \"../Embeddings/papc_embeddings.csv\"\n",
    "LABEL_ENCODER_PKL = \"../Models/papc_label_encoder.pkl\"\n",
    "MODEL_PKL         = \"../Models/papc_rf_model.pkl\"\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 1) Load embeddings with safe parsing ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "try:\n",
    "    df = pd.read_csv(\n",
    "        EMBEDDINGS_CSV,\n",
    "        converters={\"Embedding\": lambda s: np.array(ast.literal_eval(s)) if isinstance(s, str) else np.array(s)}\n",
    "    )\n",
    "    print(f\"‚úÖ Loaded embeddings from: {EMBEDDINGS_CSV}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Error loading embeddings: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 2) Load label encoder & model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "try:\n",
    "    label_encoder = joblib.load(LABEL_ENCODER_PKL)\n",
    "    print(f\"‚úÖ Label encoder loaded from: {LABEL_ENCODER_PKL}\")\n",
    "    clf = joblib.load(MODEL_PKL)\n",
    "    print(f\"‚úÖ Model loaded from: {MODEL_PKL}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Artifact missing: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 3) Prepare features & labels ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "X = np.stack(df[\"Embedding\"].values)\n",
    "y = df[\"Factor\"].values\n",
    "y_encoded = label_encoder.transform(y)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 4) Train/test split (optional stratify) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "counts = pd.Series(y_encoded).value_counts()\n",
    "stratify_param = y_encoded if counts.min() >= 2 else None\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded,\n",
    "    test_size=0.4,\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    "    stratify=stratify_param\n",
    ")\n",
    "print(f\"Train size: {len(y_train)}, Test size: {len(y_test)}\")\n",
    "if stratify_param is None:\n",
    "    print(\"‚ö†Ô∏è  Skipped stratification‚Äîat least one class has only one sample\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 5) Evaluate under MLflow ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "with mlflow.start_run(run_name=\"PAPC_Model_Evaluation\"):\n",
    "    mlflow.log_param(\"step\", \"evaluate_model\")\n",
    "    mlflow.log_param(\"test_samples\", len(y_test))\n",
    "    mlflow.log_artifact(LABEL_ENCODER_PKL, artifact_path=\"label_encoder\")\n",
    "    mlflow.log_artifact(MODEL_PKL, artifact_path=\"model\")\n",
    "    mlflow.log_param(\"start_time\", datetime.now(timezone.utc).isoformat())\n",
    "\n",
    "    # Predict\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Accuracy\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    mlflow.log_metric(\"accuracy\", acc)\n",
    "    print(f\"üîç Test accuracy: {acc:.4f}\")\n",
    "\n",
    "    # Only keep labels actually seen\n",
    "    present = np.unique(np.concatenate([y_test, y_pred]))\n",
    "    names   = [label_encoder.classes_[i] for i in present]\n",
    "\n",
    "    # Classification report\n",
    "    report_dict = classification_report(\n",
    "        y_test, y_pred,\n",
    "        labels=present,\n",
    "        target_names=names,\n",
    "        output_dict=True,\n",
    "        zero_division=0\n",
    "    )\n",
    "    report_df = pd.DataFrame(report_dict).transpose()\n",
    "    report_csv = \"papc_evaluation_classification_report.csv\"\n",
    "    report_df.to_csv(report_csv, index=True)\n",
    "    mlflow.log_artifact(report_csv, artifact_path=\"evaluation\")\n",
    "    print(f\"üìä Saved classification report: {report_csv}\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm    = confusion_matrix(y_test, y_pred, labels=present)\n",
    "    cm_df = pd.DataFrame(cm, index=names, columns=names)\n",
    "    cm_csv = \"papc_evaluation_confusion_matrix.csv\"\n",
    "    cm_df.to_csv(cm_csv, index=True)\n",
    "    mlflow.log_artifact(cm_csv, artifact_path=\"evaluation\")\n",
    "    print(f\"üìä Saved confusion matrix CSV: {cm_csv}\")\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(\"PAPC RandomForest Confusion Matrix (Test Set)\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.tight_layout()\n",
    "    cm_img = \"papc_evaluation_confusion_matrix.png\"\n",
    "    plt.savefig(cm_img)\n",
    "    plt.close()\n",
    "    mlflow.log_artifact(cm_img, artifact_path=\"evaluation\")\n",
    "    print(f\"üñºÔ∏è Saved confusion matrix plot: {cm_img}\")\n",
    "\n",
    "    mlflow.log_param(\"end_time\", datetime.now(timezone.utc).isoformat())\n",
    "\n",
    "print(\"‚úÖ PAPC model evaluation complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dc0d50-5e9f-4b73-bb5f-f8e64b2c1f62",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">\n",
    "\n",
    "# **Step 9: Visualize and Evaluate Model** <a class=\"anchor\" id=\"PAPC_page_3\"></a>\n",
    "\n",
    "[Back to Top](#PAPC_toc)\n",
    "\n",
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02e69b18-6364-4067-98b8-4c144511dee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/26 15:49:10 INFO mlflow.tracking.fluent: Experiment with name 'PAPC_Visualization_and_Eval' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded embeddings from: ../Embeddings/papc_embeddings.csv\n",
      "‚è≥ Run started at 2025-05-26T14:49:10.423311+00:00\n",
      "üîç In-sample accuracy: 1.0000\n",
      "‚úÖ Clustered data saved: ../Embeddings/papc_clustered_embeddings.csv\n",
      "‚úÖ Run finished at 2025-05-26T14:49:13.800987+00:00 (Duration: 0:00:03.377676)\n",
      "\n",
      "Classification Report:\n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "                       Health       1.00      1.00      1.00        15\n",
      "                    Lifestyle       1.00      1.00      1.00        15\n",
      "                       Person       1.00      1.00      1.00        18\n",
      "Psychological Characteristics       1.00      1.00      1.00        12\n",
      "               Social Factors       1.00      1.00      1.00        12\n",
      "\n",
      "                     accuracy                           1.00        72\n",
      "                    macro avg       1.00      1.00      1.00        72\n",
      "                 weighted avg       1.00      1.00      1.00        72\n",
      "\n",
      "üèÉ View run PAPC_Visualization_and_Eval at: http://127.0.0.1:5000/#/experiments/519429470322980382/runs/ba6cac3188134af5b3ed1abc302e8863\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/519429470322980382\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timezone\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Setup MLflow & Environment ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "load_dotenv(override=True)\n",
    "if os.getenv(\"MLFLOW_TRACKING_URI\"):\n",
    "    mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\"))\n",
    "mlflow.set_experiment(\"PAPC_Visualization_and_Eval\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Paths ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "EMBEDDINGS_CSV = '../Embeddings/papc_embeddings.csv'\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 1) Load embeddings with safe parsing ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "try:\n",
    "    df = pd.read_csv(\n",
    "        EMBEDDINGS_CSV,\n",
    "        converters={'Embedding': lambda s: np.array(ast.literal_eval(s)) if isinstance(s, str) else np.array(s)}\n",
    "    )\n",
    "    print(f\"‚úÖ Loaded embeddings from: {EMBEDDINGS_CSV}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå File not found: {EMBEDDINGS_CSV}\")\n",
    "    exit(1)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 2) Prepare features and labels ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "X = np.stack(df['Embedding'].values)  # (n_samples, emb_dim)\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(df['Factor'].values)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 3) Start MLflow run ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "with mlflow.start_run(run_name=\"PAPC_Visualization_and_Eval\"):\n",
    "    mlflow.log_param(\"step\", \"visualize_and_evaluate\")\n",
    "    mlflow.log_param(\"embeddings_csv\", EMBEDDINGS_CSV)\n",
    "    start_ts = datetime.now(timezone.utc)\n",
    "    mlflow.log_param(\"start_time\", start_ts.isoformat())\n",
    "    print(f\"‚è≥ Run started at {start_ts.isoformat()}\")\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ 4) Train a RandomForest on full data ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X, y_encoded)\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ 4a) Infer signature & prepare input example ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    preds = clf.predict(X)\n",
    "    signature = infer_signature(X, preds)\n",
    "    input_example = pd.DataFrame([X[0]], columns=[f\"emb_{i}\" for i in range(X.shape[1])])\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ 4b) Log model with signature & example ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    mlflow.sklearn.log_model(\n",
    "        clf,\n",
    "        artifact_path=\"models/papc_rf_model\",\n",
    "        signature=signature,\n",
    "        input_example=input_example\n",
    "    )\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ 5) In-sample evaluation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    in_acc = accuracy_score(y_encoded, preds)\n",
    "    mlflow.log_metric(\"in_sample_accuracy\", in_acc)\n",
    "    print(f\"üîç In-sample accuracy: {in_acc:.4f}\")\n",
    "\n",
    "    cm = confusion_matrix(y_encoded, preds)\n",
    "    cm_df = pd.DataFrame(cm, index=le.classes_, columns=le.classes_)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(\"PAPC In-Sample Confusion Matrix\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.tight_layout()\n",
    "    cm_img = \"papc_in_sample_confusion_matrix.png\"\n",
    "    plt.savefig(cm_img)\n",
    "    plt.close()\n",
    "    mlflow.log_artifact(cm_img, artifact_path=\"evaluation\")\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ 6) PCA visualization ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_encoded, cmap='viridis', alpha=0.7)\n",
    "    handles, _ = scatter.legend_elements()\n",
    "    plt.legend(handles, le.classes_, title=\"PAPC Factor\")\n",
    "    plt.title(\"PCA of PAPC Embeddings\")\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.tight_layout()\n",
    "    pca_img = \"papc_pca.png\"\n",
    "    plt.savefig(pca_img)\n",
    "    plt.close()\n",
    "    mlflow.log_artifact(pca_img, artifact_path=\"visualization\")\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ 7) K-Means clustering ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    n_clusters = len(le.classes_)\n",
    "    mlflow.log_param(\"n_clusters\", n_clusters)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(X)\n",
    "    df['Cluster'] = clusters\n",
    "\n",
    "    clustered_csv = '../Embeddings/papc_clustered_embeddings.csv'\n",
    "    os.makedirs(os.path.dirname(clustered_csv), exist_ok=True)\n",
    "    df.to_csv(clustered_csv, index=False)\n",
    "    mlflow.log_artifact(clustered_csv, artifact_path=\"clustered_data\")\n",
    "    print(f\"‚úÖ Clustered data saved: {clustered_csv}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='tab10', alpha=0.7)\n",
    "    plt.title(\"K-Means Clusters of PAPC Embeddings\")\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.tight_layout()\n",
    "    cluster_img = \"papc_clusters.png\"\n",
    "    plt.savefig(cluster_img)\n",
    "    plt.close()\n",
    "    mlflow.log_artifact(cluster_img, artifact_path=\"visualization\")\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ 8) Log end time ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    end_ts = datetime.now(timezone.utc)\n",
    "    mlflow.log_param(\"end_time\", end_ts.isoformat())\n",
    "    print(f\"‚úÖ Run finished at {end_ts.isoformat()} (Duration: {end_ts - start_ts})\")\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ 9) Print final classification report ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    report = classification_report(y_encoded, preds, target_names=le.classes_, zero_division=0)\n",
    "    print(\"\\nClassification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174d6c46-cbed-44b1-acd1-f36a71195fca",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">\n",
    "\n",
    "# **Step 10: Save Visualization and Evaluation of Model** <a class=\"anchor\" id=\"PAPC_page_3\"></a>\n",
    "\n",
    "[Back to Top](#PAPC_toc)\n",
    "\n",
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b7e7cb8-ba04-4b99-b982-7dde60729478",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/26 15:50:13 INFO mlflow.tracking.fluent: Experiment with name 'PAPC_PCA_and_Clustering' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded embeddings from: ../Embeddings/papc_embeddings.csv\n",
      "‚úÖ LabelEncoder loaded from: ../Models/papc_label_encoder.pkl\n",
      "‚úÖ RF model loaded from: ../Models/papc_rf_model.pkl\n",
      "‚è≥ Run started at 2025-05-26T14:50:14.103350+00:00\n",
      "‚úÖ PCA plot saved: papc_pca.png\n",
      "‚úÖ Clustered data saved: ../Embeddings/papc_clustered_embeddings.csv\n",
      "‚úÖ Cluster plot saved: papc_clusters.png\n",
      "‚úÖ Run finished at 2025-05-26T14:50:14.269887+00:00 (Duration: 0:00:00.166537)\n",
      "üèÉ View run PAPC_PCA_and_Clustering at: http://127.0.0.1:5000/#/experiments/776802755119594055/runs/4a0670b3d3c5472998b220606749f738\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/776802755119594055\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timezone\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import joblib\n",
    "import mlflow\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Setup MLflow ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "load_dotenv(override=True)\n",
    "if os.getenv(\"MLFLOW_TRACKING_URI\"):\n",
    "    mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\"))\n",
    "mlflow.set_experiment(\"PAPC_PCA_and_Clustering\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 1) Paths ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "EMBEDDINGS_CSV    = '../Embeddings/papc_embeddings.csv'\n",
    "LABEL_ENCODER_PKL = '../Models/papc_label_encoder.pkl'\n",
    "RF_MODEL_PKL      = '../Models/papc_rf_model.pkl'\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 2) Load embeddings with safe parsing ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "try:\n",
    "    df = pd.read_csv(\n",
    "        EMBEDDINGS_CSV,\n",
    "        converters={'Embedding': lambda s: np.array(ast.literal_eval(s)) if isinstance(s, str) else np.array(s)}\n",
    "    )\n",
    "    print(f\"‚úÖ Loaded embeddings from: {EMBEDDINGS_CSV}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Embeddings CSV not found: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 3) Load label‚Äêencoder & model (logged for traceability) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "try:\n",
    "    label_encoder = joblib.load(LABEL_ENCODER_PKL)\n",
    "    print(f\"‚úÖ LabelEncoder loaded from: {LABEL_ENCODER_PKL}\")\n",
    "    rf_model = joblib.load(RF_MODEL_PKL)\n",
    "    print(f\"‚úÖ RF model loaded from: {RF_MODEL_PKL}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Artifact missing: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 4) Prepare data ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "X = np.stack(df['Embedding'].values)\n",
    "y = df['Factor'].values\n",
    "y_encoded = label_encoder.transform(y)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 5) Begin MLflow run ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "with mlflow.start_run(run_name=\"PAPC_PCA_and_Clustering\"):\n",
    "    mlflow.log_param(\"step\", \"PCA_and_KMeans\")\n",
    "    mlflow.log_param(\"embeddings_csv\", EMBEDDINGS_CSV)\n",
    "    start_ts = datetime.now(timezone.utc)\n",
    "    mlflow.log_param(\"start_time\", start_ts.isoformat())\n",
    "    print(f\"‚è≥ Run started at {start_ts.isoformat()}\")\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ 6) PCA visualization ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_encoded, cmap='viridis', alpha=0.7)\n",
    "    handles, _ = scatter.legend_elements()\n",
    "    plt.legend(handles, label_encoder.classes_, title=\"PAPC Factor\")\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.title(\"PCA of PAPC Embeddings\")\n",
    "    plt.tight_layout()\n",
    "    pca_img = \"papc_pca.png\"\n",
    "    plt.savefig(pca_img)\n",
    "    plt.close()\n",
    "    mlflow.log_artifact(pca_img, artifact_path=\"visualization\")\n",
    "    print(f\"‚úÖ PCA plot saved: {pca_img}\")\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ 7) K-Means clustering ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    n_clusters = len(label_encoder.classes_)\n",
    "    mlflow.log_param(\"n_clusters\", n_clusters)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(X)\n",
    "    df['Cluster'] = clusters\n",
    "\n",
    "    clustered_csv = '../Embeddings/papc_clustered_embeddings.csv'\n",
    "    os.makedirs(os.path.dirname(clustered_csv), exist_ok=True)\n",
    "    df.to_csv(clustered_csv, index=False)\n",
    "    mlflow.log_artifact(clustered_csv, artifact_path=\"clustered_data\")\n",
    "    print(f\"‚úÖ Clustered data saved: {clustered_csv}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='tab10', alpha=0.7)\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.title(\"K-Means Clusters of PAPC Embeddings\")\n",
    "    plt.tight_layout()\n",
    "    cluster_img = \"papc_clusters.png\"\n",
    "    plt.savefig(cluster_img)\n",
    "    plt.close()\n",
    "    mlflow.log_artifact(cluster_img, artifact_path=\"visualization\")\n",
    "    print(f\"‚úÖ Cluster plot saved: {cluster_img}\")\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ 8) End run ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    end_ts = datetime.now(timezone.utc)\n",
    "    mlflow.log_param(\"end_time\", end_ts.isoformat())\n",
    "    duration = end_ts - start_ts\n",
    "    print(f\"‚úÖ Run finished at {end_ts.isoformat()} (Duration: {duration})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013ba63c-ce13-4667-a5d9-a799ab883b97",
   "metadata": {},
   "source": [
    "This block of code integrates several stages that not only include training but also applying the model to new data and exploring the data through clustering:\n",
    "1. **Data Loading and Feature Parsing**: Similar to Block 1, with an additional step of displaying the parsed data.\n",
    "2. **Model Creation and Logging**: Training a RandomForestClassifier and logging the model directly with MLflow for possibly immediate deployment.\n",
    "3. **Model Evaluation and Reporting**: Assessing model performance with metrics and detailed reports, and logging these evaluations.\n",
    "4. **Clustering Analysis**: Utilizing KMeans to perform clustering on the embeddings, which adds an exploratory data analysis component.\n",
    "5. **Model Application on New Data**: Demonstrating a practical application of the trained model to predict factors for new text inputs.\n",
    "6. **End-to-End Experiment Tracking**: From the beginning of the run to its completion, tracking all parameters, artifacts, and outcomes, emphasizing a full-cycle view of the modeling process.\n",
    "\n",
    "This provides a broader overview of how a model can be developed and applied within a workflow that includes prediction and clustering alongside the fundamental steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f3cfce-24bd-4f6c-a748-54b1fe39d107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5916893-8b0f-450f-9ce6-09015901f5eb",
   "metadata": {},
   "source": [
    "The results show that the RandomForestClassifier model trained on the dataset of embeddings has achieved an accuracy of 1.0 on the test set, which means it has correctly classified all the test samples. Here is the breakdown of the evaluation:\n",
    "\n",
    "### Accuracy:\n",
    "- **1.0**: The model has 100% accuracy, meaning it correctly classified every instance in the test set.\n",
    "\n",
    "### Classification Report:\n",
    "- **Precision, Recall, and F1-score** for each class (0 through 4) are all 1.00.\n",
    "- **Support** indicates the number of actual occurrences of each class in the test set.\n",
    "\n",
    "### InterprePAPCion:\n",
    "- **Precision**: This is the ratio of true positive predictions to the total predicted positives. A precision of 1.0 means that all instances predicted as a specific class were actually of that class.\n",
    "- **Recall**: This is the ratio of true positive predictions to the total actual positives. A recall of 1.0 means that all actual instances of a specific class were correctly predicted.\n",
    "- **F1-score**: This is the harmonic mean of precision and recall. An F1-score of 1.0 indicates perfect precision and recall.\n",
    "- **Support**: This indicates the number of true instances for each label in the test set. \n",
    "\n",
    "### Considerations:\n",
    "1. **Model Overfitting**: The perfect score could indicate overfitting, especially if the test set is small or not represenPAPCive of unseen data.\n",
    "2. **Test Set Size**: The test set has only 24 samples, which is relatively small. It's important to ensure that the test set is large enough and represenPAPCive to get a reliable estimate of model performance.\n",
    "3. **Data Leakage**: Double-check that there's no data leakage, meaning that no information from the test set was used during training.\n",
    "4. **Cross-Validation**: To better assess the model's performance, consider using cross-validation to ensure the model performs well across different subsets of the data.\n",
    "\n",
    "### Next Steps:\n",
    "- **Cross-validation**: Implement cross-validation to get a more robust evaluation of model performance.\n",
    "- **Larger Test Set**: If possible, increase the size of the test set to ensure the performance metrics are reliable.\n",
    "- **Feature Analysis**: Examine feature importance scores from the RandomForestClassifier to understand which parts of the embeddings contribute most to the predictions.\n",
    "\n",
    "### Updated Code for Cross-Validation:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(clf, X, y_encoded, cv=5)  # 5-fold cross-validation\n",
    "print(f\"Cross-validation scores: {cv_scores}\")\n",
    "print(f\"Mean cross-validation score: {np.mean(cv_scores)}\")\n",
    "```\n",
    "\n",
    "We added this cross-validation step will help us verify that the model generalizes well and is not just performing well on a small or potentially non-represenPAPCive test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16fac8dd-46d3-4595-99f0-f15b83e244a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/26 15:51:59 INFO mlflow.tracking.fluent: Experiment with name 'PAPC_LOO_CV' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 72 PAPC embeddings from ../Embeddings/papc_embeddings.csv\n",
      "‚úÖ Loaded PAPC label encoder and RF model\n",
      "‚ñ∂ Class counts (code ‚Üí factor):\n",
      "  0 (Health): 15 samples\n",
      "  1 (Lifestyle): 15 samples\n",
      "  2 (Person): 18 samples\n",
      "  3 (Psychological Characteristics): 12 samples\n",
      "  4 (Social Factors): 12 samples\n",
      "‚ñ∂ Running Leave-One-Out CV with 72 splits\n",
      "‚ñ∂ First 10 LOO accuracies: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]‚Ä¶\n",
      "‚ñ∂ Mean LOO accuracy: 1.0000\n",
      "‚ñ∂ Std  LOO accuracy:  0.0000\n",
      "üèÉ View run PAPC_LOO_CV at: http://127.0.0.1:5000/#/experiments/701006941915742777/runs/b35781feba674a00a7472691ee107d7f\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/701006941915742777\n",
      "‚úÖ PAPC Leave-One-Out CV complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import mlflow\n",
    "from sklearn.model_selection import LeaveOneOut, cross_val_score\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Setup MLflow ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Assumes MLFLOW_TRACKING_URI is set and experiment created\n",
    "mlflow.set_experiment(\"PAPC_LOO_CV\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 1) Load embeddings ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "embeddings_csv = '../Embeddings/papc_embeddings.csv'\n",
    "df = pd.read_csv(\n",
    "    embeddings_csv,\n",
    "    converters={'Embedding': lambda s: np.array(ast.literal_eval(s)) if isinstance(s, str) else np.array(s)}\n",
    ")\n",
    "X = np.stack(df['Embedding'].values)\n",
    "print(f\"‚úÖ Loaded {X.shape[0]} PAPC embeddings from {embeddings_csv}\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 2) Load label encoder & model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "label_encoder = joblib.load('../Models/papc_label_encoder.pkl')\n",
    "clf           = joblib.load('../Models/papc_rf_model.pkl')\n",
    "print(\"‚úÖ Loaded PAPC label encoder and RF model\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 3) Prepare labels ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "y = df['Factor'].values\n",
    "y_encoded = label_encoder.transform(y)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 4) Inspect class distribution ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "dist = pd.Series(y_encoded).value_counts().sort_index()\n",
    "print(\"‚ñ∂ Class counts (code ‚Üí factor):\")\n",
    "for code, cnt in dist.items():\n",
    "    factor = label_encoder.inverse_transform([code])[0]\n",
    "    print(f\"  {code} ({factor}): {cnt} samples\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 5) Leave-One-Out CV ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "loo = LeaveOneOut()\n",
    "n_splits = loo.get_n_splits(X)\n",
    "print(f\"‚ñ∂ Running Leave-One-Out CV with {n_splits} splits\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 6) Run CV and log in MLflow ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "with mlflow.start_run(run_name=\"PAPC_LOO_CV\"):\n",
    "    mlflow.log_param(\"cv_method\", \"LeaveOneOut\")\n",
    "    mlflow.log_param(\"n_splits\", n_splits)\n",
    "\n",
    "    cv_scores = cross_val_score(\n",
    "        clf,\n",
    "        X,\n",
    "        y_encoded,\n",
    "        cv=loo,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ 7) Summarize & print ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    mean_acc = cv_scores.mean()\n",
    "    std_acc  = cv_scores.std()\n",
    "    print(f\"‚ñ∂ First 10 LOO accuracies: {cv_scores[:10]}‚Ä¶\")\n",
    "    print(f\"‚ñ∂ Mean LOO accuracy: {mean_acc:.4f}\")\n",
    "    print(f\"‚ñ∂ Std  LOO accuracy:  {std_acc:.4f}\")\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ 8) Log metrics ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    mlflow.log_metric(\"loo_mean_accuracy\", mean_acc)\n",
    "    mlflow.log_metric(\"loo_std_accuracy\", std_acc)\n",
    "\n",
    "print(\"‚úÖ PAPC Leave-One-Out CV complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adf2f5ea-3181-4f0f-8a62-c95784cac78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 72 PAPC embeddings from ../Embeddings/papc_embeddings.csv\n",
      "PAPC factors (5): ['Health', 'Lifestyle', 'Person', 'Psychological Characteristics', 'Social Factors']\n",
      "Train/Test split: 43 train / 29 test samples\n",
      "‚úÖ Trained RandomForest on PAPC factors\n",
      "Test accuracy: 1.000\n",
      "‚úÖ Confusion matrix saved to papc_confusion_matrix.png\n",
      "\n",
      "Classification Report:\n",
      "                               precision    recall  f1-score   support\n",
      "\n",
      "                       Health       1.00      1.00      1.00         6\n",
      "                    Lifestyle       1.00      1.00      1.00         6\n",
      "                       Person       1.00      1.00      1.00         7\n",
      "Psychological Characteristics       1.00      1.00      1.00         5\n",
      "               Social Factors       1.00      1.00      1.00         5\n",
      "\n",
      "                     accuracy                           1.00        29\n",
      "                    macro avg       1.00      1.00      1.00        29\n",
      "                 weighted avg       1.00      1.00      1.00        29\n",
      "\n",
      "‚ñ∂ Using 5-fold StratifiedKFold CV\n",
      "\n",
      "CV accuracy scores: [1. 1. 1. 1. 1.]\n",
      "Mean CV accuracy: 1.000 ¬± 0.000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 1) Load embeddings safely ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "embeddings_path = '../Embeddings/papc_embeddings.csv'\n",
    "try:\n",
    "    df = pd.read_csv(\n",
    "        embeddings_path,\n",
    "        converters={'Embedding': lambda s: np.array(ast.literal_eval(s)) if isinstance(s, str) else np.array(s)}\n",
    "    )\n",
    "    print(f\"‚úÖ Loaded {len(df)} PAPC embeddings from {embeddings_path}\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"Embeddings file not found at {embeddings_path}\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 2) Prepare features & labels ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "X = np.stack(df['Embedding'].values)\n",
    "y = df['Factor'].values\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "print(f\"PAPC factors ({len(le.classes_)}): {list(le.classes_)}\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 3) Train/test split ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded,\n",
    "    test_size=0.4,\n",
    "    random_state=42,\n",
    "    stratify=y_encoded\n",
    ")\n",
    "print(f\"Train/Test split: {X_train.shape[0]} train / {X_test.shape[0]} test samples\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 4) Train RandomForest ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"‚úÖ Trained RandomForest on PAPC factors\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 5) Evaluate on hold-out set ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test accuracy: {acc:.3f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_df = pd.DataFrame(cm, index=le.classes_, columns=le.classes_)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"PAPC Confusion Matrix (Test Set)\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.tight_layout()\n",
    "cm_file = \"papc_confusion_matrix.png\"\n",
    "plt.savefig(cm_file)\n",
    "plt.close()\n",
    "print(f\"‚úÖ Confusion matrix saved to {cm_file}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    y_test, y_pred,\n",
    "    target_names=le.classes_,\n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 6) Cross-Validation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Choose StratifiedKFold unless some classes have fewer than 5 samples\n",
    "min_count = pd.Series(y_encoded).value_counts().min()\n",
    "if min_count < 5:\n",
    "    print(f\"‚ö†Ô∏è  Least-populated class has only {min_count} samples; consider LeaveOneOut CV instead.\")\n",
    "    cv = LeaveOneOut()\n",
    "else:\n",
    "    print(\"‚ñ∂ Using 5-fold StratifiedKFold CV\")\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_scores = cross_val_score(clf, X, y_encoded, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "print(f\"\\nCV accuracy scores: {np.round(cv_scores, 3)}\")\n",
    "print(f\"Mean CV accuracy: {cv_scores.mean():.3f} ¬± {cv_scores.std():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d8667c-30e8-4db0-8fca-03ccb53d773e",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">\n",
    "\n",
    "# **Step 11: Test Model Directly** <a class=\"anchor\" id=\"PAPC_page_3\"></a>\n",
    "\n",
    "[Back to Top](#PAPC_toc)\n",
    "\n",
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d8b6c39-c12d-4076-95e4-692d7a89371d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 72 rows from ../Embeddings/papc_embeddings.csv\n",
      "‚úÖ Loaded label encoder and RandomForest model.\n",
      "\n",
      "--- PAPC Model Predictions on New Examples ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identity Management\n",
      "  Input: ‚ÄúEach user is assigned a unique key to track records across the system.‚Äù\n",
      "  ‚Üí Predicted PAPC Factor: Person\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical Health Monitoring\n",
      "  Input: ‚ÄúI measure my weight and blood pressure every morning before work.‚Äù\n",
      "  ‚Üí Predicted PAPC Factor: Health\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitness Activity\n",
      "  Input: ‚ÄúI go for a 5km run three times a week and log my workout metrics.‚Äù\n",
      "  ‚Üí Predicted PAPC Factor: Health\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Educational Attainment\n",
      "  Input: ‚ÄúI completed my master‚Äôs degree in computer science last year.‚Äù\n",
      "  ‚Üí Predicted PAPC Factor: Psychological Characteristics\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Social Support Network\n",
      "  Input: ‚ÄúI rely on friends and family for emotional support during stressful periods.‚Äù\n",
      "  ‚Üí Predicted PAPC Factor: Social Factors\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from openai import OpenAI, OpenAIError\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 1) Bootstrap OpenAI client ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "load_dotenv(override=True)\n",
    "\n",
    "def _get_api_key_from_file(path=\"~/openai_api_key.txt\"):\n",
    "    p = os.path.expanduser(path)\n",
    "    try:\n",
    "        key = open(p).read().strip()\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"API key file not found at {p}\")\n",
    "    if not key:\n",
    "        raise ValueError(f\"No API key found in {p}\")\n",
    "    return key\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\") or _get_api_key_from_file()\n",
    "client  = OpenAI(api_key=api_key)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 2) Load PAPC embeddings ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "EMB_PATH = '../Embeddings/papc_embeddings.csv'\n",
    "try:\n",
    "    df = pd.read_csv(\n",
    "        EMB_PATH,\n",
    "        converters={'Embedding': lambda s: np.array(ast.literal_eval(s)) if isinstance(s, str) else np.array(s)}\n",
    "    )\n",
    "    print(f\"‚úÖ Loaded {len(df)} rows from {EMB_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load embeddings: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 3) Load artifacts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "ENCODER_PATH = '../Models/papc_label_encoder.pkl'\n",
    "MODEL_PATH   = '../Models/papc_rf_model.pkl'\n",
    "try:\n",
    "    label_encoder = joblib.load(ENCODER_PATH)\n",
    "    clf           = joblib.load(MODEL_PATH)\n",
    "    print(\"‚úÖ Loaded label encoder and RandomForest model.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load artifacts: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 4) Inference helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def get_embedding(text: str, model: str = \"text-embedding-3-small\") -> list:\n",
    "    try:\n",
    "        resp = client.embeddings.create(input=[text], model=model)\n",
    "        return resp.data[0].embedding\n",
    "    except OpenAIError as e:\n",
    "        raise RuntimeError(f\"OpenAI embedding error: {e}\")\n",
    "\n",
    "def predict_papc_factor(text: str) -> str:\n",
    "    emb  = get_embedding(text)\n",
    "    code = clf.predict([emb])[0]\n",
    "    return label_encoder.inverse_transform([code])[0]\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 5) Test on representative PAPC examples ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "examples = {\n",
    "    \"Identity Management\":       \"Each user is assigned a unique key to track records across the system.\",\n",
    "    \"Physical Health Monitoring\":\"I measure my weight and blood pressure every morning before work.\",\n",
    "    \"Fitness Activity\":          \"I go for a 5km run three times a week and log my workout metrics.\",\n",
    "    \"Educational Attainment\":    \"I completed my master‚Äôs degree in computer science last year.\",\n",
    "    \"Social Support Network\":    \"I rely on friends and family for emotional support during stressful periods.\",\n",
    "}\n",
    "\n",
    "print(\"\\n--- PAPC Model Predictions on New Examples ---\\n\")\n",
    "for desc, sample in examples.items():\n",
    "    try:\n",
    "        pred = predict_papc_factor(sample)\n",
    "        print(f\"{desc}\\n  Input: ‚Äú{sample}‚Äù\\n  ‚Üí Predicted PAPC Factor: {pred}\\n\")\n",
    "    except Exception as ex:\n",
    "        print(f\"‚ùå Error on '{desc}': {ex}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab54c21-ee36-4c0e-b9c4-445b11fa65ed",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">\n",
    "\n",
    "# **Step 12: Test Neo4j Connection** <a class=\"anchor\" id=\"PAPC_page_4\"></a>\n",
    "\n",
    "\n",
    "[Back to Top](#PAPC_toc)\n",
    "\n",
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce3fd307-5fcf-46d7-867d-78ecd5a83887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://localhost:7474/browser/\" target=\"_blank\">üîó Open Neo4j Bolt Connection</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import webbrowser\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Define Neo4j Browser URL\n",
    "neo4j_browser_url = \"http://localhost:7474/browser/\"\n",
    "\n",
    "# Create a clickable link\n",
    "html_code = f'<a href=\"{neo4j_browser_url}\" target=\"_blank\">üîó Open Neo4j Bolt Connection</a>'\n",
    "\n",
    "# Display the clickable link in Jupyter Notebook\n",
    "display(HTML(html_code))\n",
    "\n",
    "# Open the Neo4j Browser in a new tab automatically\n",
    "webbrowser.open_new_tab(neo4j_browser_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a12796f0-11cb-40cb-8c48-e9e20760ed42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Bolt port 7687 is reachable!\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "HOST = \"localhost\"\n",
    "PORT = 7687\n",
    "\n",
    "sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "result = sock.connect_ex((HOST, PORT))\n",
    "\n",
    "if result == 0:\n",
    "    print(f\"‚úÖ Bolt port {PORT} is reachable!\")\n",
    "else:\n",
    "    print(f\"‚ùå Bolt port {PORT} is NOT reachable!\")\n",
    "\n",
    "sock.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3abeb19a-7681-4c01-bb52-3b567513142a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Python Connection Failed: Unknown protocol 'neo4j'\n"
     ]
    }
   ],
   "source": [
    "from py2neo import Graph\n",
    "import mlflow\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ End any active MLflow run ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Original URI (using neo4j://) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "raw_uri      = \"neo4j://localhost:7687\"\n",
    "NEO4J_USER   = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"mypassword\"\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Rewrite to bolt:// for py2neo ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "if raw_uri.startswith(\"neo4j://\"):\n",
    "    uri = \"bolt://\" + raw_uri[len(\"neo4j://\"):]\n",
    "else:\n",
    "    uri = raw_uri\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Attempt connection ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "try:\n",
    "    graph = Graph(uri, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "    # run a simple test query\n",
    "    message = graph.run(\"RETURN 'Connection successful!' AS message\").evaluate()\n",
    "    print(\"‚úÖ Python Connected Successfully:\", message)\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Python Connection Failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1142a99b-5cc2-4917-b423-5b8d52a45c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Connecting to Bolt URI: bolt://localhost:7687\n",
      "‚ùå Connection failed: Unknown protocol 'neo4j'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown protocol 'neo4j'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# 4) Attempt connection\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m     graph \u001b[38;5;241m=\u001b[39m \u001b[43mGraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbolt_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpwd\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     greeting \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124müéâ Bolt connection successful!\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m AS msg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ\u001b[39m\u001b[38;5;124m\"\u001b[39m, greeting)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/py2neo/database.py:288\u001b[0m, in \u001b[0;36mGraph.__init__\u001b[0;34m(self, profile, name, **settings)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, profile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msettings):\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice \u001b[38;5;241m=\u001b[39m \u001b[43mGraphService\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m=\u001b[39m name\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema \u001b[38;5;241m=\u001b[39m Schema(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/py2neo/database.py:119\u001b[0m, in \u001b[0;36mGraphService.__init__\u001b[0;34m(self, profile, **settings)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connector_settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit_size\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m profile\u001b[38;5;241m.\u001b[39mrouting:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Ensures credentials are checked on construction\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     connector_settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit_size\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connector \u001b[38;5;241m=\u001b[39m \u001b[43mConnector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconnector_settings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graphs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/py2neo/client/__init__.py:948\u001b[0m, in \u001b[0;36mConnector.__init__\u001b[0;34m(self, profile, user_agent, init_size, max_size, max_age, routing_refresh_ttl)\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, profile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, user_agent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, init_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    946\u001b[0m              max_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, max_age\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, routing_refresh_ttl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile \u001b[38;5;241m=\u001b[39m ServiceProfile(profile)\n\u001b[0;32m--> 948\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_routers \u001b[38;5;241m=\u001b[39m [\u001b[43mConnectionProfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user_agent \u001b[38;5;241m=\u001b[39m user_agent\n\u001b[1;32m    950\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server_agent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/py2neo/__init__.py:176\u001b[0m, in \u001b[0;36mConnectionProfile.__init__\u001b[0;34m(self, profile, **settings)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__password \u001b[38;5;241m=\u001b[39m DEFAULT_PASSWORD\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__address \u001b[38;5;241m=\u001b[39m Address\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 176\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_env_vars\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/py2neo/__init__.py:220\u001b[0m, in \u001b[0;36mConnectionProfile._apply_env_vars\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_apply_env_vars\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m NEO4J_URI:\n\u001b[0;32m--> 220\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNEO4J_URI\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m NEO4J_AUTH:\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_settings(auth\u001b[38;5;241m=\u001b[39mNEO4J_AUTH)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/py2neo/__init__.py:232\u001b[0m, in \u001b[0;36mConnectionProfile._apply_uri\u001b[0;34m(self, uri)\u001b[0m\n\u001b[1;32m    230\u001b[0m parsed \u001b[38;5;241m=\u001b[39m urlsplit(uri)\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parsed\u001b[38;5;241m.\u001b[39mscheme \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 232\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_scheme\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparsed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheme\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m@\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m parsed\u001b[38;5;241m.\u001b[39mnetloc:\n\u001b[1;32m    234\u001b[0m     settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maddress\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m parsed\u001b[38;5;241m.\u001b[39mnetloc\u001b[38;5;241m.\u001b[39mpartition(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m@\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/py2neo/__init__.py:249\u001b[0m, in \u001b[0;36mConnectionProfile._apply_scheme\u001b[0;34m(self, scheme)\u001b[0m\n\u001b[1;32m    247\u001b[0m     protocol, _, ext \u001b[38;5;241m=\u001b[39m scheme\u001b[38;5;241m.\u001b[39mpartition(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_settings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprotocol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msecure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m ext \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_settings(protocol\u001b[38;5;241m=\u001b[39mprotocol, secure\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/py2neo/__init__.py:266\u001b[0m, in \u001b[0;36mConnectionProfile._apply_settings\u001b[0;34m(self, uri, scheme, protocol, secure, verify, address, host, port, port_number, auth, user, password, **other)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_scheme(scheme)\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol:\n\u001b[0;32m--> 266\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_protocol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m secure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__secure \u001b[38;5;241m=\u001b[39m secure\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/py2neo/__init__.py:297\u001b[0m, in \u001b[0;36mConnectionProfile._apply_protocol\u001b[0;34m(self, protocol)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_apply_protocol\u001b[39m(\u001b[38;5;28mself\u001b[39m, protocol):\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbolt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 297\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown protocol \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m protocol)\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__protocol \u001b[38;5;241m=\u001b[39m protocol\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown protocol 'neo4j'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from py2neo import Graph\n",
    "import mlflow\n",
    "\n",
    "# 1) Tear down any active MLflow run\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "# 2) Load env\n",
    "load_dotenv(override=True)\n",
    "raw_uri = os.getenv(\"NEO4J_URI\", \"\")\n",
    "user    = os.getenv(\"NEO4J_USERNAME\")\n",
    "pwd     = os.getenv(\"NEO4J_PASSWORD\")\n",
    "\n",
    "if not raw_uri:\n",
    "    raise RuntimeError(\"‚ùå NEO4J_URI is not set in .env\")\n",
    "if not user or not pwd:\n",
    "    raise RuntimeError(\"‚ùå NEO4J_USERNAME or NEO4J_PASSWORD not set in .env\")\n",
    "\n",
    "# 3) Rewrite protocol\n",
    "if raw_uri.startswith(\"neo4j://\"):\n",
    "    bolt_uri = \"bolt://\" + raw_uri[len(\"neo4j://\"):]\n",
    "else:\n",
    "    bolt_uri = raw_uri\n",
    "\n",
    "print(f\"üîç Connecting to Bolt URI: {bolt_uri}\")\n",
    "\n",
    "# 4) Attempt connection\n",
    "try:\n",
    "    graph = Graph(bolt_uri, auth=(user, pwd))\n",
    "    greeting = graph.run(\"RETURN 'üéâ Bolt connection successful!' AS msg\").evaluate()\n",
    "    print(\"‚úÖ\", greeting)\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Connection failed:\", e)\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54f9b222-dd9d-4205-b4ab-3221085fbe7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection failed: Unknown protocol 'neo4j'\n",
      "üèÉ View run Test Neo4j Connection at: http://127.0.0.1:5000/#/experiments/886681214121108750/runs/8f80dd79628a4cda85587faa4064eb8b\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/886681214121108750\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from py2neo import Graph\n",
    "import mlflow\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Load environment ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "load_dotenv()\n",
    "raw_uri       = os.getenv(\"NEO4J_URI\", \"\")\n",
    "NEO4J_USER    = os.getenv(\"NEO4J_USERNAME\")\n",
    "NEO4J_PASSWORD= os.getenv(\"NEO4J_PASSWORD\")\n",
    "MLFLOW_URI    = os.getenv(\"MLFLOW_TRACKING_URI\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Fix URI for py2neo ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "if raw_uri.startswith(\"neo4j://\"):\n",
    "    bolt_uri = \"bolt://\" + raw_uri[len(\"neo4j://\"):]\n",
    "else:\n",
    "    bolt_uri = raw_uri\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Configure MLflow ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "mlflow.set_tracking_uri(MLFLOW_URI)\n",
    "mlflow.set_experiment(\"PAPC\")\n",
    "\n",
    "def test_neo4j_connection(uri, user, pwd):\n",
    "    graph = Graph(uri, auth=(user, pwd))\n",
    "    return graph.run(\"RETURN 'Connection successful!' AS greeting\").evaluate()\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Run the connection test under MLflow ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "with mlflow.start_run(run_name=\"Test Neo4j Connection\"):\n",
    "    mlflow.log_param(\"Test\", \"Neo4j Connection\")\n",
    "    \n",
    "    try:\n",
    "        result = test_neo4j_connection(bolt_uri, NEO4J_USER, NEO4J_PASSWORD)\n",
    "    except Exception as e:\n",
    "        result = f\"Connection failed: {e}\"\n",
    "    \n",
    "    mlflow.log_param(\"Connection Result\", result)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0ce307e-9ad1-4891-9f4f-8966af9ce07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run Test & Clear Neo4j at: http://127.0.0.1:5000/#/experiments/616263351584470447/runs/dba3a670164f4a5a916cafc8a2cc1914\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/616263351584470447\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown protocol 'neo4j'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# ‚îÄ‚îÄ‚îÄ Test & Clear Neo4j inside an MLflow run ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mlflow\u001b[38;5;241m.\u001b[39mstart_run(run_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest & Clear Neo4j\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# 1) Test connection\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtest_neo4j_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNEO4J_URI\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNEO4J_USER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNEO4J_PASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mlog_param(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection Test\u001b[39m\u001b[38;5;124m\"\u001b[39m, result)\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müîó Connection test result: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 26\u001b[0m, in \u001b[0;36mtest_neo4j_connection\u001b[0;34m(uri, user, password)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtest_neo4j_connection\u001b[39m(uri, user, password):\n\u001b[0;32m---> 26\u001b[0m     graph \u001b[38;5;241m=\u001b[39m \u001b[43mGraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConnection successful!\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m AS greeting\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/py2neo/database.py:288\u001b[0m, in \u001b[0;36mGraph.__init__\u001b[0;34m(self, profile, name, **settings)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, profile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msettings):\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice \u001b[38;5;241m=\u001b[39m \u001b[43mGraphService\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m=\u001b[39m name\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema \u001b[38;5;241m=\u001b[39m Schema(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/py2neo/database.py:119\u001b[0m, in \u001b[0;36mGraphService.__init__\u001b[0;34m(self, profile, **settings)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connector_settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit_size\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m profile\u001b[38;5;241m.\u001b[39mrouting:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Ensures credentials are checked on construction\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     connector_settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit_size\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connector \u001b[38;5;241m=\u001b[39m \u001b[43mConnector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconnector_settings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graphs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/py2neo/client/__init__.py:948\u001b[0m, in \u001b[0;36mConnector.__init__\u001b[0;34m(self, profile, user_agent, init_size, max_size, max_age, routing_refresh_ttl)\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, profile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, user_agent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, init_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    946\u001b[0m              max_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, max_age\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, routing_refresh_ttl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile \u001b[38;5;241m=\u001b[39m ServiceProfile(profile)\n\u001b[0;32m--> 948\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_routers \u001b[38;5;241m=\u001b[39m [\u001b[43mConnectionProfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user_agent \u001b[38;5;241m=\u001b[39m user_agent\n\u001b[1;32m    950\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server_agent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/py2neo/__init__.py:176\u001b[0m, in \u001b[0;36mConnectionProfile.__init__\u001b[0;34m(self, profile, **settings)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__password \u001b[38;5;241m=\u001b[39m DEFAULT_PASSWORD\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__address \u001b[38;5;241m=\u001b[39m Address\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 176\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_env_vars\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/py2neo/__init__.py:220\u001b[0m, in \u001b[0;36mConnectionProfile._apply_env_vars\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_apply_env_vars\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m NEO4J_URI:\n\u001b[0;32m--> 220\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNEO4J_URI\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m NEO4J_AUTH:\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_settings(auth\u001b[38;5;241m=\u001b[39mNEO4J_AUTH)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/py2neo/__init__.py:232\u001b[0m, in \u001b[0;36mConnectionProfile._apply_uri\u001b[0;34m(self, uri)\u001b[0m\n\u001b[1;32m    230\u001b[0m parsed \u001b[38;5;241m=\u001b[39m urlsplit(uri)\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parsed\u001b[38;5;241m.\u001b[39mscheme \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 232\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_scheme\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparsed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheme\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m@\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m parsed\u001b[38;5;241m.\u001b[39mnetloc:\n\u001b[1;32m    234\u001b[0m     settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maddress\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m parsed\u001b[38;5;241m.\u001b[39mnetloc\u001b[38;5;241m.\u001b[39mpartition(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m@\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/py2neo/__init__.py:249\u001b[0m, in \u001b[0;36mConnectionProfile._apply_scheme\u001b[0;34m(self, scheme)\u001b[0m\n\u001b[1;32m    247\u001b[0m     protocol, _, ext \u001b[38;5;241m=\u001b[39m scheme\u001b[38;5;241m.\u001b[39mpartition(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_settings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprotocol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msecure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m ext \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_settings(protocol\u001b[38;5;241m=\u001b[39mprotocol, secure\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/py2neo/__init__.py:266\u001b[0m, in \u001b[0;36mConnectionProfile._apply_settings\u001b[0;34m(self, uri, scheme, protocol, secure, verify, address, host, port, port_number, auth, user, password, **other)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_scheme(scheme)\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol:\n\u001b[0;32m--> 266\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_protocol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m secure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__secure \u001b[38;5;241m=\u001b[39m secure\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/py2neo/__init__.py:297\u001b[0m, in \u001b[0;36mConnectionProfile._apply_protocol\u001b[0;34m(self, protocol)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_apply_protocol\u001b[39m(\u001b[38;5;28mself\u001b[39m, protocol):\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbolt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 297\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown protocol \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m protocol)\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__protocol \u001b[38;5;241m=\u001b[39m protocol\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown protocol 'neo4j'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from py2neo import Graph\n",
    "import mlflow\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Load environment variables ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "load_dotenv()\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Neo4j connection settings ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "raw_uri    = os.getenv(\"NEO4J_URI\")\n",
    "NEO4J_USER = os.getenv(\"NEO4J_USERNAME\")\n",
    "NEO4J_PASS = os.getenv(\"NEO4J_PASSWORD\")\n",
    "\n",
    "# Normalize URI: use bolt:// if someone set neo4j://\n",
    "if raw_uri and raw_uri.startswith(\"neo4j://\"):\n",
    "    NEO4J_URI = raw_uri.replace(\"neo4j://\", \"bolt://\", 1)\n",
    "else:\n",
    "    NEO4J_URI = raw_uri\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ MLflow tracking setup ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "MLFLOW_TRACKING_URI = os.getenv(\"MLFLOW_TRACKING_URI\")\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "mlflow.set_experiment(\"MCMI\")\n",
    "\n",
    "def test_neo4j_connection(uri, user, password):\n",
    "    graph = Graph(uri, auth=(user, password))\n",
    "    return graph.run(\"RETURN 'Connection successful!' AS greeting\").evaluate()\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Test & Clear Neo4j inside an MLflow run ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "with mlflow.start_run(run_name=\"Test & Clear Neo4j\"):\n",
    "    # 1) Test connection\n",
    "    result = test_neo4j_connection(NEO4J_URI, NEO4J_USER, NEO4J_PASS)\n",
    "    mlflow.log_param(\"Connection Test\", result)\n",
    "    print(f\"üîó Connection test result: {result}\")\n",
    "\n",
    "    # 2) Clear the entire database\n",
    "    graph = Graph(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASS))\n",
    "    graph.delete_all()\n",
    "    mlflow.log_param(\"Database Cleared\", True)\n",
    "    print(\"üóëÔ∏è  All nodes and relationships have been deleted.\")\n",
    "\n",
    "    # 3) Confirm it's empty\n",
    "    remaining = graph.run(\"MATCH (n) RETURN count(n) AS nodes\").evaluate()\n",
    "    mlflow.log_param(\"Remaining Nodes\", remaining)\n",
    "    print(f\"üìä Remaining node count after clear: {remaining}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a2d41d-2f56-4b50-b4b1-77ebcd2c7279",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">\n",
    "\n",
    "# **Step 13: Create PAPC Schema in Neo4j** <a class=\"anchor\" id=\"PAPC_page_4\"></a>\n",
    "\n",
    "\n",
    "[Back to Top](#PAPC_toc)\n",
    "\n",
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "957e5833-d44d-401d-8df7-0b430efac4f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown protocol 'neo4j'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSet NEO4J_URI, NEO4J_USERNAME & NEO4J_PASSWORD in .env\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# ‚îÄ‚îÄ‚îÄ 2) Connect & clear ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mGraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbolt_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mNEO4J_USER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNEO4J_PASS\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müîó Connected to Neo4j via \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbolt_uri\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m graph\u001b[38;5;241m.\u001b[39mdelete_all()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/py2neo/database.py:288\u001b[0m, in \u001b[0;36mGraph.__init__\u001b[0;34m(self, profile, name, **settings)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, profile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msettings):\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice \u001b[38;5;241m=\u001b[39m \u001b[43mGraphService\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m=\u001b[39m name\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema \u001b[38;5;241m=\u001b[39m Schema(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/py2neo/database.py:119\u001b[0m, in \u001b[0;36mGraphService.__init__\u001b[0;34m(self, profile, **settings)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connector_settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit_size\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m profile\u001b[38;5;241m.\u001b[39mrouting:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Ensures credentials are checked on construction\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     connector_settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit_size\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connector \u001b[38;5;241m=\u001b[39m \u001b[43mConnector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconnector_settings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graphs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/py2neo/client/__init__.py:948\u001b[0m, in \u001b[0;36mConnector.__init__\u001b[0;34m(self, profile, user_agent, init_size, max_size, max_age, routing_refresh_ttl)\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, profile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, user_agent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, init_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    946\u001b[0m              max_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, max_age\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, routing_refresh_ttl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile \u001b[38;5;241m=\u001b[39m ServiceProfile(profile)\n\u001b[0;32m--> 948\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_routers \u001b[38;5;241m=\u001b[39m [\u001b[43mConnectionProfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user_agent \u001b[38;5;241m=\u001b[39m user_agent\n\u001b[1;32m    950\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server_agent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/py2neo/__init__.py:176\u001b[0m, in \u001b[0;36mConnectionProfile.__init__\u001b[0;34m(self, profile, **settings)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__password \u001b[38;5;241m=\u001b[39m DEFAULT_PASSWORD\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__address \u001b[38;5;241m=\u001b[39m Address\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 176\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_env_vars\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/py2neo/__init__.py:220\u001b[0m, in \u001b[0;36mConnectionProfile._apply_env_vars\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_apply_env_vars\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m NEO4J_URI:\n\u001b[0;32m--> 220\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNEO4J_URI\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m NEO4J_AUTH:\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_settings(auth\u001b[38;5;241m=\u001b[39mNEO4J_AUTH)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/py2neo/__init__.py:232\u001b[0m, in \u001b[0;36mConnectionProfile._apply_uri\u001b[0;34m(self, uri)\u001b[0m\n\u001b[1;32m    230\u001b[0m parsed \u001b[38;5;241m=\u001b[39m urlsplit(uri)\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parsed\u001b[38;5;241m.\u001b[39mscheme \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 232\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_scheme\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparsed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheme\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m@\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m parsed\u001b[38;5;241m.\u001b[39mnetloc:\n\u001b[1;32m    234\u001b[0m     settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maddress\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m parsed\u001b[38;5;241m.\u001b[39mnetloc\u001b[38;5;241m.\u001b[39mpartition(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m@\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/py2neo/__init__.py:249\u001b[0m, in \u001b[0;36mConnectionProfile._apply_scheme\u001b[0;34m(self, scheme)\u001b[0m\n\u001b[1;32m    247\u001b[0m     protocol, _, ext \u001b[38;5;241m=\u001b[39m scheme\u001b[38;5;241m.\u001b[39mpartition(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_settings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprotocol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msecure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m ext \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_settings(protocol\u001b[38;5;241m=\u001b[39mprotocol, secure\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/py2neo/__init__.py:266\u001b[0m, in \u001b[0;36mConnectionProfile._apply_settings\u001b[0;34m(self, uri, scheme, protocol, secure, verify, address, host, port, port_number, auth, user, password, **other)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_scheme(scheme)\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol:\n\u001b[0;32m--> 266\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_protocol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m secure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__secure \u001b[38;5;241m=\u001b[39m secure\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/py2neo/__init__.py:297\u001b[0m, in \u001b[0;36mConnectionProfile._apply_protocol\u001b[0;34m(self, protocol)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_apply_protocol\u001b[39m(\u001b[38;5;28mself\u001b[39m, protocol):\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbolt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 297\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown protocol \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m protocol)\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__protocol \u001b[38;5;241m=\u001b[39m protocol\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown protocol 'neo4j'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from py2neo import Graph\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 1) Load environment & fix URI ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "load_dotenv(override=True)\n",
    "raw_uri = os.getenv(\"NEO4J_URI\", \"\")\n",
    "if raw_uri.startswith(\"neo4j://\"):\n",
    "    bolt_uri = \"bolt://\" + raw_uri[len(\"neo4j://\"):]\n",
    "else:\n",
    "    bolt_uri = raw_uri\n",
    "\n",
    "NEO4J_USER = os.getenv(\"NEO4J_USERNAME\")\n",
    "NEO4J_PASS = os.getenv(\"NEO4J_PASSWORD\")\n",
    "if not all([bolt_uri, NEO4J_USER, NEO4J_PASS]):\n",
    "    raise RuntimeError(\"Please set NEO4J_URI, NEO4J_USERNAME & NEO4J_PASSWORD in your .env\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 2) Connect & clear ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "graph = Graph(bolt_uri, auth=(NEO4J_USER, NEO4J_PASS))\n",
    "print(f\"üîó Connected to Neo4j via {bolt_uri}\")\n",
    "graph.delete_all()\n",
    "print(\"üóëÔ∏è  Cleared all existing nodes & relationships\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 3) Load the PAPC flattened CSV ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "csv_path = os.path.join(\"..\", \"Datasets\", \"papc.csv\")\n",
    "papc_df = pd.read_csv(csv_path)\n",
    "print(f\"‚úÖ Loaded {len(papc_df)} rows from {csv_path}\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 4) Build the PAPC taxonomy graph ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "for _, row in papc_df.iterrows():\n",
    "    factor    = row['Factor']\n",
    "    adjective = row['Adjective']\n",
    "    synonym   = row['Synonym']\n",
    "    verb      = row['Verb']\n",
    "    noun      = row['Noun']\n",
    "\n",
    "    # Merge Factor and Adjective nodes\n",
    "    graph.run(\"MERGE (f:PAPC_Factor {name: $factor})\", factor=factor)\n",
    "    graph.run(\"MERGE (a:PAPC_Adjective {name: $adjective})\", adjective=adjective)\n",
    "    graph.run(\n",
    "        \"\"\"\n",
    "        MATCH (f:PAPC_Factor {name:$factor}), (a:PAPC_Adjective {name:$adjective})\n",
    "        MERGE (f)-[:PAPC_HAS_ADJECTIVE]->(a)\n",
    "        \"\"\",\n",
    "        factor=factor, adjective=adjective\n",
    "    )\n",
    "\n",
    "    # Merge & link Synonym\n",
    "    if synonym:\n",
    "        graph.run(\"MERGE (s:PAPC_Synonym {name: $synonym})\", synonym=synonym)\n",
    "        graph.run(\n",
    "            \"\"\"\n",
    "            MATCH (a:PAPC_Adjective {name:$adjective}), (s:PAPC_Synonym {name:$synonym})\n",
    "            MERGE (a)-[:PAPC_HAS_SYNONYM]->(s)\n",
    "            \"\"\",\n",
    "            adjective=adjective, synonym=synonym\n",
    "        )\n",
    "\n",
    "    # Merge & link Verb\n",
    "    if verb:\n",
    "        graph.run(\"MERGE (v:PAPC_Verb {name: $verb})\", verb=verb)\n",
    "        graph.run(\n",
    "            \"\"\"\n",
    "            MATCH (a:PAPC_Adjective {name:$adjective}), (v:PAPC_Verb {name:$verb})\n",
    "            MERGE (a)-[:PAPC_HAS_VERB]->(v)\n",
    "            \"\"\",\n",
    "            adjective=adjective, verb=verb\n",
    "        )\n",
    "\n",
    "    # Merge & link Noun\n",
    "    if noun:\n",
    "        graph.run(\"MERGE (n:PAPC_Noun {name: $noun})\", noun=noun)\n",
    "        graph.run(\n",
    "            \"\"\"\n",
    "            MATCH (a:PAPC_Adjective {name:$adjective}), (n:PAPC_Noun {name:$noun})\n",
    "            MERGE (a)-[:PAPC_HAS_NOUN]->(n)\n",
    "            \"\"\",\n",
    "            adjective=adjective, noun=noun\n",
    "        )\n",
    "\n",
    "print(\"üéâ Completed building the PAPC taxonomy graph in Neo4j.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5da5e04-52b0-432d-9324-ba465af1865b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "746cc0fc-8e2f-4f59-b0e8-75b663e2802b",
   "metadata": {},
   "source": [
    "```cypher\n",
    "// 1) List all nodes with their labels and properties:\n",
    "MATCH (n)\n",
    "RETURN n\n",
    "ORDER BY labels(n), n.name;\n",
    "\n",
    "// 2) List all relationships between nodes:\n",
    "MATCH (a)-[r]->(b)\n",
    "RETURN a, r, b\n",
    "ORDER BY type(r);\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a55358-91a2-4b3f-8bc7-e22d822b975e",
   "metadata": {},
   "source": [
    "## Process for Building the PAPC Personality Model\n",
    "\n",
    "1. **Data Preparation**\n",
    "\n",
    "   * Define taxonomy (`Factor \\to Adjective \\to Synonym/Verb/Noun`).\n",
    "   * Normalize lists and flatten to CSV (`PAPC.csv`).\n",
    "\n",
    "2. **Embedding Generation**\n",
    "\n",
    "   * Load `PAPC.csv` into pandas.\n",
    "   * Initialize OpenAI client and MLflow.\n",
    "   * Generate embeddings via `client.embeddings.create(...)` for full text prompt.\n",
    "   * Save `PAPC_embeddings.csv` with `Embedding` column.\n",
    "\n",
    "3. **Label Encoding**\n",
    "\n",
    "   * Load embeddings CSV.\n",
    "   * Fit `LabelEncoder` on `Factor` column.\n",
    "   * Save encoder (`PAPC_label_encoder.pkl`).\n",
    "   * Visualize mapping with bar chart and log to MLflow.\n",
    "\n",
    "4. **Model Training**\n",
    "\n",
    "   * Load embeddings and label encoder.\n",
    "   * Split into train/test (stratify when possible).\n",
    "   * Train `RandomForestClassifier`.\n",
    "   * Log metrics, artifacts, and model to MLflow.\n",
    "\n",
    "5. **Model Evaluation**\n",
    "\n",
    "   * Reload model and encoder.\n",
    "   * Split and predict on test set.\n",
    "   * Compute accuracy, classification report, confusion matrix.\n",
    "   * Log evaluation artifacts to MLflow.\n",
    "   * Perform cross-validation (LOO if class imbalance).\n",
    "\n",
    "6. **Inference on New Data**\n",
    "\n",
    "   * Load model & encoder.\n",
    "   * Define `predict_factor(text)` helper that requests embedding and predicts.\n",
    "   * Test on meaningful example sentences.\n",
    "   * Log predictions to MLflow.\n",
    "\n",
    "7. **Visualization & Clustering**\n",
    "\n",
    "   * Train RF on full dataset.\n",
    "   * In‚Äësample evaluation & plot confusion matrix.\n",
    "   * PCA into 2D & scatter by encoded class.\n",
    "   * K‚ÄëMeans clustering on embeddings, save `PAPC_clustered_embeddings.csv`.\n",
    "   * Plot clusters in PCA space.\n",
    "\n",
    "8. **Neo4j Graph Construction**\n",
    "\n",
    "   * Clear existing graph: `MATCH (n) DETACH DELETE n`.\n",
    "   * Ingest `PAPC.csv`, create nodes for Factor, Adjective, Synonym, Verb, Noun.\n",
    "   * Create `:RELATES_TO` relationships following taxonomy hierarchy.\n",
    "\n",
    "9. **Cypher Query to View Graph**\n",
    "\n",
    "   ```cypher\n",
    "   MATCH p = (f:Factor)-[:HAS_ADJECTIVE]->(a:Adjective)\n",
    "             -[:HAS_SYNONYM|HAS_VERB|HAS_NOUN]->(x)\n",
    "   RETURN p LIMIT 100\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "## BibTeX Entries (`references.bib`)\n",
    "\n",
    "```bipapcex\n",
    "@article{Holland1959,\n",
    "  author    = {Holland, John L.},\n",
    "  title     = {A Theory of Vocational Choice},\n",
    "  journal   = {Journal of Counseling Psychology},\n",
    "  year      = {1959},\n",
    "  volume    = {6},\n",
    "  number    = {1},\n",
    "  pages     = {35--47},\n",
    "  doi       = {10.1037/h0040767},\n",
    "}\n",
    "\n",
    "@book{Holland1973,\n",
    "  author    = {Holland, John L.},\n",
    "  title     = {Making Vocational Choices: A Theory of Careers},\n",
    "  publisher = {Prentice-Hall},\n",
    "  year      = {1973},\n",
    "}\n",
    "\n",
    "@article{GatiMeir1982,\n",
    "  author    = {Gati, Itamar and Meir, Eliyahu I.},\n",
    "  title     = {Congruence and Consistency Derived from the Circular and the Hierarchical Models as Predictors of Occupational Choice Satisfaction},\n",
    "  journal   = {Journal of Vocational Behavior},\n",
    "  year      = {1982},\n",
    "  volume    = {20},\n",
    "  pages     = {354--365},\n",
    "  doi       = {10.1016/0001-8791(82)90022-7},\n",
    "}\n",
    "\n",
    "@article{OsipowAshbyWall1966,\n",
    "  author    = {Osipow, Samuel H. and Ashby, John and Wall, Harry},\n",
    "  title     = {Personality Types and Vocational Choice: A Test of Holland's Theory},\n",
    "  journal   = {The Personnel and Guidance Journal},\n",
    "  year      = {1966},\n",
    "  volume    = {45},\n",
    "  number    = {1},\n",
    "  pages     = {37--42},\n",
    "  doi       = {10.1002/j.2164-4918.1966.tb03063.x},\n",
    "}\n",
    "\n",
    "@article{Nauta2010,\n",
    "  author    = {Nauta, Margaret M.},\n",
    "  title     = {The Development, Evolution, and Status of Holland's Theory of Vocational Personalities: Reflections and Future Directions for Counseling Psychology},\n",
    "  journal   = {Journal of Counseling Psychology},\n",
    "  year      = {2010},\n",
    "  volume    = {57},\n",
    "  number    = {1},\n",
    "  pages     = {11--22},\n",
    "  doi       = {10.1037/a0018213},\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9073f9-01c9-4778-b64e-8ce2fd50a6a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
