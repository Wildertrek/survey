{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Personality Atlas — Quick Start\n\nPersonality psychology has a fragmentation problem. Clinical psychologists use the MMPI and SCID. Trait researchers use OCEAN and HEXACO. Organizational consultants reach for DISC or MBTI. Each tradition has its own instruments, its own factor structures, and its own vocabulary — and almost no shared infrastructure for comparing constructs across traditions.\n\nThis notebook walks through a **Computational Atlas** that builds that infrastructure. It embeds 44 personality models — 6,694 traits across 358 factors — into a single vector space where any trait from any tradition can be searched, compared, and classified.\n\n**What you will see:**\n- Load any of the 44 models with the same 10 lines of code (Section 2)\n- Visualize how 7 research traditions organize in embedding space (Section 4)\n- Query a single trait and find related constructs across all 44 models (Section 5)\n- Test whether classifiers generalize to novel items they have never seen (Section 7)\n- Compare 1536 vs 3072-dim embeddings from Hugging Face (Section 8)\n- Validate on 368 human-authored items from 21 published instruments (Section 9)\n- Route 222 DSM-5 disorders through the atlas as a clinical stress test (Section 10)\n\nEverything runs from pre-computed assets in the repository. No API keys, no database, no waiting.\n\n> Raetano, J., Gregor, J., & Tamang, S. (2026). *A Survey and Computational Atlas of Personality Models.* ACM TIST. Under review.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Wildertrek/survey/blob/main/notebooks/atlas_quick_start.ipynb)"
  },
  {
   "cell_type": "code",
   "source": "# Video walkthrough of this notebook (~35 min)\n# Covers the dataset creation process, all 12 research questions, and the MindBench companion project.\nfrom IPython.display import YouTubeVideo\nYouTubeVideo('PLACEHOLDER', width=800, height=450)  # TODO: replace PLACEHOLDER with YouTube video ID after upload",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Setup\n\nClone the atlas repository and install FAISS (the only dependency Colab does not already have). Takes about 30 seconds."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clone the atlas repository (skip if already cloned)\nimport os\nif not os.path.exists(\"atlas\"):\n    !git clone --depth 1 https://github.com/Wildertrek/survey.git atlas\nelse:\n    print(\"Atlas already cloned — skipping.\")"
  },
  {
   "cell_type": "code",
   "source": "# Install dependencies (uses Colab's pre-installed sklearn — no version conflicts)\n!pip install -q faiss-cpu\n\n# Suppress sklearn version warning (models trained with 1.5.0, works fine with newer versions)\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Load and Predict — Any of the 44 Models\n\nEvery model in the atlas follows the same file convention: a **dataset CSV** (traits labeled by factor), a **pre-computed embedding CSV** (1,536-dim vectors from OpenAI `text-embedding-3-small`), and a **trained Random Forest** classifier with its label encoder.\n\nChange `MODEL` to any model abbreviation — `ocean`, `hex`, `mmpi`, `scid`, `npi`, `dt4`, `tci`, `stbv`, etc. — and the same code loads, classifies, and reports. That uniformity is the point: one interface for 44 models from seven different research traditions."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Pick any model abbreviation: ocean, hex, mbti, mmpi, scid, npi, dt4, tci, ...\n",
    "MODEL = \"ocean\"\n",
    "\n",
    "df = pd.read_csv(f\"atlas/datasets/{MODEL}.csv\")\n",
    "embeddings = pd.read_csv(f\"atlas/Embeddings/{MODEL}_embeddings.csv\")\n",
    "model = joblib.load(f\"atlas/models/{MODEL}_rf_model.pkl\")\n",
    "encoder = joblib.load(f\"atlas/models/{MODEL}_label_encoder.pkl\")\n",
    "\n",
    "X = np.array([ast.literal_eval(e) for e in embeddings[\"Embedding\"]])\n",
    "predictions = encoder.inverse_transform(model.predict(X))\n",
    "accuracy = (predictions == df[\"Factor\"].values).mean()\n",
    "\n",
    "print(f\"{MODEL.upper()}: {len(df)} traits, {len(set(predictions))} factors, accuracy = {accuracy:.1%}\")\n",
    "print(f\"Factors: {sorted(set(predictions))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Atlas Overview — All 44 Models\n\nHow well does each classifier separate its own model's factors? The loop below runs every model and reports accuracy on the full dataset.\n\nMost models score above 95%. With 1,536 embedding dimensions and only a handful of factors, a Random Forest has plenty of room to find separating boundaries. High accuracy here is a necessary condition — it proves the lexical encoding captures meaningful distinctions between factors — but it is not a generalization test. We address that in Section 7 with items the classifiers have never seen."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "slugs = sorted([f.replace(\".csv\", \"\") for f in os.listdir(\"atlas/datasets\") if f.endswith(\".csv\")])\n",
    "print(f\"Atlas contains {len(slugs)} personality models:\\n\")\n",
    "\n",
    "results = []\n",
    "for slug in slugs:\n",
    "    df = pd.read_csv(f\"atlas/datasets/{slug}.csv\")\n",
    "    n_factors = df[\"Factor\"].nunique()\n",
    "    \n",
    "    model = joblib.load(f\"atlas/models/{slug}_rf_model.pkl\")\n",
    "    enc = joblib.load(f\"atlas/models/{slug}_label_encoder.pkl\")\n",
    "    emb = pd.read_csv(f\"atlas/Embeddings/{slug}_embeddings.csv\")\n",
    "    X = np.array([ast.literal_eval(e) for e in emb[\"Embedding\"]])\n",
    "    preds = enc.inverse_transform(model.predict(X))\n",
    "    acc = (preds == df[\"Factor\"].values).mean()\n",
    "    \n",
    "    results.append({\"Model\": slug.upper(), \"Traits\": len(df), \"Factors\": n_factors, \"Accuracy\": f\"{acc:.1%}\"})\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"Accuracy\", ascending=False)\n",
    "results_df.index = range(1, len(results_df) + 1)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Cross-Model PCA — How 6,694 Traits Organize in Embedding Space\n\nStack every trait from all 44 models into one matrix and run PCA. Two patterns emerge:\n\n**Theoretical neighbors become geometric neighbors.** Clinical models (MMPI, SCID, BDI) cluster together. Narcissism instruments (NPI, PNI, Dark Triad) form their own region. Trait models (OCEAN, HEXACO) land nearby. The embedding space recovers relationships that were never explicitly encoded — they fall out of the geometry.\n\n**The space is genuinely high-dimensional.** 50 principal components capture a modest share of total variance, meaning personality traits spread across many independent semantic directions. That is consistent with constructs that evolved independently across seven research traditions over the past century.\n\nThe three figures below reproduce the paper's PCA analysis (Figures 5-8)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# Category assignments for the 7-category taxonomy\nCATEGORIES = {\n    \"Trait-Based\": [\"ocean\", \"hex\", \"mbti\", \"epm\", \"sixteenpf\", \"ftm\"],\n    \"Narcissism-Based\": [\"npi\", \"pni\", \"hsns\", \"dtm\", \"dt4\", \"ffni\", \"ffni_sf\", \"narq\", \"mcmin\", \"ipn\"],\n    \"Motivational/Value\": [\"stbv\", \"sdt\", \"rft\", \"aam\", \"mst\", \"cs\"],\n    \"Cognitive/Learning\": [\"pct\", \"cest\", \"scm\", \"fsls\"],\n    \"Clinical/Health\": [\"mmpi\", \"scid\", \"bdi\", \"gad7\", \"wais\", \"tci\", \"mcmi\", \"tmp\", \"rit\", \"tat\"],\n    \"Interpersonal/Conflict\": [\"disc\", \"tki\"],\n    \"Application-Specific\": [\"riasec\", \"cmoa\", \"tei\", \"bt\", \"em\", \"papc\"]\n}\nslug_to_cat = {s: c for c, slugs in CATEGORIES.items() for s in slugs}\n\n# Load all embeddings\nall_vecs, all_labels, all_cats = [], [], []\nfor slug in slugs:\n    df = pd.read_csv(f\"atlas/datasets/{slug}.csv\")\n    emb = pd.read_csv(f\"atlas/Embeddings/{slug}_embeddings.csv\")\n    X = np.array([ast.literal_eval(e) for e in emb[\"Embedding\"]])\n    all_vecs.append(X)\n    all_labels.extend([slug.upper()] * len(X))\n    all_cats.extend([slug_to_cat.get(slug, \"Unknown\")] * len(X))\n\nX_all = np.vstack(all_vecs)\nn_unknown = sum(1 for c in all_cats if c == \"Unknown\")\nprint(f\"Loaded {X_all.shape[0]} embeddings ({X_all.shape[1]}-dim) from {len(slugs)} models\")\nif n_unknown > 0:\n    unknown_models = sorted(set(lbl for lbl, cat in zip(all_labels, all_cats) if cat == \"Unknown\"))\n    print(f\"WARNING: {n_unknown} embeddings from {len(unknown_models)} models have Unknown category: {unknown_models}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA — Scree plot (Paper Figure 5)\n",
    "pca = PCA(n_components=50)\n",
    "X_pca = pca.fit_transform(X_all)\n",
    "cumvar = np.cumsum(pca.explained_variance_ratio_) * 100\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.bar(range(1, 51), pca.explained_variance_ratio_ * 100, alpha=0.6, label=\"Individual\")\n",
    "ax.plot(range(1, 51), cumvar, \"r-o\", markersize=3, label=\"Cumulative\")\n",
    "ax.axhline(y=cumvar[-1], color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "ax.set_xlabel(\"Principal Component\")\n",
    "ax.set_ylabel(\"Variance Explained (%)\")\n",
    "ax.set_title(f\"PCA Scree Plot — {X_all.shape[0]} Trait Embeddings from 44 Models\\n50 PCs capture {cumvar[-1]:.1f}% of variance\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# PCA — Model centroids in PC1-PC2 space (Paper Figure 6)\n# Colors match the paper's PCA figures (01_cross_model_pca_analysis.py)\ncat_colors = {\n    \"Trait-Based\": \"#1f77b4\", \"Narcissism-Based\": \"#9467bd\",\n    \"Motivational/Value\": \"#2ca02c\", \"Cognitive/Learning\": \"#ff7f0e\",\n    \"Clinical/Health\": \"#d62728\", \"Interpersonal/Conflict\": \"#e377c2\",\n    \"Application-Specific\": \"#7f7f7f\"\n}\n\ncentroids = pd.DataFrame({\n    \"PC1\": X_pca[:, 0], \"PC2\": X_pca[:, 1],\n    \"Model\": all_labels, \"Category\": all_cats\n}).groupby([\"Model\", \"Category\"])[[\"PC1\", \"PC2\"]].mean().reset_index()\n\nfig, ax = plt.subplots(figsize=(12, 8))\nfor cat, color in cat_colors.items():\n    subset = centroids[centroids[\"Category\"] == cat]\n    ax.scatter(subset[\"PC1\"], subset[\"PC2\"], c=color, s=80, label=cat, alpha=0.8, edgecolors=\"white\", linewidth=0.5)\n    for _, row in subset.iterrows():\n        ax.annotate(row[\"Model\"], (row[\"PC1\"], row[\"PC2\"]), fontsize=7, alpha=0.7,\n                    xytext=(4, 4), textcoords=\"offset points\")\n\nax.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)\")\nax.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)\")\nax.set_title(\"44 Personality Models in PC1-PC2 Space (Model Centroids)\")\nax.legend(loc=\"best\", fontsize=8)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# PCA — All traits colored by category (Paper Figure 8)\nfig, ax = plt.subplots(figsize=(12, 8))\nfor cat, color in cat_colors.items():\n    mask = [c == cat for c in all_cats]\n    ax.scatter(X_pca[mask, 0], X_pca[mask, 1], c=color, s=4, alpha=0.3, label=cat)\n\nax.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)\")\nax.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)\")\nax.set_title(f\"All {X_all.shape[0]} Trait Embeddings — 44 Models, 7 Categories\")\nax.legend(markerscale=5, fontsize=8)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Cross-Model Search with FAISS\n\nThis is what the atlas is for. With every trait from every model in a single FAISS index, you can take any personality construct and retrieve its nearest neighbors *across all 44 models and all 7 categories*.\n\nA clinician studying depression can query a BDI item and find related constructs in the MMPI, TCI, and motivational models. A researcher working on narcissism can trace how Machiavellianism connects to MMPI Psychopathic Deviate and OCEAN low Agreeableness. These cross-tradition connections would normally require deep expertise spanning multiple literatures. The atlas finds them in milliseconds.\n\nBelow we build the index (cosine similarity via inner product on L2-normalized vectors) and run two example queries."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# Build FAISS index over entire atlas\n",
    "X_norm = X_all / np.linalg.norm(X_all, axis=1, keepdims=True)\n",
    "index = faiss.IndexFlatIP(X_norm.shape[1])\n",
    "index.add(X_norm.astype(np.float32))\n",
    "\n",
    "# Build metadata for lookups\n",
    "all_factors = []\n",
    "for slug in slugs:\n",
    "    df = pd.read_csv(f\"atlas/datasets/{slug}.csv\")\n",
    "    all_factors.extend(df[\"Factor\"].values)\n",
    "\n",
    "print(f\"FAISS index: {index.ntotal} vectors, {X_norm.shape[1]}-dim\")\n",
    "print(f\"Ready for cross-model personality search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Query: find similar traits across all 44 models\n# This demonstrates cross-category retrieval — the atlas's core value\nquery_slug = \"ocean\"\nquery_factor = \"Extraversion\"\n\nquery_df = pd.read_csv(f\"atlas/datasets/{query_slug}.csv\")\nquery_emb = pd.read_csv(f\"atlas/Embeddings/{query_slug}_embeddings.csv\")\n\n# Find first row matching the target factor\nidx = query_df[query_df[\"Factor\"] == query_factor].index[0]\nq = np.array([ast.literal_eval(query_emb[\"Embedding\"].iloc[idx])]).astype(np.float32)\nq = q / np.linalg.norm(q)\n\nD, I = index.search(q, 20)\n\nquery_trait = query_df.iloc[idx]\nprint(f\"Query: {query_slug.upper()} / {query_factor} — \\\"{query_trait['Adjective']}\\\"\\n\")\nprint(f\"{'Rank':<5} {'Model':<12} {'Factor':<35} {'Category':<22} {'Score':.5}\")\nprint(\"-\" * 85)\nfor rank, (i, score) in enumerate(zip(I[0], D[0]), 1):\n    print(f\"{rank:<5} {all_labels[i]:<12} {all_factors[i]:<35} {all_cats[i]:<22} {score:.4f}\")"
  },
  {
   "cell_type": "code",
   "source": "# Second query: Dark Triad Machiavellianism → cross-category retrieval\n# Shows how a narcissism-based construct connects to clinical, trait, and motivational models\nquery_slug = \"dtm\"\nquery_factor = \"Machiavellianism\"\n\nquery_df = pd.read_csv(f\"atlas/datasets/{query_slug}.csv\")\nquery_emb = pd.read_csv(f\"atlas/Embeddings/{query_slug}_embeddings.csv\")\n\nidx = query_df[query_df[\"Factor\"] == query_factor].index[0]\nq = np.array([ast.literal_eval(query_emb[\"Embedding\"].iloc[idx])]).astype(np.float32)\nq = q / np.linalg.norm(q)\n\nD, I = index.search(q, 20)\n\nquery_trait = query_df.iloc[idx]\nprint(f\"Query: {query_slug.upper()} / {query_factor} — \\\"{query_trait['Adjective']}\\\"\\n\")\nprint(f\"{'Rank':<5} {'Model':<12} {'Factor':<35} {'Category':<22} {'Score':.5}\")\nprint(\"-\" * 85)\nfor rank, (i, score) in enumerate(zip(I[0], D[0]), 1):\n    print(f\"{rank:<5} {all_labels[i]:<12} {all_factors[i]:<35} {all_cats[i]:<22} {score:.4f}\")\n\n# Count unique categories and models in results\nresult_cats = set(all_cats[i] for i in I[0])\nresult_models = set(all_labels[i] for i in I[0])\nprint(f\"\\n→ {len(result_cats)} categories, {len(result_models)} models in top 20 — cross-tradition retrieval\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Lexical Schema — What's Inside Each Model\n\nEach model's dataset is a table of traits. Every trait has five columns: **Adjective** (the root descriptor), **Synonym**, **Verb**, **Noun**, and **Factor** (the classification label). This five-column lexical encoding gives each trait a rich semantic surface for the embedding model to work with — richer than single-word labels, but grounded in actual psychometric vocabulary.\n\nChange `INSPECT` below to browse any model's factor structure and sample adjectives."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Inspect any model's lexical schema\nINSPECT = \"hex\"  # change to any slug (ocean, hex, mbti, mmpi, scid, npi, dt4, tci, ...)\n\ndf = pd.read_csv(f\"atlas/datasets/{INSPECT}.csv\")\nprint(f\"{INSPECT.upper()}: {len(df)} traits across {df['Factor'].nunique()} factors\\n\")\n\nfor factor, group in df.groupby(\"Factor\"):\n    unique_adj = group[\"Adjective\"].unique()[:5]\n    print(f\"  {factor} ({len(group)} traits): {', '.join(unique_adj)}, ...\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Experiment 1 — Discriminant Validity (RQ1-RQ6)\n\nThe paper poses six research questions about the atlas's discriminant validity. Can classifiers trained on atlas embeddings generalize to novel items? Does model complexity predict difficulty? Do LLM judges agree with each other? Where do classifiers fail? Do independent models converge in the shared embedding space? And do the seven atlas categories show systematic differences?\n\nThis section answers all six. The FAISS cross-model search in Section 5 already demonstrated **RQ5** (convergent validity) — we reference those results here rather than repeating them.\n\n### RQ1: Can classifiers discriminate between factors on novel items?\n\nWe test on **5,052 novel items** generated independently by GPT-4o from factor definitions alone, without access to the training data. Pre-computed 1,536-dim embeddings ship with the repository, so this runs without an API key.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\nfrom sklearn.metrics import accuracy_score\n\n# Load pre-computed test items (generated by GPT-4o, embedded offline)\ntest_items = json.load(open(\"atlas/data/test_items/test_items.json\"))\ntest_emb = np.load(\"atlas/data/test_items/test_items_embeddings.npz\")[\"embeddings\"]\n\n# Evaluate all 44 models on novel items\nnovel_results = []\nfor slug in slugs:\n    model_rf = joblib.load(f\"atlas/models/{slug}_rf_model.pkl\")\n    enc = joblib.load(f\"atlas/models/{slug}_label_encoder.pkl\")\n    \n    idx = [i for i, item in enumerate(test_items) if item[\"slug\"] == slug]\n    if not idx:\n        continue\n    \n    X_novel = test_emb[idx]\n    y_true = [test_items[i][\"expected_factor\"] for i in idx]\n    y_pred = enc.inverse_transform(model_rf.predict(X_novel))\n    acc = accuracy_score(y_true, y_pred)\n    novel_results.append({\"Model\": slug.upper(), \"Items\": len(idx), \"Novel Accuracy\": f\"{acc:.1%}\", \"_acc\": acc})\n\nnovel_df = pd.DataFrame(novel_results).sort_values(\"_acc\", ascending=False).drop(columns=[\"_acc\"])\nnovel_df.index = range(1, len(novel_df) + 1)\n\nmean_novel = np.mean([r[\"_acc\"] for r in novel_results])\nprint(f\"Mean novel-item accuracy: {mean_novel:.1%} (vs. ~98% on training data)\")\nprint(f\"This is the generalization accuracy reported in the paper.\\n\")\nnovel_df",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### RQ2: Does model complexity predict classification difficulty?\n\nModels with more factors should be harder to classify — more decision boundaries, more chances for semantic overlap. The scatter plot below tests this directly using pre-computed results from the `validation/` directory.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# RQ2: Factor count vs classification accuracy\n# Pre-computed results from the paper's validation pipeline\nfc = pd.read_csv(\"atlas/results/validation/factor_complexity.csv\")\n\nfig, ax = plt.subplots(figsize=(10, 6))\nfor cat in sorted(fc[\"Category\"].unique()):\n    subset = fc[fc[\"Category\"] == cat]\n    ax.scatter(subset[\"Factors\"], subset[\"RF Accuracy\"], label=cat, s=60, alpha=0.8, edgecolors=\"white\", linewidth=0.5)\n\n# Fit and plot trend line\nfrom scipy import stats\nslope, intercept, r, p, se = stats.linregress(fc[\"Factors\"], fc[\"RF Accuracy\"])\nx_line = np.linspace(fc[\"Factors\"].min(), fc[\"Factors\"].max(), 100)\nax.plot(x_line, slope * x_line + intercept, \"k--\", alpha=0.5, label=f\"r = {r:.2f}, p < .001\")\n\nax.set_xlabel(\"Number of Factors\")\nax.set_ylabel(\"Novel-Item Accuracy\")\nax.set_title(\"RQ2: Factor Count Predicts Classification Difficulty\")\nax.legend(fontsize=8)\nplt.tight_layout()\nplt.show()\n\nprint(f\"Pearson r = {r:.2f}, p = {p:.4f}\")\nprint(f\"Models with 2-4 factors average {fc[fc['Factors'] <= 4]['RF Accuracy'].mean():.1%}\")\nprint(f\"Models with 10+ factors average {fc[fc['Factors'] >= 10]['RF Accuracy'].mean():.1%}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### RQ3-RQ4: LLM Judge Agreement and Human-AI Alignment\n\nFor Experiment 1, three LLM judges (GPT-4o, Claude Sonnet 3.5, Gemini 1.5 Pro) independently classified each novel item. This lets us ask two questions:\n\n- **RQ3:** Do state-of-the-art LLMs from different providers agree on factor assignments? (inter-judge agreement)\n- **RQ4:** Where do RF classifiers fail relative to LLM judges? (human-AI alignment)\n\nThe table below loads pre-computed judge metrics from the validation pipeline. High inter-judge agreement means the items have clear factor membership. Where RF classifiers disagree with judges, the embeddings may not capture the relevant semantic distinctions.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# RQ3-RQ4: LLM Judge Agreement and RF-Judge Alignment\n# Pre-computed from the paper's three-judge validation pipeline\nimr = pd.read_csv(\"atlas/results/validation/individual_model_results.csv\")\n\n# RQ3: Inter-judge agreement (how often all 3 LLM judges pick the same factor)\nmean_inter = imr[\"Inter-Judge Agree\"].dropna().mean()\nprint(f\"RQ3: Inter-Judge Agreement\")\nprint(f\"  Mean agreement across models: {mean_inter:.1%}\")\nprint(f\"  This is near-perfect: the LLM judges almost always agree on factor assignment.\\n\")\n\n# RQ4: RF-Judge alignment (where do RF classifiers disagree with LLM consensus?)\nmean_rf_judge = imr[\"Judge RF-Agree\"].dropna().mean()\nmean_valid = imr[\"Judge Valid Rate\"].dropna().mean()\nprint(f\"RQ4: RF-Judge Alignment\")\nprint(f\"  Mean RF-Judge agreement: {mean_rf_judge:.1%}\")\nprint(f\"  Mean Judge valid rate:   {mean_valid:.1%}\")\nprint(f\"  Gap: {(mean_valid - mean_rf_judge)*100:.1f}pp — judges validate items the RF misclassifies.\\n\")\n\n# Show models with lowest RF-Judge agreement (where embeddings struggle)\nlow_agree = imr[imr[\"Judge RF-Agree\"].notna()].nsmallest(10, \"Judge RF-Agree\")[\n    [\"Model\", \"Category\", \"Factors\", \"RF Accuracy\", \"Judge RF-Agree\", \"Judge Valid Rate\"]\n].copy()\nlow_agree[\"RF Accuracy\"] = low_agree[\"RF Accuracy\"].apply(lambda x: f\"{x:.1%}\")\nlow_agree[\"Judge RF-Agree\"] = low_agree[\"Judge RF-Agree\"].apply(lambda x: f\"{x:.1%}\")\nlow_agree[\"Judge Valid Rate\"] = low_agree[\"Judge Valid Rate\"].apply(lambda x: f\"{x:.1%}\")\nlow_agree.index = range(1, len(low_agree) + 1)\nprint(\"Models with lowest RF-Judge agreement (where embeddings struggle most):\")\nlow_agree",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### RQ5: Do independent models converge in embedding space?\n\nSection 5 already answered this. When we queried OCEAN Extraversion, the top-20 results included models from multiple categories and traditions. When we queried Dark Triad Machiavellianism, the retrieval spanned narcissism, clinical, and trait models. The FAISS index finds cross-tradition convergences without any explicit alignment step.\n\n### RQ6: Do the seven atlas categories show systematic performance differences?\n\nIf all categories performed equally, the atlas taxonomy would not matter for classification. The table below breaks down Experiment 1 accuracy by category, showing where the atlas works well (Motivational, Narcissism) and where it struggles (Interpersonal, Application-Specific).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# RQ6: Category-level performance breakdown\ncat_df = pd.read_csv(\"atlas/results/validation/category_results.csv\")\n\nprint(f\"RQ6: Performance by Atlas Category\\n\")\nprint(f\"{'Category':<25} {'Models':>6} {'Factors':>7} {'Items':>6} {'Mean Acc':>9} {'Std':>6} {'Best Model':<12} {'Worst Model':<12}\")\nprint(\"-\" * 100)\nfor _, row in cat_df.sort_values(\"Mean RF Acc\", ascending=False).iterrows():\n    print(f\"{row['Category']:<25} {row['Models']:>6} {row['Total Factors']:>7} {row['Test Items']:>6} \"\n          f\"{row['Mean RF Acc']:>8.1%} {row['Std RF Acc']:>6.2f} {row['Best Model']:<12} {row['Worst Model']:<12}\")\n\nprint(f\"\\nMotivational and Narcissism categories perform best (fewer, more semantically\")\nprint(f\"distinct factors). Interpersonal has only 2 models (TKI, DISC) — DISC alone has\")\nprint(f\"29 factors, dragging the category average below 25%.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 8. Experiment 2 — Improvement Strategies (RQ7-RQ9)\n\nExperiment 1 established the baseline. Experiment 2 asks: can we do better? The paper tests three improvement strategies:\n\n- **RQ7 (Embedding upgrade):** Does switching from 1,536-dim to 3,072-dim embeddings improve classification?\n- **RQ8 (Data augmentation):** For the 14 weakest models, does augmenting training data with paraphrased items help?\n- **RQ9 (Hierarchical classification):** For 8 models with nested factor structures, does a two-level category-then-factor pipeline help?\n\nThe embedding upgrade (RQ7) is the only strategy we can reproduce live in this notebook, since the augmented and hierarchical models require the full training pipeline. For RQ8 and RQ9, we load pre-computed results from the `validation/` directory.\n\nThe 3,072-dim embeddings (440 MB) and retrained classifiers (107 MB) are hosted on [Hugging Face Hub](https://huggingface.co/datasets/Wildertrek/personality-atlas-3072). The cells below download them on demand and compare accuracy side by side.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "!pip install -q huggingface_hub\n\nimport os\nos.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", message=\".*HF_TOKEN.*\")\nwarnings.filterwarnings(\"ignore\", message=\".*unauthenticated.*\")\n\nfrom huggingface_hub import hf_hub_download\n\nHF_REPO = \"Wildertrek/personality-atlas-3072\"\n\ndef load_3072(slug):\n    \"\"\"Download and load 3072-dim assets for a single model from HuggingFace.\"\"\"\n    emb_path = hf_hub_download(HF_REPO, f\"Embeddings_3072/{slug}_embeddings.csv\", repo_type=\"dataset\")\n    model_path = hf_hub_download(HF_REPO, f\"models_3072/{slug}_rf_model.pkl\", repo_type=\"dataset\")\n    enc_path = hf_hub_download(HF_REPO, f\"models_3072/{slug}_label_encoder.pkl\", repo_type=\"dataset\")\n    emb_df = pd.read_csv(emb_path)\n    X = np.array([ast.literal_eval(e) for e in emb_df[\"Embedding\"]])\n    return X, joblib.load(model_path), joblib.load(enc_path)\n\nprint(\"Ready to download 3072-dim assets from HuggingFace.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Compare 1536 vs 3072 accuracy across all 44 models\ncomparison = []\nfor slug in slugs:\n    df = pd.read_csv(f\"atlas/datasets/{slug}.csv\")\n    y_true = df[\"Factor\"].values\n\n    # 1536-dim (already loaded from repo)\n    m1536 = joblib.load(f\"atlas/models/{slug}_rf_model.pkl\")\n    e1536 = joblib.load(f\"atlas/models/{slug}_label_encoder.pkl\")\n    emb1536 = pd.read_csv(f\"atlas/Embeddings/{slug}_embeddings.csv\")\n    X1536 = np.array([ast.literal_eval(e) for e in emb1536[\"Embedding\"]])\n    acc1536 = (e1536.inverse_transform(m1536.predict(X1536)) == y_true).mean()\n\n    # 3072-dim (downloaded from HuggingFace)\n    X3072, m3072, e3072 = load_3072(slug)\n    acc3072 = (e3072.inverse_transform(m3072.predict(X3072)) == y_true).mean()\n\n    delta = (acc3072 - acc1536) * 100\n    comparison.append({\n        \"Model\": slug.upper(),\n        \"1536-dim\": f\"{acc1536:.1%}\",\n        \"3072-dim\": f\"{acc3072:.1%}\",\n        \"Delta\": f\"{delta:+.1f}pp\",\n        \"_delta\": delta,  # numeric for sorting\n    })\n\ncomp_df = pd.DataFrame(comparison).sort_values(\"_delta\", ascending=False).drop(columns=[\"_delta\"])\ncomp_df.index = range(1, len(comp_df) + 1)\n\nimproved = sum(1 for c in comparison if c[\"_delta\"] > 0.05)\ndecreased = sum(1 for c in comparison if c[\"_delta\"] < -0.05)\nunchanged = len(comparison) - improved - decreased\nmean_1536 = np.mean([float(c[\"1536-dim\"].rstrip(\"%\")) for c in comparison])\nmean_3072 = np.mean([float(c[\"3072-dim\"].rstrip(\"%\")) for c in comparison])\n\nprint(f\"1536-dim mean: {mean_1536:.1f}% | 3072-dim mean: {mean_3072:.1f}% | Delta: {mean_3072 - mean_1536:+.1f}pp\")\nprint(f\"{improved} improved, {decreased} decreased, {unchanged} unchanged\\n\")\ncomp_df",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Novel item accuracy: 1536-dim vs 3072-dim\n\nThe comparison above used each model's own training data, where both embedding sizes tend to score high. The more informative test: run both classifiers on the **5,052 novel items** from Section 7 — items neither classifier has seen during training. Pre-computed 3072-dim embeddings for these items are downloaded from HuggingFace.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Novel item accuracy: 1536-dim vs 3072-dim on truly novel test items\n# Downloads 3072-dim test item embeddings from HuggingFace (no API key needed)\ntest3072_path = hf_hub_download(HF_REPO, \"test_items/test_items_embeddings_3072.npz\", repo_type=\"dataset\")\ntest_emb_3072 = np.load(test3072_path)[\"embeddings\"]\n\nnovel_comparison = []\nfor slug in slugs:\n    m1536 = joblib.load(f\"atlas/models/{slug}_rf_model.pkl\")\n    e1536 = joblib.load(f\"atlas/models/{slug}_label_encoder.pkl\")\n    X3072_m, m3072, e3072 = load_3072(slug)\n\n    idx = [i for i, item in enumerate(test_items) if item[\"slug\"] == slug]\n    if not idx:\n        continue\n\n    y_true = [test_items[i][\"expected_factor\"] for i in idx]\n    acc_n1536 = accuracy_score(y_true, e1536.inverse_transform(m1536.predict(test_emb[idx])))\n    acc_n3072 = accuracy_score(y_true, e3072.inverse_transform(m3072.predict(test_emb_3072[idx])))\n    delta = (acc_n3072 - acc_n1536) * 100\n    novel_comparison.append({\n        \"Model\": slug.upper(), \"Items\": len(idx),\n        \"1536-dim\": f\"{acc_n1536:.1%}\", \"3072-dim\": f\"{acc_n3072:.1%}\",\n        \"Delta\": f\"{delta:+.1f}pp\", \"_delta\": delta\n    })\n\nnc_df = pd.DataFrame(novel_comparison).sort_values(\"_delta\", ascending=False).drop(columns=[\"_delta\"])\nnc_df.index = range(1, len(nc_df) + 1)\n\nmean_n1536 = np.mean([float(c[\"1536-dim\"].rstrip(\"%\")) for c in novel_comparison])\nmean_n3072 = np.mean([float(c[\"3072-dim\"].rstrip(\"%\")) for c in novel_comparison])\nimproved = sum(1 for c in novel_comparison if c[\"_delta\"] > 0.05)\ndecreased = sum(1 for c in novel_comparison if c[\"_delta\"] < -0.05)\nunchanged = len(novel_comparison) - improved - decreased\n\nprint(f\"Novel item accuracy: 1536-dim mean: {mean_n1536:.1f}% | 3072-dim mean: {mean_n3072:.1f}% | Delta: {mean_n3072 - mean_n1536:+.1f}pp\")\nprint(f\"{improved} improved, {decreased} decreased, {unchanged} unchanged\\n\")\nnc_df",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### RQ8-RQ9: Data Augmentation and Hierarchical Classification\n\nThe embedding upgrade (RQ7) helps models that already have decent factor separation. For models where factors overlap semantically, we need different strategies:\n\n- **RQ8 (Data augmentation):** For 14 models below 50% accuracy, we augmented training data with LLM-generated paraphrases. This gives the classifier more examples of boundary cases.\n- **RQ9 (Hierarchical classification):** For 8 models with nested factor structures (e.g., SCID with 21 DSM categories), we first classify into a coarse category, then into the specific factor.\n\nThe table below loads the best result from each strategy for every model. The `Best Source` column shows which strategy produced the highest accuracy.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# RQ7-RQ9: Combined improvement strategies\n# Pre-computed results from the paper's full training pipeline\nexp2 = pd.read_csv(\"atlas/results/validation/experiment2_comparison.csv\")\n\n# Column names in CSV use the paper's original RQ numbering (swapped during revision):\n#   CSV \"RQ9 (3072)\" = Paper RQ7 (embedding upgrade, 1536 → 3072-dim)\n#   CSV \"RQ7 (Aug)\"  = Paper RQ8 (data augmentation for 14 weakest models)\n#   CSV \"RQ8 (Hier)\" = Paper RQ9 (hierarchical classification for 8 nested models)\nexp2 = exp2.rename(columns={\n    \"RQ9 (3072)\": \"RQ7_embed\",\n    \"RQ7 (Aug)\": \"RQ8_augment\",\n    \"RQ8 (Hier)\": \"RQ9_hier\"\n})\n\n# Show top improvers by strategy\nprint(\"RQ7-RQ9: Best improvement per model (Experiment 2)\\n\")\nprint(f\"{'Model':<12} {'Exp1':>6} {'RQ7 3072':>9} {'RQ8 Aug':>8} {'RQ9 Hier':>9} {'Best':>6} {'Source':>8} {'Delta':>7}\")\nprint(\"-\" * 75)\nfor _, row in exp2.sort_values(\"Delta\", ascending=False).head(15).iterrows():\n    rq7 = f\"{row['RQ7_embed']:.1%}\" if pd.notna(row['RQ7_embed']) else \"—\"\n    rq8 = f\"{row['RQ8_augment']:.1%}\" if pd.notna(row['RQ8_augment']) else \"—\"\n    rq9 = f\"{row['RQ9_hier']:.1%}\" if pd.notna(row['RQ9_hier']) else \"—\"\n    src = row[\"Best Source\"]\n    if src == \"rq9\": src = \"RQ7\"\n    elif src == \"rq7\": src = \"RQ8\"\n    elif src == \"rq8\": src = \"RQ9\"\n    print(f\"{row['Model']:<12} {row['Exp1']:>5.1%} {rq7:>9} {rq8:>8} {rq9:>9} {row['Best']:>5.1%} {src:>8} {row['Delta']:>+6.1%}\")\n\n# Summary statistics\nn_aug = exp2[\"RQ8_augment\"].notna().sum()\nn_hier = exp2[\"RQ9_hier\"].notna().sum()\nimproved = (exp2[\"Delta\"] > 0.005).sum()\nmean_best = exp2[\"Best\"].mean()\n\nprint(f\"\\nSummary:\")\nprint(f\"  RQ7 (embedding upgrade): all 44 models tested\")\nprint(f\"  RQ8 (data augmentation): {n_aug} models below 50% accuracy\")\nprint(f\"  RQ9 (hierarchical):      {n_hier} models with nested factor structures\")\nprint(f\"  {improved}/{len(exp2)} models improved by at least one strategy\")\nprint(f\"  Combined best mean accuracy: {mean_best:.1%} (vs. {exp2['Exp1'].mean():.1%} baseline)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 9. Experiment 3 — External Validation with Published Instruments\n\nExperiments 1 and 2 relied entirely on LLM-generated test items. A reviewer could reasonably ask: do these classifiers work on items written by human psychometricians?\n\nExperiment 3 answers with three tests:\n\n- **RQ10 (Multi-generator consistency):** We generated a second item set using Claude Opus 4.6 instead of GPT-4o. If the results hold across generators, the findings are not artifacts of one model's output distribution.\n- **RQ11 (Human item classification):** We collected 368 items from 21 published instruments (BFI-44, GAD-7, HEXACO-60, Short Dark Triad, NARQ, etc.) and classified them through the atlas. Higher accuracy than Experiment 1 would indicate that published items — designed to load cleanly on single factors — are easier to classify than LLM-generated items.\n- **RQ12 (Cross-instrument convergent validity):** We built an evaluation bank combining atlas training data, GPT-4o items, and human items (12,114 vectors total), then queried each human item to see whether it retrieves related content from the correct model and category.\n\nAll pre-computed embeddings ship with the repository. No API keys needed.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# RQ10: Multi-Generator Consistency — GPT-4o vs Claude Opus 4.6\n# If Experiment 1 results depend on GPT-4o's output distribution, they are not generalizable.\n# We generated 5,369 items with Opus and compare accuracy model-by-model.\n\nopus_items = json.load(open(\"atlas/data/opus_items.json\"))\nopus_emb = np.load(\"atlas/data/opus_items_embeddings.npz\")[\"embeddings\"]\n\nrq10_results = []\nfor slug in slugs:\n    model_rf = joblib.load(f\"atlas/models/{slug}_rf_model.pkl\")\n    enc = joblib.load(f\"atlas/models/{slug}_label_encoder.pkl\")\n\n    # GPT-4o items (from Section 7)\n    gpt_idx = [i for i, item in enumerate(test_items) if item[\"slug\"] == slug]\n    # Opus items\n    opus_idx = [i for i, item in enumerate(opus_items) if item[\"slug\"] == slug]\n\n    if not gpt_idx or not opus_idx:\n        continue\n\n    acc_gpt = accuracy_score(\n        [test_items[i][\"expected_factor\"] for i in gpt_idx],\n        enc.inverse_transform(model_rf.predict(test_emb[gpt_idx]))\n    )\n    acc_opus = accuracy_score(\n        [opus_items[i][\"expected_factor\"] for i in opus_idx],\n        enc.inverse_transform(model_rf.predict(opus_emb[opus_idx]))\n    )\n    rq10_results.append({\n        \"Model\": slug.upper(), \"GPT-4o Items\": len(gpt_idx), \"Opus Items\": len(opus_idx),\n        \"GPT-4o Acc\": acc_gpt, \"Opus Acc\": acc_opus, \"Delta\": acc_gpt - acc_opus\n    })\n\nrq10_df = pd.DataFrame(rq10_results)\nmean_gpt = rq10_df[\"GPT-4o Acc\"].mean()\nmean_opus = rq10_df[\"Opus Acc\"].mean()\n\n# Paired t-test\nfrom scipy import stats\nt_stat, p_val = stats.ttest_rel(rq10_df[\"GPT-4o Acc\"], rq10_df[\"Opus Acc\"])\nd = (mean_gpt - mean_opus) / rq10_df[\"Delta\"].std()  # Cohen's d\n\nprint(f\"RQ10: Multi-Generator Consistency\")\nprint(f\"  GPT-4o mean accuracy: {mean_gpt:.1%} ({sum(len([i for i, item in enumerate(test_items) if item['slug'] == r['Model'].lower()]) for r in rq10_results)} items)\")\nprint(f\"  Opus mean accuracy:   {mean_opus:.1%} ({len(opus_items)} items)\")\nprint(f\"  Delta: {(mean_gpt - mean_opus)*100:.1f} percentage points\")\nprint(f\"  Paired t({len(rq10_df)-1}) = {t_stat:.2f}, p = {p_val:.3f}, Cohen's d = {d:.2f}\")\nprint(f\"\\nThe {(mean_gpt - mean_opus)*100:.1f}pp difference is statistically modest (d = {d:.2f}).\")\nprint(\"Experiment 1 findings are not artifacts of a single generator.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### RQ11: Human-Authored Item Classification\n\n368 items from 21 published psychometric instruments — BFI-44, GAD-7, HEXACO-60, Short Dark Triad, Short Dark Tetrad, NARQ, and 15 others — each embedded and classified through the corresponding atlas model's Random Forest. These items were written by psychometricians to load cleanly on single factors, so we expect higher accuracy than the LLM-generated items in Section 7.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# RQ11: Human-Authored Item Classification\nhuman_items = json.load(open(\"atlas/data/human_items.json\"))\nhuman_emb = np.load(\"atlas/data/human_items_embeddings.npz\")[\"embeddings\"]\n\nrq11_results = []\nfor slug in sorted(set(item[\"slug\"] for item in human_items)):\n    model_rf = joblib.load(f\"atlas/models/{slug}_rf_model.pkl\")\n    enc = joblib.load(f\"atlas/models/{slug}_label_encoder.pkl\")\n\n    idx = [i for i, item in enumerate(human_items) if item[\"slug\"] == slug]\n    X_human = human_emb[idx]\n    y_true = [human_items[i][\"expected_factor\"] for i in idx]\n    y_pred = enc.inverse_transform(model_rf.predict(X_human))\n\n    acc = accuracy_score(y_true, y_pred)\n    instrument = human_items[idx[0]][\"instrument\"]\n    category = slug_to_cat.get(slug, \"Unknown\")\n    rq11_results.append({\n        \"Model\": slug.upper(), \"Instrument\": instrument,\n        \"Category\": category, \"Items\": len(idx), \"Accuracy\": acc\n    })\n\nrq11_df = pd.DataFrame(rq11_results).sort_values(\"Accuracy\", ascending=False)\nrq11_df.index = range(1, len(rq11_df) + 1)\n\noverall_correct = sum(r[\"Accuracy\"] * r[\"Items\"] for r in rq11_results)\noverall_total = sum(r[\"Items\"] for r in rq11_results)\noverall_acc = overall_correct / overall_total\n\nprint(f\"RQ11: Human-Authored Item Classification\")\nprint(f\"  Overall: {overall_acc:.1%} ({int(overall_correct)}/{overall_total} correct)\")\nprint(f\"  vs. Experiment 1 (LLM items): {mean_novel:.1%} — delta: {(overall_acc - mean_novel)*100:+.1f}pp\\n\")\n\n# Format for display\ndisplay_df = rq11_df.copy()\ndisplay_df[\"Accuracy\"] = display_df[\"Accuracy\"].apply(lambda x: f\"{x:.1%}\")\ndisplay_df",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### RQ12: Cross-Instrument Convergent Validity\n\nThe strongest test of the atlas's integrative value: when a human-authored item is queried against a bank containing atlas training data, LLM-generated items, and other human items, does it find related content from the correct model and category?\n\nWe build an evaluation bank of 12,114 vectors (6,694 atlas + 5,052 GPT-4o + 368 human) and query each of the 368 human items. For each query, we check whether the top-20 results include at least one vector from the correct model (model hit rate) and category (category hit rate).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# RQ12: Cross-Instrument Convergent Validity\n# Build evaluation bank: atlas (6,694) + GPT-4o (5,052) + human (368) = 12,114 vectors\nfrom collections import Counter\n\n# Atlas vectors and metadata (already loaded in Section 4-5)\nbank_vecs = [X_norm.copy()]  # atlas vectors already L2-normalized\nbank_models = list(all_labels)  # model labels\nbank_cats = list(all_cats)  # category labels\nbank_sources = [\"atlas\"] * len(all_labels)\n\n# GPT-4o test items\ngpt_norm = test_emb / np.linalg.norm(test_emb, axis=1, keepdims=True)\nbank_vecs.append(gpt_norm.astype(np.float32))\nbank_models.extend([item[\"slug\"].upper() for item in test_items])\nbank_cats.extend([slug_to_cat.get(item[\"slug\"], \"Unknown\") for item in test_items])\nbank_sources.extend([\"gpt4o\"] * len(test_items))\n\n# Human items\nhuman_norm = human_emb / np.linalg.norm(human_emb, axis=1, keepdims=True)\nbank_vecs.append(human_norm.astype(np.float32))\nbank_models.extend([item[\"slug\"].upper() for item in human_items])\nbank_cats.extend([slug_to_cat.get(item[\"slug\"], \"Unknown\") for item in human_items])\nbank_sources.extend([\"human\"] * len(human_items))\n\n# Build FAISS index\nbank_all = np.vstack(bank_vecs).astype(np.float32)\nbank_index = faiss.IndexFlatIP(bank_all.shape[1])\nbank_index.add(bank_all)\nprint(f\"Evaluation bank: {bank_index.ntotal} vectors (atlas: {len(all_labels)}, GPT-4o: {len(test_items)}, human: {len(human_items)})\")\n\n# Query each human item\nmodel_hits = 0\ncat_hits = 0\nmodels_per_query = []\nsource_counts = Counter()\n\nfor i, item in enumerate(human_items):\n    q = human_norm[i:i+1]\n    D, I = bank_index.search(q, 21)  # k=21 to skip self-match\n\n    # Skip the first result if it's the item itself (cosine ~1.0)\n    neighbors = [j for j in I[0] if j != (len(all_labels) + len(test_items) + i)][:20]\n\n    neighbor_models = [bank_models[j] for j in neighbors]\n    neighbor_cats = [bank_cats[j] for j in neighbors]\n    neighbor_sources = [bank_sources[j] for j in neighbors]\n\n    target_model = item[\"slug\"].upper()\n    target_cat = slug_to_cat.get(item[\"slug\"], \"Unknown\")\n\n    if target_model in neighbor_models:\n        model_hits += 1\n    if target_cat in neighbor_cats:\n        cat_hits += 1\n\n    models_per_query.append(len(set(neighbor_models)))\n    source_counts.update(neighbor_sources)\n\nn = len(human_items)\nprint(f\"\\nRQ12: Cross-Instrument Convergent Validity ({n} queries)\")\nprint(f\"  Model hit rate:    {model_hits}/{n} ({model_hits/n:.1%})\")\nprint(f\"  Category hit rate: {cat_hits}/{n} ({cat_hits/n:.1%})\")\nprint(f\"  Mean models per query: {np.mean(models_per_query):.1f}\")\n\ntotal_neighbors = sum(source_counts.values())\nprint(f\"\\n  Source distribution of retrieved neighbors:\")\nfor src in [\"atlas\", \"gpt4o\", \"human\"]:\n    pct = source_counts[src] / total_neighbors * 100\n    print(f\"    {src:<8}: {source_counts[src]:>5} ({pct:.1f}%)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 10. DSM-5 Clinical Alignment\n\nThe atlas was built from personality research instruments, not clinical diagnostic tools. But personality and psychopathology overlap heavily — the DSM-5 includes dimensional personality models, and clinical instruments like the MMPI and SCID already sit in the atlas.\n\nIf the taxonomy is well-structured, clinical constructs from outside the training data should route to the correct category. We test this by embedding all **222 DSM-5-TR disorders** (using each disorder's name and diagnostic keywords) and querying them against the atlas FAISS index. For each disorder, the top-10 nearest traits vote on a category.\n\nThe question: what fraction land in Clinical/Health? Data and pre-computed embeddings are included in the repository.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Classify 222 DSM-5-TR disorders through the atlas\n# Uses pre-computed embeddings — no API key needed\nimport json\nfrom collections import Counter\n\ndsm5 = json.load(open(\"atlas/data/dsm5_disorders.json\"))\ndsm5_emb = pd.read_csv(\"atlas/data/dsm5_embeddings.csv\")\ndsm5_vecs = np.array([ast.literal_eval(e) for e in dsm5_emb[\"Embedding\"]]).astype(np.float32)\ndsm5_vecs = dsm5_vecs / np.linalg.norm(dsm5_vecs, axis=1, keepdims=True)\n\n# Query each disorder against the atlas FAISS index (built in Section 5)\ndsm5_results = []\nfor i, disorder in enumerate(dsm5):\n    q = dsm5_vecs[i:i+1]\n    D, I = index.search(q, 10)\n    top_cats = [all_cats[j] for j in I[0]]\n    predicted_cat = Counter(top_cats).most_common(1)[0][0]\n    dsm5_results.append({\n        \"disorder\": disorder[\"disorder_name\"],\n        \"dsm5_category\": disorder[\"dsm5_category\"],\n        \"predicted_atlas_cat\": predicted_cat,\n        \"is_clinical\": predicted_cat == \"Clinical/Health\",\n        \"top_cat_counts\": dict(Counter(top_cats))\n    })\n\nclinical_count = sum(1 for r in dsm5_results if r[\"is_clinical\"])\npct = clinical_count / len(dsm5_results) * 100\nprint(f\"DSM-5 Clinical Alignment: {clinical_count}/{len(dsm5_results)} disorders ({pct:.1f}%) route to Clinical/Health\")\nprint(f\"This confirms the atlas taxonomy correctly captures clinical constructs.\\n\")\n\n# Show the few that don't route to Clinical/Health\nnon_clinical = [r for r in dsm5_results if not r[\"is_clinical\"]]\nif non_clinical:\n    print(f\"{len(non_clinical)} disorders route elsewhere:\")\n    for r in non_clinical:\n        print(f\"  {r['disorder'][:60]:<62} → {r['predicted_atlas_cat']}\")\nelse:\n    print(\"All 222 disorders route to Clinical/Health.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\nThe full pipeline — from raw psychometric instruments to a searchable, classifiable embedding space — is documented in the paper and fully reproducible from this repository.\n\n**Want to go deeper?** The [Deep Dive notebook](https://colab.research.google.com/github/Wildertrek/survey/blob/main/notebooks/atlas_deep_dive.ipynb) lets you pick any single model and take it apart: train a classifier from scratch, inspect the confusion matrix, visualize embedding geometry, and measure inter-factor similarity.\n\n**Repository:** [github.com/Wildertrek/survey](https://github.com/Wildertrek/survey) | **3072-dim assets:** [Hugging Face Hub](https://huggingface.co/datasets/Wildertrek/personality-atlas-3072)\n**Paper:** Raetano, J., Gregor, J., & Tamang, S. (2026). *A Survey and Computational Atlas of Personality Models.* ACM TIST.\n**License:** MIT",
   "metadata": {}
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}