{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personality Atlas — Quick Start\n",
    "\n",
    "**Reproduce the atlas results in under 5 minutes.** No API keys required.\n",
    "\n",
    "> Raetano, J., Gregor, J., & Tamang, S. (2026). *A Survey and Computational Atlas of Personality Models.* ACM TIST. Under review.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Wildertrek/survey/blob/main/notebooks/atlas_quick_start.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup (~30 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clone the atlas repository (skip if already cloned)\nimport os\nif not os.path.exists(\"atlas\"):\n    !git clone --depth 1 https://github.com/Wildertrek/survey.git atlas\nelse:\n    print(\"Atlas already cloned — skipping.\")"
  },
  {
   "cell_type": "code",
   "source": "# Install dependencies (uses Colab's pre-installed sklearn — no version conflicts)\n!pip install -q faiss-cpu\n\n# Suppress sklearn version warning (models trained with 1.5.0, works fine with newer versions)\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Predict — Any of 44 Models (~10 lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Pick any model: ocean, hexaco, mbti, mmpi, scid, npi, dt4, tci, ...\n",
    "SLUG = \"ocean\"\n",
    "\n",
    "df = pd.read_csv(f\"atlas/datasets/{SLUG}.csv\")\n",
    "embeddings = pd.read_csv(f\"atlas/Embeddings/{SLUG}_embeddings.csv\")\n",
    "model = joblib.load(f\"atlas/models/{SLUG}_rf_model.pkl\")\n",
    "encoder = joblib.load(f\"atlas/models/{SLUG}_label_encoder.pkl\")\n",
    "\n",
    "X = np.array([ast.literal_eval(e) for e in embeddings[\"Embedding\"]])\n",
    "predictions = encoder.inverse_transform(model.predict(X))\n",
    "accuracy = (predictions == df[\"Factor\"].values).mean()\n",
    "\n",
    "print(f\"{SLUG.upper()}: {len(df)} traits, {len(set(predictions))} factors, accuracy = {accuracy:.1%}\")\n",
    "print(f\"Factors: {sorted(set(predictions))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Atlas Overview — All 44 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "slugs = sorted([f.replace(\".csv\", \"\") for f in os.listdir(\"atlas/datasets\") if f.endswith(\".csv\")])\n",
    "print(f\"Atlas contains {len(slugs)} personality models:\\n\")\n",
    "\n",
    "results = []\n",
    "for slug in slugs:\n",
    "    df = pd.read_csv(f\"atlas/datasets/{slug}.csv\")\n",
    "    n_factors = df[\"Factor\"].nunique()\n",
    "    \n",
    "    model = joblib.load(f\"atlas/models/{slug}_rf_model.pkl\")\n",
    "    enc = joblib.load(f\"atlas/models/{slug}_label_encoder.pkl\")\n",
    "    emb = pd.read_csv(f\"atlas/Embeddings/{slug}_embeddings.csv\")\n",
    "    X = np.array([ast.literal_eval(e) for e in emb[\"Embedding\"]])\n",
    "    preds = enc.inverse_transform(model.predict(X))\n",
    "    acc = (preds == df[\"Factor\"].values).mean()\n",
    "    \n",
    "    results.append({\"Model\": slug.upper(), \"Traits\": len(df), \"Factors\": n_factors, \"Accuracy\": f\"{acc:.1%}\"})\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"Accuracy\", ascending=False)\n",
    "results_df.index = range(1, len(results_df) + 1)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reproduce PCA — Cross-Model Dimensionality Analysis (Paper Figure 5-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# Category assignments for the 7-category taxonomy\nCATEGORIES = {\n    \"Trait-Based\": [\"ocean\", \"hex\", \"mbti\", \"epm\", \"sixteenpf\", \"ftm\"],\n    \"Narcissism-Based\": [\"npi\", \"pni\", \"hsns\", \"dtm\", \"dt4\", \"ffni\", \"ffni_sf\", \"narq\", \"mcmin\", \"ipn\"],\n    \"Motivational/Value\": [\"stbv\", \"sdt\", \"rft\", \"aam\", \"mst\", \"cs\"],\n    \"Cognitive/Learning\": [\"pct\", \"cest\", \"scm\", \"fsls\"],\n    \"Clinical/Health\": [\"mmpi\", \"scid\", \"bdi\", \"gad7\", \"wais\", \"tci\", \"mcmi\", \"tmp\", \"rit\", \"tat\"],\n    \"Interpersonal/Conflict\": [\"disc\", \"tki\"],\n    \"Application-Specific\": [\"riasec\", \"cmoa\", \"tei\", \"bt\", \"em\", \"papc\"]\n}\nslug_to_cat = {s: c for c, slugs in CATEGORIES.items() for s in slugs}\n\n# Load all embeddings\nall_vecs, all_labels, all_cats = [], [], []\nfor slug in slugs:\n    df = pd.read_csv(f\"atlas/datasets/{slug}.csv\")\n    emb = pd.read_csv(f\"atlas/Embeddings/{slug}_embeddings.csv\")\n    X = np.array([ast.literal_eval(e) for e in emb[\"Embedding\"]])\n    all_vecs.append(X)\n    all_labels.extend([slug.upper()] * len(X))\n    all_cats.extend([slug_to_cat.get(slug, \"Unknown\")] * len(X))\n\nX_all = np.vstack(all_vecs)\nn_unknown = sum(1 for c in all_cats if c == \"Unknown\")\nprint(f\"Loaded {X_all.shape[0]} embeddings ({X_all.shape[1]}-dim) from {len(slugs)} models\")\nif n_unknown > 0:\n    unknown_slugs = sorted(set(s for s, c in zip([slugs[i] for i in range(len(slugs)) for _ in range(1)], all_cats) if c == \"Unknown\"))\n    print(f\"WARNING: {n_unknown} embeddings have Unknown category\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA — Scree plot (Paper Figure 5)\n",
    "pca = PCA(n_components=50)\n",
    "X_pca = pca.fit_transform(X_all)\n",
    "cumvar = np.cumsum(pca.explained_variance_ratio_) * 100\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.bar(range(1, 51), pca.explained_variance_ratio_ * 100, alpha=0.6, label=\"Individual\")\n",
    "ax.plot(range(1, 51), cumvar, \"r-o\", markersize=3, label=\"Cumulative\")\n",
    "ax.axhline(y=cumvar[-1], color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "ax.set_xlabel(\"Principal Component\")\n",
    "ax.set_ylabel(\"Variance Explained (%)\")\n",
    "ax.set_title(f\"PCA Scree Plot — {X_all.shape[0]} Trait Embeddings from 44 Models\\n50 PCs capture {cumvar[-1]:.1f}% of variance\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA — Model centroids in PC1-PC2 space (Paper Figure 6)\n",
    "cat_colors = {\n",
    "    \"Trait-Based\": \"#1f77b4\", \"Narcissism-Based\": \"#ff7f0e\",\n",
    "    \"Motivational/Value\": \"#2ca02c\", \"Cognitive/Learning\": \"#d62728\",\n",
    "    \"Clinical/Health\": \"#9467bd\", \"Interpersonal/Conflict\": \"#8c564b\",\n",
    "    \"Application-Specific\": \"#e377c2\"\n",
    "}\n",
    "\n",
    "centroids = pd.DataFrame({\n",
    "    \"PC1\": X_pca[:, 0], \"PC2\": X_pca[:, 1],\n",
    "    \"Model\": all_labels, \"Category\": all_cats\n",
    "}).groupby([\"Model\", \"Category\"])[[\"PC1\", \"PC2\"]].mean().reset_index()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "for cat, color in cat_colors.items():\n",
    "    subset = centroids[centroids[\"Category\"] == cat]\n",
    "    ax.scatter(subset[\"PC1\"], subset[\"PC2\"], c=color, s=80, label=cat, alpha=0.8, edgecolors=\"white\", linewidth=0.5)\n",
    "    for _, row in subset.iterrows():\n",
    "        ax.annotate(row[\"Model\"], (row[\"PC1\"], row[\"PC2\"]), fontsize=7, alpha=0.7,\n",
    "                    xytext=(4, 4), textcoords=\"offset points\")\n",
    "\n",
    "ax.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)\")\n",
    "ax.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)\")\n",
    "ax.set_title(\"44 Personality Models in PC1-PC2 Space (Model Centroids)\")\n",
    "ax.legend(loc=\"best\", fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA — All traits colored by category (Paper Figure 8)\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "for cat, color in cat_colors.items():\n",
    "    mask = [c == cat for c in all_cats]\n",
    "    ax.scatter(X_pca[mask, 0], X_pca[mask, 1], c=color, s=4, alpha=0.3, label=cat)\n",
    "\n",
    "ax.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)\")\n",
    "ax.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)\")\n",
    "ax.set_title(f\"All {X_all.shape[0]} Trait Embeddings — 44 Models, 7 Categories\")\n",
    "ax.legend(markerscale=5, fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cross-Model Search with FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# Build FAISS index over entire atlas\n",
    "X_norm = X_all / np.linalg.norm(X_all, axis=1, keepdims=True)\n",
    "index = faiss.IndexFlatIP(X_norm.shape[1])\n",
    "index.add(X_norm.astype(np.float32))\n",
    "\n",
    "# Build metadata for lookups\n",
    "all_factors = []\n",
    "for slug in slugs:\n",
    "    df = pd.read_csv(f\"atlas/datasets/{slug}.csv\")\n",
    "    all_factors.extend(df[\"Factor\"].values)\n",
    "\n",
    "print(f\"FAISS index: {index.ntotal} vectors, {X_norm.shape[1]}-dim\")\n",
    "print(f\"Ready for cross-model personality search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Query: find similar traits across all 44 models\n# This demonstrates cross-category retrieval — the atlas's core value\nquery_slug = \"ocean\"\nquery_factor = \"Extraversion\"\n\nquery_df = pd.read_csv(f\"atlas/datasets/{query_slug}.csv\")\nquery_emb = pd.read_csv(f\"atlas/Embeddings/{query_slug}_embeddings.csv\")\n\n# Find first row matching the target factor\nidx = query_df[query_df[\"Factor\"] == query_factor].index[0]\nq = np.array([ast.literal_eval(query_emb[\"Embedding\"].iloc[idx])]).astype(np.float32)\nq = q / np.linalg.norm(q)\n\nD, I = index.search(q, 20)\n\nquery_trait = query_df.iloc[idx]\nprint(f\"Query: {query_slug.upper()} / {query_factor} — \\\"{query_trait['Adjective']}\\\"\\n\")\nprint(f\"{'Rank':<5} {'Model':<12} {'Factor':<35} {'Category':<22} {'Score':.5}\")\nprint(\"-\" * 85)\nfor rank, (i, score) in enumerate(zip(I[0], D[0]), 1):\n    print(f\"{rank:<5} {all_labels[i]:<12} {all_factors[i]:<35} {all_cats[i]:<22} {score:.4f}\")"
  },
  {
   "cell_type": "code",
   "source": "# Second query: Dark Triad Machiavellianism → cross-category retrieval\n# Shows how a narcissism-based construct connects to clinical, trait, and motivational models\nquery_slug = \"dtm\"\nquery_factor = \"Machiavellianism\"\n\nquery_df = pd.read_csv(f\"atlas/datasets/{query_slug}.csv\")\nquery_emb = pd.read_csv(f\"atlas/Embeddings/{query_slug}_embeddings.csv\")\n\nidx = query_df[query_df[\"Factor\"] == query_factor].index[0]\nq = np.array([ast.literal_eval(query_emb[\"Embedding\"].iloc[idx])]).astype(np.float32)\nq = q / np.linalg.norm(q)\n\nD, I = index.search(q, 20)\n\nquery_trait = query_df.iloc[idx]\nprint(f\"Query: {query_slug.upper()} / {query_factor} — \\\"{query_trait['Adjective']}\\\"\\n\")\nprint(f\"{'Rank':<5} {'Model':<12} {'Factor':<35} {'Category':<22} {'Score':.5}\")\nprint(\"-\" * 85)\nfor rank, (i, score) in enumerate(zip(I[0], D[0]), 1):\n    print(f\"{rank:<5} {all_labels[i]:<12} {all_factors[i]:<35} {all_cats[i]:<22} {score:.4f}\")\n\n# Count unique categories and models in results\nresult_cats = set(all_cats[i] for i in I[0])\nresult_models = set(all_labels[i] for i in I[0])\nprint(f\"\\n→ {len(result_cats)} categories, {len(result_models)} models in top 20 — cross-tradition retrieval\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Lexical Schema — What's in Each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Inspect any model's lexical schema\nINSPECT = \"hex\"  # change to any slug (ocean, hex, mbti, mmpi, scid, npi, dt4, tci, ...)\n\ndf = pd.read_csv(f\"atlas/datasets/{INSPECT}.csv\")\nprint(f\"{INSPECT.upper()}: {len(df)} traits across {df['Factor'].nunique()} factors\\n\")\n\nfor factor, group in df.groupby(\"Factor\"):\n    unique_adj = group[\"Adjective\"].unique()[:5]\n    print(f\"  {factor} ({len(group)} traits): {', '.join(unique_adj)}, ...\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Experiment 1 — Novel Item Evaluation (Paper Section 6.2)\n\nThe accuracy leaderboard in Section 3 measures how well each classifier separates its own training lexicon --- a reproduction check, not a generalization test. Models with few traits relative to 1,536 embedding dimensions can reach 100% by memorizing the training set.\n\nHere we evaluate on **5,052 truly novel test items** generated independently by GPT-4o from factor definitions alone, without access to the training data. Pre-computed 1,536-dim embeddings are included in the repository (no API key needed). This is the actual generalization accuracy reported in the paper.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\nfrom sklearn.metrics import accuracy_score\n\n# Load pre-computed test items (generated by GPT-4o, embedded offline)\ntest_items = json.load(open(\"atlas/data/test_items/test_items.json\"))\ntest_emb = np.load(\"atlas/data/test_items/test_items_embeddings.npz\")[\"embeddings\"]\n\n# Evaluate all 44 models on novel items\nnovel_results = []\nfor slug in slugs:\n    model_rf = joblib.load(f\"atlas/models/{slug}_rf_model.pkl\")\n    enc = joblib.load(f\"atlas/models/{slug}_label_encoder.pkl\")\n    \n    idx = [i for i, item in enumerate(test_items) if item[\"slug\"] == slug]\n    if not idx:\n        continue\n    \n    X_novel = test_emb[idx]\n    y_true = [test_items[i][\"expected_factor\"] for i in idx]\n    y_pred = enc.inverse_transform(model_rf.predict(X_novel))\n    acc = accuracy_score(y_true, y_pred)\n    novel_results.append({\"Model\": slug.upper(), \"Items\": len(idx), \"Novel Accuracy\": f\"{acc:.1%}\", \"_acc\": acc})\n\nnovel_df = pd.DataFrame(novel_results).sort_values(\"_acc\", ascending=False).drop(columns=[\"_acc\"])\nnovel_df.index = range(1, len(novel_df) + 1)\n\nmean_novel = np.mean([r[\"_acc\"] for r in novel_results])\nprint(f\"Mean novel-item accuracy: {mean_novel:.1%} (vs. ~98% on training data)\")\nprint(f\"This is the generalization accuracy reported in the paper.\\n\")\nnovel_df",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 8. Experiment 2 — 3072-dim Embedding Upgrade (Optional)\n\nThe atlas also includes upgraded **3072-dim** embeddings (`text-embedding-3-large`) and retrained RF classifiers from Experiment 2. These are hosted on [Hugging Face Hub](https://huggingface.co/datasets/Wildertrek/personality-atlas-3072) due to size (547 MB total).\n\nRun the cells below to download and compare 1536 vs 3072 accuracy.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "!pip install -q huggingface_hub\n\nimport os\nos.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", message=\".*HF_TOKEN.*\")\nwarnings.filterwarnings(\"ignore\", message=\".*unauthenticated.*\")\n\nfrom huggingface_hub import hf_hub_download\n\nHF_REPO = \"Wildertrek/personality-atlas-3072\"\n\ndef load_3072(slug):\n    \"\"\"Download and load 3072-dim assets for a single model from HuggingFace.\"\"\"\n    emb_path = hf_hub_download(HF_REPO, f\"Embeddings_3072/{slug}_embeddings.csv\", repo_type=\"dataset\")\n    model_path = hf_hub_download(HF_REPO, f\"models_3072/{slug}_rf_model.pkl\", repo_type=\"dataset\")\n    enc_path = hf_hub_download(HF_REPO, f\"models_3072/{slug}_label_encoder.pkl\", repo_type=\"dataset\")\n    emb_df = pd.read_csv(emb_path)\n    X = np.array([ast.literal_eval(e) for e in emb_df[\"Embedding\"]])\n    return X, joblib.load(model_path), joblib.load(enc_path)\n\nprint(\"Ready to download 3072-dim assets from HuggingFace.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Compare 1536 vs 3072 accuracy across all 44 models\ncomparison = []\nfor slug in slugs:\n    df = pd.read_csv(f\"atlas/datasets/{slug}.csv\")\n    y_true = df[\"Factor\"].values\n\n    # 1536-dim (already loaded from repo)\n    m1536 = joblib.load(f\"atlas/models/{slug}_rf_model.pkl\")\n    e1536 = joblib.load(f\"atlas/models/{slug}_label_encoder.pkl\")\n    emb1536 = pd.read_csv(f\"atlas/Embeddings/{slug}_embeddings.csv\")\n    X1536 = np.array([ast.literal_eval(e) for e in emb1536[\"Embedding\"]])\n    acc1536 = (e1536.inverse_transform(m1536.predict(X1536)) == y_true).mean()\n\n    # 3072-dim (downloaded from HuggingFace)\n    X3072, m3072, e3072 = load_3072(slug)\n    acc3072 = (e3072.inverse_transform(m3072.predict(X3072)) == y_true).mean()\n\n    delta = (acc3072 - acc1536) * 100\n    comparison.append({\n        \"Model\": slug.upper(),\n        \"1536-dim\": f\"{acc1536:.1%}\",\n        \"3072-dim\": f\"{acc3072:.1%}\",\n        \"Delta\": f\"{delta:+.1f}pp\",\n        \"_delta\": delta,  # numeric for sorting\n    })\n\ncomp_df = pd.DataFrame(comparison).sort_values(\"_delta\", ascending=False).drop(columns=[\"_delta\"])\ncomp_df.index = range(1, len(comp_df) + 1)\n\nimproved = sum(1 for c in comparison if c[\"_delta\"] > 0.05)\ndecreased = sum(1 for c in comparison if c[\"_delta\"] < -0.05)\nunchanged = len(comparison) - improved - decreased\nmean_1536 = np.mean([float(c[\"1536-dim\"].rstrip(\"%\")) for c in comparison])\nmean_3072 = np.mean([float(c[\"3072-dim\"].rstrip(\"%\")) for c in comparison])\n\nprint(f\"1536-dim mean: {mean_1536:.1f}% | 3072-dim mean: {mean_3072:.1f}% | Delta: {mean_3072 - mean_1536:+.1f}pp\")\nprint(f\"{improved} improved, {decreased} decreased, {unchanged} unchanged\\n\")\ncomp_df",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Novel item accuracy: 1536-dim vs 3072-dim\n\nThe table above compares accuracy on each model's own training data. Below we repeat the comparison on the **5,052 novel test items** from Section 7, which the classifiers have never seen. Pre-computed 3072-dim embeddings are downloaded from HuggingFace (no API key needed).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Novel item accuracy: 1536-dim vs 3072-dim on truly novel test items\n# Downloads 3072-dim test item embeddings from HuggingFace (no API key needed)\ntest3072_path = hf_hub_download(HF_REPO, \"test_items/test_items_embeddings_3072.npz\", repo_type=\"dataset\")\ntest_emb_3072 = np.load(test3072_path)[\"embeddings\"]\n\nnovel_comparison = []\nfor slug in slugs:\n    m1536 = joblib.load(f\"atlas/models/{slug}_rf_model.pkl\")\n    e1536 = joblib.load(f\"atlas/models/{slug}_label_encoder.pkl\")\n    X3072_m, m3072, e3072 = load_3072(slug)\n\n    idx = [i for i, item in enumerate(test_items) if item[\"slug\"] == slug]\n    if not idx:\n        continue\n\n    y_true = [test_items[i][\"expected_factor\"] for i in idx]\n    acc_n1536 = accuracy_score(y_true, e1536.inverse_transform(m1536.predict(test_emb[idx])))\n    acc_n3072 = accuracy_score(y_true, e3072.inverse_transform(m3072.predict(test_emb_3072[idx])))\n    delta = (acc_n3072 - acc_n1536) * 100\n    novel_comparison.append({\n        \"Model\": slug.upper(), \"Items\": len(idx),\n        \"1536-dim\": f\"{acc_n1536:.1%}\", \"3072-dim\": f\"{acc_n3072:.1%}\",\n        \"Delta\": f\"{delta:+.1f}pp\", \"_delta\": delta\n    })\n\nnc_df = pd.DataFrame(novel_comparison).sort_values(\"_delta\", ascending=False).drop(columns=[\"_delta\"])\nnc_df.index = range(1, len(nc_df) + 1)\n\nmean_n1536 = np.mean([float(c[\"1536-dim\"].rstrip(\"%\")) for c in novel_comparison])\nmean_n3072 = np.mean([float(c[\"3072-dim\"].rstrip(\"%\")) for c in novel_comparison])\nimproved = sum(1 for c in novel_comparison if c[\"_delta\"] > 0.05)\ndecreased = sum(1 for c in novel_comparison if c[\"_delta\"] < -0.05)\nunchanged = len(novel_comparison) - improved - decreased\n\nprint(f\"Novel item accuracy: 1536-dim mean: {mean_n1536:.1f}% | 3072-dim mean: {mean_n3072:.1f}% | Delta: {mean_n3072 - mean_n1536:+.1f}pp\")\nprint(f\"{improved} improved, {decreased} decreased, {unchanged} unchanged\\n\")\nnc_df",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 9. Experiment 3 — External Validation: DSM-5 Clinical Alignment (Paper Section 6.3)\n\nThe atlas was built from personality research instruments, but personality constructs overlap heavily with clinical psychology. If the atlas taxonomy is well-structured, clinical constructs should route systematically to the correct category.\n\nWe test this by classifying all **222 DSM-5-TR disorders** through the atlas. Each disorder's name and diagnostic keywords are embedded and queried against the FAISS index. The predicted category for each disorder tells us whether the atlas's Clinical/Health category actually captures clinical constructs — or whether they leak into other categories.\n\n**Data:** `atlas/data/dsm5_disorders.json` (222 disorders, 21 DSM-5 categories, 786 keywords)  \n**Pre-computed:** `atlas/data/dsm5_embeddings.csv` (no API key needed)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Classify 222 DSM-5-TR disorders through the atlas\n# Uses pre-computed embeddings — no API key needed\nimport json\nfrom collections import Counter\n\ndsm5 = json.load(open(\"atlas/data/dsm5_disorders.json\"))\ndsm5_emb = pd.read_csv(\"atlas/data/dsm5_embeddings.csv\")\ndsm5_vecs = np.array([ast.literal_eval(e) for e in dsm5_emb[\"Embedding\"]]).astype(np.float32)\ndsm5_vecs = dsm5_vecs / np.linalg.norm(dsm5_vecs, axis=1, keepdims=True)\n\n# Query each disorder against the atlas FAISS index (built in Section 5)\ndsm5_results = []\nfor i, disorder in enumerate(dsm5):\n    q = dsm5_vecs[i:i+1]\n    D, I = index.search(q, 10)\n    top_cats = [all_cats[j] for j in I[0]]\n    predicted_cat = Counter(top_cats).most_common(1)[0][0]\n    dsm5_results.append({\n        \"disorder\": disorder[\"disorder_name\"],\n        \"dsm5_category\": disorder[\"dsm5_category\"],\n        \"predicted_atlas_cat\": predicted_cat,\n        \"is_clinical\": predicted_cat == \"Clinical/Health\",\n        \"top_cat_counts\": dict(Counter(top_cats))\n    })\n\nclinical_count = sum(1 for r in dsm5_results if r[\"is_clinical\"])\npct = clinical_count / len(dsm5_results) * 100\nprint(f\"DSM-5 Clinical Alignment: {clinical_count}/{len(dsm5_results)} disorders ({pct:.1f}%) route to Clinical/Health\")\nprint(f\"This confirms the atlas taxonomy correctly captures clinical constructs.\\n\")\n\n# Show the few that don't route to Clinical/Health\nnon_clinical = [r for r in dsm5_results if not r[\"is_clinical\"]]\nif non_clinical:\n    print(f\"{len(non_clinical)} disorders route elsewhere:\")\n    for r in non_clinical:\n        print(f\"  {r['disorder'][:60]:<62} → {r['predicted_atlas_cat']}\")\nelse:\n    print(\"All 222 disorders route to Clinical/Health.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Per DSM-5 category breakdown — which clinical domains route where?\ndsm5_cats = sorted(set(r[\"dsm5_category\"] for r in dsm5_results))\n\nprint(f\"{'DSM-5 Category':<45} {'N':>3}  {'Clinical':>8}  {'Other':>5}  {'Pct':>5}\")\nprint(\"-\" * 72)\nfor cat in dsm5_cats:\n    cat_results = [r for r in dsm5_results if r[\"dsm5_category\"] == cat]\n    n = len(cat_results)\n    clin = sum(1 for r in cat_results if r[\"is_clinical\"])\n    other = n - clin\n    pct = clin / n * 100\n    marker = \"\" if pct == 100 else \" *\"\n    print(f\"{cat:<45} {n:>3}  {clin:>8}  {other:>5}  {pct:>4.0f}%{marker}\")\n\nprint(f\"\\n* = categories where some disorders route outside Clinical/Health\")\nprint(f\"Total: {len(dsm5_results)} disorders, {clinical_count} ({pct:.1f}%) route to Clinical/Health\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n**Repository:** [github.com/Wildertrek/survey](https://github.com/Wildertrek/survey) | **3072-dim assets:** [Hugging Face Hub](https://huggingface.co/datasets/Wildertrek/personality-atlas-3072)  \n**Paper:** Raetano, J., Gregor, J., & Tamang, S. (2026). *A Survey and Computational Atlas of Personality Models.* ACM TIST.  \n**License:** MIT",
   "metadata": {}
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}