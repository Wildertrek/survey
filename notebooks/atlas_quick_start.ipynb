{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Personality Atlas — Quick Start\n\nPersonality psychology has a fragmentation problem. Clinical psychologists use the MMPI and SCID. Trait researchers use OCEAN and HEXACO. Organizational consultants reach for DISC or MBTI. Each tradition has its own instruments, its own factor structures, and its own vocabulary — and almost no shared infrastructure for comparing constructs across traditions.\n\nThis notebook walks through a **Computational Atlas** that builds that infrastructure. It embeds 44 personality models — 6,694 traits across 358 factors — into a single vector space where any trait from any tradition can be searched, compared, and classified.\n\n**What you will see:**\n- Load any of the 44 models with the same 10 lines of code (Section 2)\n- Visualize how 7 research traditions organize in embedding space (Section 4)\n- Query a single trait and find related constructs across all 44 models (Section 5)\n- Test whether classifiers generalize to novel items they have never seen (Section 7)\n- Compare 1536 vs 3072-dim embeddings from Hugging Face (Section 8)\n- Route 222 DSM-5 disorders through the atlas as a clinical stress test (Section 9)\n\nEverything runs from pre-computed assets in the repository. No API keys, no database, no waiting.\n\n> Raetano, J., Gregor, J., & Tamang, S. (2026). *A Survey and Computational Atlas of Personality Models.* ACM TIST. Under review.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Wildertrek/survey/blob/main/notebooks/atlas_quick_start.ipynb)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Setup\n\nClone the atlas repository and install FAISS (the only dependency Colab does not already have). Takes about 30 seconds."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clone the atlas repository (skip if already cloned)\nimport os\nif not os.path.exists(\"atlas\"):\n    !git clone --depth 1 https://github.com/Wildertrek/survey.git atlas\nelse:\n    print(\"Atlas already cloned — skipping.\")"
  },
  {
   "cell_type": "code",
   "source": "# Install dependencies (uses Colab's pre-installed sklearn — no version conflicts)\n!pip install -q faiss-cpu\n\n# Suppress sklearn version warning (models trained with 1.5.0, works fine with newer versions)\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Load and Predict — Any of the 44 Models\n\nEvery model in the atlas follows the same file convention: a **dataset CSV** (traits labeled by factor), a **pre-computed embedding CSV** (1,536-dim vectors from OpenAI `text-embedding-3-small`), and a **trained Random Forest** classifier with its label encoder.\n\nChange `MODEL` to any model abbreviation — `ocean`, `hex`, `mmpi`, `scid`, `npi`, `dt4`, `tci`, `stbv`, etc. — and the same code loads, classifies, and reports. That uniformity is the point: one interface for 44 models from seven different research traditions."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Pick any model abbreviation: ocean, hex, mbti, mmpi, scid, npi, dt4, tci, ...\n",
    "MODEL = \"ocean\"\n",
    "\n",
    "df = pd.read_csv(f\"atlas/datasets/{MODEL}.csv\")\n",
    "embeddings = pd.read_csv(f\"atlas/Embeddings/{MODEL}_embeddings.csv\")\n",
    "model = joblib.load(f\"atlas/models/{MODEL}_rf_model.pkl\")\n",
    "encoder = joblib.load(f\"atlas/models/{MODEL}_label_encoder.pkl\")\n",
    "\n",
    "X = np.array([ast.literal_eval(e) for e in embeddings[\"Embedding\"]])\n",
    "predictions = encoder.inverse_transform(model.predict(X))\n",
    "accuracy = (predictions == df[\"Factor\"].values).mean()\n",
    "\n",
    "print(f\"{MODEL.upper()}: {len(df)} traits, {len(set(predictions))} factors, accuracy = {accuracy:.1%}\")\n",
    "print(f\"Factors: {sorted(set(predictions))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Atlas Overview — All 44 Models\n\nHow well does each classifier separate its own model's factors? The loop below runs every model and reports accuracy on the full dataset.\n\nMost models score above 95%. With 1,536 embedding dimensions and only a handful of factors, a Random Forest has plenty of room to find separating boundaries. High accuracy here is a necessary condition — it proves the lexical encoding captures meaningful distinctions between factors — but it is not a generalization test. We address that in Section 7 with items the classifiers have never seen."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "slugs = sorted([f.replace(\".csv\", \"\") for f in os.listdir(\"atlas/datasets\") if f.endswith(\".csv\")])\n",
    "print(f\"Atlas contains {len(slugs)} personality models:\\n\")\n",
    "\n",
    "results = []\n",
    "for slug in slugs:\n",
    "    df = pd.read_csv(f\"atlas/datasets/{slug}.csv\")\n",
    "    n_factors = df[\"Factor\"].nunique()\n",
    "    \n",
    "    model = joblib.load(f\"atlas/models/{slug}_rf_model.pkl\")\n",
    "    enc = joblib.load(f\"atlas/models/{slug}_label_encoder.pkl\")\n",
    "    emb = pd.read_csv(f\"atlas/Embeddings/{slug}_embeddings.csv\")\n",
    "    X = np.array([ast.literal_eval(e) for e in emb[\"Embedding\"]])\n",
    "    preds = enc.inverse_transform(model.predict(X))\n",
    "    acc = (preds == df[\"Factor\"].values).mean()\n",
    "    \n",
    "    results.append({\"Model\": slug.upper(), \"Traits\": len(df), \"Factors\": n_factors, \"Accuracy\": f\"{acc:.1%}\"})\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"Accuracy\", ascending=False)\n",
    "results_df.index = range(1, len(results_df) + 1)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Cross-Model PCA — How 6,694 Traits Organize in Embedding Space\n\nStack every trait from all 44 models into one matrix and run PCA. Two patterns emerge:\n\n**Theoretical neighbors become geometric neighbors.** Clinical models (MMPI, SCID, BDI) cluster together. Narcissism instruments (NPI, PNI, Dark Triad) form their own region. Trait models (OCEAN, HEXACO) land nearby. The embedding space recovers relationships that were never explicitly encoded — they fall out of the geometry.\n\n**The space is genuinely high-dimensional.** 50 principal components capture a modest share of total variance, meaning personality traits spread across many independent semantic directions. That is consistent with constructs that evolved independently across seven research traditions over the past century.\n\nThe three figures below reproduce the paper's PCA analysis (Figures 5-8)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# Category assignments for the 7-category taxonomy\nCATEGORIES = {\n    \"Trait-Based\": [\"ocean\", \"hex\", \"mbti\", \"epm\", \"sixteenpf\", \"ftm\"],\n    \"Narcissism-Based\": [\"npi\", \"pni\", \"hsns\", \"dtm\", \"dt4\", \"ffni\", \"ffni_sf\", \"narq\", \"mcmin\", \"ipn\"],\n    \"Motivational/Value\": [\"stbv\", \"sdt\", \"rft\", \"aam\", \"mst\", \"cs\"],\n    \"Cognitive/Learning\": [\"pct\", \"cest\", \"scm\", \"fsls\"],\n    \"Clinical/Health\": [\"mmpi\", \"scid\", \"bdi\", \"gad7\", \"wais\", \"tci\", \"mcmi\", \"tmp\", \"rit\", \"tat\"],\n    \"Interpersonal/Conflict\": [\"disc\", \"tki\"],\n    \"Application-Specific\": [\"riasec\", \"cmoa\", \"tei\", \"bt\", \"em\", \"papc\"]\n}\nslug_to_cat = {s: c for c, slugs in CATEGORIES.items() for s in slugs}\n\n# Load all embeddings\nall_vecs, all_labels, all_cats = [], [], []\nfor slug in slugs:\n    df = pd.read_csv(f\"atlas/datasets/{slug}.csv\")\n    emb = pd.read_csv(f\"atlas/Embeddings/{slug}_embeddings.csv\")\n    X = np.array([ast.literal_eval(e) for e in emb[\"Embedding\"]])\n    all_vecs.append(X)\n    all_labels.extend([slug.upper()] * len(X))\n    all_cats.extend([slug_to_cat.get(slug, \"Unknown\")] * len(X))\n\nX_all = np.vstack(all_vecs)\nn_unknown = sum(1 for c in all_cats if c == \"Unknown\")\nprint(f\"Loaded {X_all.shape[0]} embeddings ({X_all.shape[1]}-dim) from {len(slugs)} models\")\nif n_unknown > 0:\n    unknown_models = sorted(set(lbl for lbl, cat in zip(all_labels, all_cats) if cat == \"Unknown\"))\n    print(f\"WARNING: {n_unknown} embeddings from {len(unknown_models)} models have Unknown category: {unknown_models}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA — Scree plot (Paper Figure 5)\n",
    "pca = PCA(n_components=50)\n",
    "X_pca = pca.fit_transform(X_all)\n",
    "cumvar = np.cumsum(pca.explained_variance_ratio_) * 100\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.bar(range(1, 51), pca.explained_variance_ratio_ * 100, alpha=0.6, label=\"Individual\")\n",
    "ax.plot(range(1, 51), cumvar, \"r-o\", markersize=3, label=\"Cumulative\")\n",
    "ax.axhline(y=cumvar[-1], color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "ax.set_xlabel(\"Principal Component\")\n",
    "ax.set_ylabel(\"Variance Explained (%)\")\n",
    "ax.set_title(f\"PCA Scree Plot — {X_all.shape[0]} Trait Embeddings from 44 Models\\n50 PCs capture {cumvar[-1]:.1f}% of variance\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# PCA — Model centroids in PC1-PC2 space (Paper Figure 6)\n# Colors match the paper's PCA figures (01_cross_model_pca_analysis.py)\ncat_colors = {\n    \"Trait-Based\": \"#1f77b4\", \"Narcissism-Based\": \"#9467bd\",\n    \"Motivational/Value\": \"#2ca02c\", \"Cognitive/Learning\": \"#ff7f0e\",\n    \"Clinical/Health\": \"#d62728\", \"Interpersonal/Conflict\": \"#e377c2\",\n    \"Application-Specific\": \"#7f7f7f\"\n}\n\ncentroids = pd.DataFrame({\n    \"PC1\": X_pca[:, 0], \"PC2\": X_pca[:, 1],\n    \"Model\": all_labels, \"Category\": all_cats\n}).groupby([\"Model\", \"Category\"])[[\"PC1\", \"PC2\"]].mean().reset_index()\n\nfig, ax = plt.subplots(figsize=(12, 8))\nfor cat, color in cat_colors.items():\n    subset = centroids[centroids[\"Category\"] == cat]\n    ax.scatter(subset[\"PC1\"], subset[\"PC2\"], c=color, s=80, label=cat, alpha=0.8, edgecolors=\"white\", linewidth=0.5)\n    for _, row in subset.iterrows():\n        ax.annotate(row[\"Model\"], (row[\"PC1\"], row[\"PC2\"]), fontsize=7, alpha=0.7,\n                    xytext=(4, 4), textcoords=\"offset points\")\n\nax.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)\")\nax.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)\")\nax.set_title(\"44 Personality Models in PC1-PC2 Space (Model Centroids)\")\nax.legend(loc=\"best\", fontsize=8)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# PCA — All traits colored by category (Paper Figure 8)\nfig, ax = plt.subplots(figsize=(12, 8))\nfor cat, color in cat_colors.items():\n    mask = [c == cat for c in all_cats]\n    ax.scatter(X_pca[mask, 0], X_pca[mask, 1], c=color, s=4, alpha=0.3, label=cat)\n\nax.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)\")\nax.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)\")\nax.set_title(f\"All {X_all.shape[0]} Trait Embeddings — 44 Models, 7 Categories\")\nax.legend(markerscale=5, fontsize=8)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Cross-Model Search with FAISS\n\nThis is what the atlas is for. With every trait from every model in a single FAISS index, you can take any personality construct and retrieve its nearest neighbors *across all 44 models and all 7 categories*.\n\nA clinician studying depression can query a BDI item and find related constructs in the MMPI, TCI, and motivational models. A researcher working on narcissism can trace how Machiavellianism connects to MMPI Psychopathic Deviate and OCEAN low Agreeableness. These cross-tradition connections would normally require deep expertise spanning multiple literatures. The atlas finds them in milliseconds.\n\nBelow we build the index (cosine similarity via inner product on L2-normalized vectors) and run two example queries."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# Build FAISS index over entire atlas\n",
    "X_norm = X_all / np.linalg.norm(X_all, axis=1, keepdims=True)\n",
    "index = faiss.IndexFlatIP(X_norm.shape[1])\n",
    "index.add(X_norm.astype(np.float32))\n",
    "\n",
    "# Build metadata for lookups\n",
    "all_factors = []\n",
    "for slug in slugs:\n",
    "    df = pd.read_csv(f\"atlas/datasets/{slug}.csv\")\n",
    "    all_factors.extend(df[\"Factor\"].values)\n",
    "\n",
    "print(f\"FAISS index: {index.ntotal} vectors, {X_norm.shape[1]}-dim\")\n",
    "print(f\"Ready for cross-model personality search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Query: find similar traits across all 44 models\n# This demonstrates cross-category retrieval — the atlas's core value\nquery_slug = \"ocean\"\nquery_factor = \"Extraversion\"\n\nquery_df = pd.read_csv(f\"atlas/datasets/{query_slug}.csv\")\nquery_emb = pd.read_csv(f\"atlas/Embeddings/{query_slug}_embeddings.csv\")\n\n# Find first row matching the target factor\nidx = query_df[query_df[\"Factor\"] == query_factor].index[0]\nq = np.array([ast.literal_eval(query_emb[\"Embedding\"].iloc[idx])]).astype(np.float32)\nq = q / np.linalg.norm(q)\n\nD, I = index.search(q, 20)\n\nquery_trait = query_df.iloc[idx]\nprint(f\"Query: {query_slug.upper()} / {query_factor} — \\\"{query_trait['Adjective']}\\\"\\n\")\nprint(f\"{'Rank':<5} {'Model':<12} {'Factor':<35} {'Category':<22} {'Score':.5}\")\nprint(\"-\" * 85)\nfor rank, (i, score) in enumerate(zip(I[0], D[0]), 1):\n    print(f\"{rank:<5} {all_labels[i]:<12} {all_factors[i]:<35} {all_cats[i]:<22} {score:.4f}\")"
  },
  {
   "cell_type": "code",
   "source": "# Second query: Dark Triad Machiavellianism → cross-category retrieval\n# Shows how a narcissism-based construct connects to clinical, trait, and motivational models\nquery_slug = \"dtm\"\nquery_factor = \"Machiavellianism\"\n\nquery_df = pd.read_csv(f\"atlas/datasets/{query_slug}.csv\")\nquery_emb = pd.read_csv(f\"atlas/Embeddings/{query_slug}_embeddings.csv\")\n\nidx = query_df[query_df[\"Factor\"] == query_factor].index[0]\nq = np.array([ast.literal_eval(query_emb[\"Embedding\"].iloc[idx])]).astype(np.float32)\nq = q / np.linalg.norm(q)\n\nD, I = index.search(q, 20)\n\nquery_trait = query_df.iloc[idx]\nprint(f\"Query: {query_slug.upper()} / {query_factor} — \\\"{query_trait['Adjective']}\\\"\\n\")\nprint(f\"{'Rank':<5} {'Model':<12} {'Factor':<35} {'Category':<22} {'Score':.5}\")\nprint(\"-\" * 85)\nfor rank, (i, score) in enumerate(zip(I[0], D[0]), 1):\n    print(f\"{rank:<5} {all_labels[i]:<12} {all_factors[i]:<35} {all_cats[i]:<22} {score:.4f}\")\n\n# Count unique categories and models in results\nresult_cats = set(all_cats[i] for i in I[0])\nresult_models = set(all_labels[i] for i in I[0])\nprint(f\"\\n→ {len(result_cats)} categories, {len(result_models)} models in top 20 — cross-tradition retrieval\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Lexical Schema — What's Inside Each Model\n\nEach model's dataset is a table of traits. Every trait has five columns: **Adjective** (the root descriptor), **Synonym**, **Verb**, **Noun**, and **Factor** (the classification label). This five-column lexical encoding gives each trait a rich semantic surface for the embedding model to work with — richer than single-word labels, but grounded in actual psychometric vocabulary.\n\nChange `INSPECT` below to browse any model's factor structure and sample adjectives."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Inspect any model's lexical schema\nINSPECT = \"hex\"  # change to any slug (ocean, hex, mbti, mmpi, scid, npi, dt4, tci, ...)\n\ndf = pd.read_csv(f\"atlas/datasets/{INSPECT}.csv\")\nprint(f\"{INSPECT.upper()}: {len(df)} traits across {df['Factor'].nunique()} factors\\n\")\n\nfor factor, group in df.groupby(\"Factor\"):\n    unique_adj = group[\"Adjective\"].unique()[:5]\n    print(f\"  {factor} ({len(group)} traits): {', '.join(unique_adj)}, ...\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Experiment 1 — Novel Item Evaluation\n\nThe accuracy numbers in Section 3 measure how well each classifier separates its own training lexicon. That is a reproduction check, not a generalization test. Models with few factors relative to 1,536 embedding dimensions can reach 100% by memorizing the training set.\n\nThe real question: **can these classifiers recognize personality items they have never seen?**\n\nWe test on **5,052 novel items** generated independently by GPT-4o from factor definitions alone, without access to the training data. Pre-computed 1,536-dim embeddings ship with the repository, so this runs without an API key. The mean accuracy across all 44 models is the generalization number reported in the paper.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\nfrom sklearn.metrics import accuracy_score\n\n# Load pre-computed test items (generated by GPT-4o, embedded offline)\ntest_items = json.load(open(\"atlas/data/test_items/test_items.json\"))\ntest_emb = np.load(\"atlas/data/test_items/test_items_embeddings.npz\")[\"embeddings\"]\n\n# Evaluate all 44 models on novel items\nnovel_results = []\nfor slug in slugs:\n    model_rf = joblib.load(f\"atlas/models/{slug}_rf_model.pkl\")\n    enc = joblib.load(f\"atlas/models/{slug}_label_encoder.pkl\")\n    \n    idx = [i for i, item in enumerate(test_items) if item[\"slug\"] == slug]\n    if not idx:\n        continue\n    \n    X_novel = test_emb[idx]\n    y_true = [test_items[i][\"expected_factor\"] for i in idx]\n    y_pred = enc.inverse_transform(model_rf.predict(X_novel))\n    acc = accuracy_score(y_true, y_pred)\n    novel_results.append({\"Model\": slug.upper(), \"Items\": len(idx), \"Novel Accuracy\": f\"{acc:.1%}\", \"_acc\": acc})\n\nnovel_df = pd.DataFrame(novel_results).sort_values(\"_acc\", ascending=False).drop(columns=[\"_acc\"])\nnovel_df.index = range(1, len(novel_df) + 1)\n\nmean_novel = np.mean([r[\"_acc\"] for r in novel_results])\nprint(f\"Mean novel-item accuracy: {mean_novel:.1%} (vs. ~98% on training data)\")\nprint(f\"This is the generalization accuracy reported in the paper.\\n\")\nnovel_df",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 8. Experiment 2 — 3072-dim Embedding Upgrade\n\nOpenAI's `text-embedding-3-large` produces 3,072-dim vectors — twice the resolution of the default. Does doubling the embedding dimensionality improve factor classification?\n\nThe upgraded embeddings (440 MB) and retrained classifiers (107 MB) are hosted on [Hugging Face Hub](https://huggingface.co/datasets/Wildertrek/personality-atlas-3072). The cells below download them on demand and compare accuracy side by side — first on training data, then on the novel test items.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "!pip install -q huggingface_hub\n\nimport os\nos.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", message=\".*HF_TOKEN.*\")\nwarnings.filterwarnings(\"ignore\", message=\".*unauthenticated.*\")\n\nfrom huggingface_hub import hf_hub_download\n\nHF_REPO = \"Wildertrek/personality-atlas-3072\"\n\ndef load_3072(slug):\n    \"\"\"Download and load 3072-dim assets for a single model from HuggingFace.\"\"\"\n    emb_path = hf_hub_download(HF_REPO, f\"Embeddings_3072/{slug}_embeddings.csv\", repo_type=\"dataset\")\n    model_path = hf_hub_download(HF_REPO, f\"models_3072/{slug}_rf_model.pkl\", repo_type=\"dataset\")\n    enc_path = hf_hub_download(HF_REPO, f\"models_3072/{slug}_label_encoder.pkl\", repo_type=\"dataset\")\n    emb_df = pd.read_csv(emb_path)\n    X = np.array([ast.literal_eval(e) for e in emb_df[\"Embedding\"]])\n    return X, joblib.load(model_path), joblib.load(enc_path)\n\nprint(\"Ready to download 3072-dim assets from HuggingFace.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Compare 1536 vs 3072 accuracy across all 44 models\ncomparison = []\nfor slug in slugs:\n    df = pd.read_csv(f\"atlas/datasets/{slug}.csv\")\n    y_true = df[\"Factor\"].values\n\n    # 1536-dim (already loaded from repo)\n    m1536 = joblib.load(f\"atlas/models/{slug}_rf_model.pkl\")\n    e1536 = joblib.load(f\"atlas/models/{slug}_label_encoder.pkl\")\n    emb1536 = pd.read_csv(f\"atlas/Embeddings/{slug}_embeddings.csv\")\n    X1536 = np.array([ast.literal_eval(e) for e in emb1536[\"Embedding\"]])\n    acc1536 = (e1536.inverse_transform(m1536.predict(X1536)) == y_true).mean()\n\n    # 3072-dim (downloaded from HuggingFace)\n    X3072, m3072, e3072 = load_3072(slug)\n    acc3072 = (e3072.inverse_transform(m3072.predict(X3072)) == y_true).mean()\n\n    delta = (acc3072 - acc1536) * 100\n    comparison.append({\n        \"Model\": slug.upper(),\n        \"1536-dim\": f\"{acc1536:.1%}\",\n        \"3072-dim\": f\"{acc3072:.1%}\",\n        \"Delta\": f\"{delta:+.1f}pp\",\n        \"_delta\": delta,  # numeric for sorting\n    })\n\ncomp_df = pd.DataFrame(comparison).sort_values(\"_delta\", ascending=False).drop(columns=[\"_delta\"])\ncomp_df.index = range(1, len(comp_df) + 1)\n\nimproved = sum(1 for c in comparison if c[\"_delta\"] > 0.05)\ndecreased = sum(1 for c in comparison if c[\"_delta\"] < -0.05)\nunchanged = len(comparison) - improved - decreased\nmean_1536 = np.mean([float(c[\"1536-dim\"].rstrip(\"%\")) for c in comparison])\nmean_3072 = np.mean([float(c[\"3072-dim\"].rstrip(\"%\")) for c in comparison])\n\nprint(f\"1536-dim mean: {mean_1536:.1f}% | 3072-dim mean: {mean_3072:.1f}% | Delta: {mean_3072 - mean_1536:+.1f}pp\")\nprint(f\"{improved} improved, {decreased} decreased, {unchanged} unchanged\\n\")\ncomp_df",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Novel item accuracy: 1536-dim vs 3072-dim\n\nThe comparison above used each model's own training data, where both embedding sizes tend to score high. The more informative test: run both classifiers on the **5,052 novel items** from Section 7 — items neither classifier has seen during training. Pre-computed 3072-dim embeddings for these items are downloaded from HuggingFace.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Novel item accuracy: 1536-dim vs 3072-dim on truly novel test items\n# Downloads 3072-dim test item embeddings from HuggingFace (no API key needed)\ntest3072_path = hf_hub_download(HF_REPO, \"test_items/test_items_embeddings_3072.npz\", repo_type=\"dataset\")\ntest_emb_3072 = np.load(test3072_path)[\"embeddings\"]\n\nnovel_comparison = []\nfor slug in slugs:\n    m1536 = joblib.load(f\"atlas/models/{slug}_rf_model.pkl\")\n    e1536 = joblib.load(f\"atlas/models/{slug}_label_encoder.pkl\")\n    X3072_m, m3072, e3072 = load_3072(slug)\n\n    idx = [i for i, item in enumerate(test_items) if item[\"slug\"] == slug]\n    if not idx:\n        continue\n\n    y_true = [test_items[i][\"expected_factor\"] for i in idx]\n    acc_n1536 = accuracy_score(y_true, e1536.inverse_transform(m1536.predict(test_emb[idx])))\n    acc_n3072 = accuracy_score(y_true, e3072.inverse_transform(m3072.predict(test_emb_3072[idx])))\n    delta = (acc_n3072 - acc_n1536) * 100\n    novel_comparison.append({\n        \"Model\": slug.upper(), \"Items\": len(idx),\n        \"1536-dim\": f\"{acc_n1536:.1%}\", \"3072-dim\": f\"{acc_n3072:.1%}\",\n        \"Delta\": f\"{delta:+.1f}pp\", \"_delta\": delta\n    })\n\nnc_df = pd.DataFrame(novel_comparison).sort_values(\"_delta\", ascending=False).drop(columns=[\"_delta\"])\nnc_df.index = range(1, len(nc_df) + 1)\n\nmean_n1536 = np.mean([float(c[\"1536-dim\"].rstrip(\"%\")) for c in novel_comparison])\nmean_n3072 = np.mean([float(c[\"3072-dim\"].rstrip(\"%\")) for c in novel_comparison])\nimproved = sum(1 for c in novel_comparison if c[\"_delta\"] > 0.05)\ndecreased = sum(1 for c in novel_comparison if c[\"_delta\"] < -0.05)\nunchanged = len(novel_comparison) - improved - decreased\n\nprint(f\"Novel item accuracy: 1536-dim mean: {mean_n1536:.1f}% | 3072-dim mean: {mean_n3072:.1f}% | Delta: {mean_n3072 - mean_n1536:+.1f}pp\")\nprint(f\"{improved} improved, {decreased} decreased, {unchanged} unchanged\\n\")\nnc_df",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 9. Experiment 3 — DSM-5 Clinical Alignment\n\nThe atlas was built from personality research instruments, not clinical diagnostic tools. But personality and psychopathology overlap heavily — the DSM-5 includes dimensional personality models, and clinical instruments like the MMPI and SCID already sit in the atlas.\n\nIf the taxonomy is well-structured, clinical constructs from outside the training data should route to the correct category. We test this by embedding all **222 DSM-5-TR disorders** (using each disorder's name and diagnostic keywords) and querying them against the atlas FAISS index. For each disorder, the top-10 nearest traits vote on a category.\n\nThe question: what fraction land in Clinical/Health? Data and pre-computed embeddings are included in the repository.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Classify 222 DSM-5-TR disorders through the atlas\n# Uses pre-computed embeddings — no API key needed\nimport json\nfrom collections import Counter\n\ndsm5 = json.load(open(\"atlas/data/dsm5_disorders.json\"))\ndsm5_emb = pd.read_csv(\"atlas/data/dsm5_embeddings.csv\")\ndsm5_vecs = np.array([ast.literal_eval(e) for e in dsm5_emb[\"Embedding\"]]).astype(np.float32)\ndsm5_vecs = dsm5_vecs / np.linalg.norm(dsm5_vecs, axis=1, keepdims=True)\n\n# Query each disorder against the atlas FAISS index (built in Section 5)\ndsm5_results = []\nfor i, disorder in enumerate(dsm5):\n    q = dsm5_vecs[i:i+1]\n    D, I = index.search(q, 10)\n    top_cats = [all_cats[j] for j in I[0]]\n    predicted_cat = Counter(top_cats).most_common(1)[0][0]\n    dsm5_results.append({\n        \"disorder\": disorder[\"disorder_name\"],\n        \"dsm5_category\": disorder[\"dsm5_category\"],\n        \"predicted_atlas_cat\": predicted_cat,\n        \"is_clinical\": predicted_cat == \"Clinical/Health\",\n        \"top_cat_counts\": dict(Counter(top_cats))\n    })\n\nclinical_count = sum(1 for r in dsm5_results if r[\"is_clinical\"])\npct = clinical_count / len(dsm5_results) * 100\nprint(f\"DSM-5 Clinical Alignment: {clinical_count}/{len(dsm5_results)} disorders ({pct:.1f}%) route to Clinical/Health\")\nprint(f\"This confirms the atlas taxonomy correctly captures clinical constructs.\\n\")\n\n# Show the few that don't route to Clinical/Health\nnon_clinical = [r for r in dsm5_results if not r[\"is_clinical\"]]\nif non_clinical:\n    print(f\"{len(non_clinical)} disorders route elsewhere:\")\n    for r in non_clinical:\n        print(f\"  {r['disorder'][:60]:<62} → {r['predicted_atlas_cat']}\")\nelse:\n    print(\"All 222 disorders route to Clinical/Health.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Per DSM-5 category breakdown — which clinical domains route where?\ndsm5_cats = sorted(set(r[\"dsm5_category\"] for r in dsm5_results))\n\nprint(f\"{'DSM-5 Category':<45} {'N':>3}  {'Clinical':>8}  {'Other':>5}  {'Pct':>5}\")\nprint(\"-\" * 72)\nfor cat in dsm5_cats:\n    cat_results = [r for r in dsm5_results if r[\"dsm5_category\"] == cat]\n    n = len(cat_results)\n    clin = sum(1 for r in cat_results if r[\"is_clinical\"])\n    other = n - clin\n    cat_pct = clin / n * 100\n    marker = \"\" if cat_pct == 100 else \" *\"\n    print(f\"{cat:<45} {n:>3}  {clin:>8}  {other:>5}  {cat_pct:>4.0f}%{marker}\")\n\noverall_pct = clinical_count / len(dsm5_results) * 100\nprint(f\"\\n* = categories where some disorders route outside Clinical/Health\")\nprint(f\"Total: {len(dsm5_results)} disorders, {clinical_count} ({overall_pct:.1f}%) route to Clinical/Health\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\nThe full pipeline — from raw psychometric instruments to a searchable, classifiable embedding space — is documented in the paper and fully reproducible from this repository.\n\n**Want to go deeper?** The [Deep Dive notebook](https://colab.research.google.com/github/Wildertrek/survey/blob/main/notebooks/atlas_deep_dive.ipynb) lets you pick any single model and take it apart: train a classifier from scratch, inspect the confusion matrix, visualize embedding geometry, and measure inter-factor similarity.\n\n**Repository:** [github.com/Wildertrek/survey](https://github.com/Wildertrek/survey) | **3072-dim assets:** [Hugging Face Hub](https://huggingface.co/datasets/Wildertrek/personality-atlas-3072)\n**Paper:** Raetano, J., Gregor, J., & Tamang, S. (2026). *A Survey and Computational Atlas of Personality Models.* ACM TIST.\n**License:** MIT",
   "metadata": {}
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}