{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Personality Atlas — Deep Dive\n\nThe [Quick Start notebook](https://colab.research.google.com/github/Wildertrek/survey/blob/main/notebooks/atlas_quick_start.ipynb) covers the atlas at a high level — 44 models, cross-model PCA, FAISS search, DSM-5 validation. This notebook goes the other direction: **pick one model and take it apart.**\n\nYou will train a classifier from scratch, inspect where it fails, run cross-validation, visualize the embedding space with PCA and KMeans, identify which embedding dimensions carry the most weight, measure how similar the factors are to each other, and then step outside the model to see how its constructs relate to the other 43 models in the atlas.\n\nChange `MODEL` in Section 2 and every cell adapts automatically. No API keys required.\n\n> Raetano, J., Gregor, J., & Tamang, S. (2026). *A Survey and Computational Atlas of Personality Models.* ACM TIST. Under review.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Wildertrek/survey/blob/main/notebooks/atlas_deep_dive.ipynb)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Setup"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"atlas\"):\n",
    "    !git clone --depth 1 https://github.com/Wildertrek/survey.git atlas\n",
    "else:\n",
    "    print(\"Atlas already cloned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q faiss-cpu\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")\n",
    "\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import faiss\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "print(\"All libraries loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Select a Model\n\nChange `MODEL` below to any of the 44 model abbreviations. Every subsequent cell adapts automatically — factor distributions, PCA projections, confusion matrices, cross-model searches, and accuracy comparisons all update to reflect the selected model.\n\n| Category | Models |\n|----------|--------|\n| **Trait-Based** | `ocean` (Big Five), `hex` (HEXACO), `mbti`, `epm` (Eysenck), `sixteenpf`, `ftm` (Four Temperaments) |\n| **Narcissism-Based** | `npi`, `pni`, `ffni`, `ffni_sf`, `narq`, `hsns`, `dtm` (Dark Triad), `dt4` (Dark Tetrad), `mcmin`, `ipn` |\n| **Motivational/Value** | `stbv` (Schwartz Values), `sdt` (Self-Determination), `rft`, `aam`, `mst`, `cs` (Clifton) |\n| **Cognitive/Learning** | `pct`, `scm`, `cest`, `fsls` (Felder-Silverman) |\n| **Clinical/Health** | `mmpi`, `scid` (DSM), `bdi` (Depression), `gad7` (Anxiety), `wais`, `tci`, `mcmi`, `tmp`, `rit` (Rorschach), `tat` |\n| **Interpersonal/Conflict** | `disc`, `tki` (Thomas-Kilmann) |\n| **Application-Specific** | `riasec` (Holland Careers), `cmoa`, `tei`, `bt` (Bartle Types), `em` (Enneagram), `papc` |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# CHANGE THIS to explore a different model\nMODEL = \"ocean\"\n# ============================================\n\n# Load dataset, embeddings, pre-trained model, and label encoder\ndf = pd.read_csv(f\"atlas/datasets/{MODEL}.csv\")\nemb_df = pd.read_csv(f\"atlas/Embeddings/{MODEL}_embeddings.csv\")\nclf = joblib.load(f\"atlas/models/{MODEL}_rf_model.pkl\")\nle = joblib.load(f\"atlas/models/{MODEL}_label_encoder.pkl\")\n\n# Parse embeddings from CSV strings to numpy array\nX = np.array([ast.literal_eval(e) for e in emb_df[\"Embedding\"]])\ny = df[\"Factor\"].values\ny_encoded = le.transform(y)\n\nprint(f\"Model: {MODEL.upper()}\")\nprint(f\"Traits: {len(df)}, Factors: {df['Factor'].nunique()}, Embedding dim: {X.shape[1]}\")\nprint(f\"Factors: {sorted(df['Factor'].unique())}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Dataset Inspection\n\nBefore building any model, look at the data. How many traits does each factor have? Are the factors balanced or heavily skewed?\n\nImbalanced factors matter: a classifier that always predicts the majority class can score high accuracy while learning nothing about the minority factors. The distribution below tells you whether to expect that problem."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factor distribution\n",
    "factor_counts = df[\"Factor\"].value_counts().sort_index()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart\n",
    "factor_counts.plot(kind=\"barh\", ax=axes[0], color=sns.color_palette(\"husl\", len(factor_counts)))\n",
    "axes[0].set_xlabel(\"Trait Count\")\n",
    "axes[0].set_title(f\"{MODEL.upper()} — Factor Distribution ({len(df)} traits)\")\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(factor_counts, labels=factor_counts.index, autopct=\"%1.0f%%\",\n",
    "            colors=sns.color_palette(\"husl\", len(factor_counts)))\n",
    "axes[1].set_title(f\"{MODEL.upper()} — Factor Proportions\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexical schema — sample traits per factor\n",
    "print(f\"{MODEL.upper()}: {len(df)} traits across {df['Factor'].nunique()} factors\\n\")\n",
    "\n",
    "for factor, group in df.groupby(\"Factor\"):\n",
    "    sample = group[\"Adjective\"].unique()[:6]\n",
    "    print(f\"  {factor} ({len(group)} traits): {', '.join(sample)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Train a Random Forest Classifier\n\nEach trait is represented as a 1,536-dim embedding vector from OpenAI's `text-embedding-3-small`. The classification task: given an embedding, predict which factor it belongs to.\n\nWe train a fresh Random Forest using an 80/20 stratified split with the same hyperparameters as the pre-trained model shipped in the repository. If the freshly trained accuracy matches the pre-trained model, the result is reproducible — the numbers in the paper are not an artifact of a lucky random seed.\n\n**Expect high accuracy.** For models with few well-separated factors (like OCEAN's five), 100% is normal — 1,536 dimensions is more than enough capacity to separate five clusters with distinct vocabularies. This is a sanity check, not the hard test. The hard test comes in Section 13b, where we evaluate on items the classifier has never seen."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80/20 stratified split — same parameters as original notebooks\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Train fresh RF (same hyperparameters as the pre-trained models)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test accuracy: {acc:.1%} ({sum(y_pred == y_test)}/{len(y_test)} correct)\\n\")\n",
    "\n",
    "# Compare with pre-trained model from repo\n",
    "y_pred_pretrained = clf.predict(X_test)\n",
    "acc_pretrained = accuracy_score(y_test, y_pred_pretrained)\n",
    "print(f\"Pre-trained model accuracy: {acc_pretrained:.1%}\")\n",
    "print(f\"Freshly trained accuracy:   {acc:.1%}\")\n",
    "if abs(acc - acc_pretrained) < 0.001:\n",
    "    print(\"Identical — reproduced successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full classification report\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Confusion Matrix\n\nIf you are running the default model (OCEAN), the matrix below is probably solid blue along the diagonal — 100% accuracy, zero confusion. That is the correct result. Five factors with distinct vocabularies (\"anxious, moody, tense\" vs. \"organized, disciplined, methodical\") are trivially separable in a 1,536-dim space. A classifier that *couldn't* separate them would signal a problem with the embeddings, not a hard task.\n\nThe confusion matrix becomes genuinely informative when you select a model with more factors and semantic overlap — try `mmpi`, `scid`, or `ffni`. Off-diagonal entries there reveal factor pairs that share enough vocabulary to confuse the classifier, which often corresponds to constructs that psychometricians themselves debate splitting or merging.\n\nThe normalized view (right panel) shows per-factor recall regardless of class size, which matters when factors are imbalanced.\n\n**Keep this in mind for later:** 100% on a model's own training lexicon does not mean 100% on items it has never seen. Section 13b runs that harder test."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_df = pd.DataFrame(cm, index=le.classes_, columns=le.classes_)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Raw counts\n",
    "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[0])\n",
    "axes[0].set_xlabel(\"Predicted\")\n",
    "axes[0].set_ylabel(\"Actual\")\n",
    "axes[0].set_title(f\"{MODEL.upper()} — Confusion Matrix (counts)\")\n",
    "\n",
    "# Normalized (percentages per row)\n",
    "cm_norm = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n",
    "cm_norm_df = pd.DataFrame(cm_norm, index=le.classes_, columns=le.classes_)\n",
    "sns.heatmap(cm_norm_df, annot=True, fmt=\".0%\", cmap=\"Blues\", ax=axes[1])\n",
    "axes[1].set_xlabel(\"Predicted\")\n",
    "axes[1].set_ylabel(\"Actual\")\n",
    "axes[1].set_title(f\"{MODEL.upper()} — Confusion Matrix (row-normalized)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Cross-Validation\n\nA single 80/20 split can be lucky or unlucky depending on which traits land in the test set. Five-fold cross-validation gives a more stable estimate: every trait appears in the test set exactly once across the five folds.\n\nFor OCEAN and other models with few, well-separated factors, expect all five folds to hit 100% with zero variance. That is not a sign of overfitting — it means the factor boundaries are consistent regardless of which examples the classifier trains on. The embedding geometry genuinely separates these constructs.\n\nThe diagnostic value of cross-validation appears with harder models. Select `mmpi` or `scid` and re-run: you will see fold-to-fold variance of several percentage points, revealing factors that sit near decision boundaries and are sensitive to which specific traits end up in each fold.\n\n**The narrative arc so far:** Sections 4-6 confirm the atlas correctly encodes what psychometrics already knows — factor structure is preserved in the embedding space. Section 12 will show that difficulty varies across the 44 models, and Section 13b will test whether classifiers generalize beyond their own training lexicon."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_val_score(\n",
    "    RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    X, y_encoded, cv=5, scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "print(f\"5-Fold Cross-Validation for {MODEL.upper()}:\")\n",
    "print(f\"  Fold scores: {', '.join(f'{s:.1%}' for s in cv_scores)}\")\n",
    "print(f\"  Mean: {cv_scores.mean():.1%} (+/- {cv_scores.std() * 2:.1%})\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.bar(range(1, 6), cv_scores, color=sns.color_palette(\"husl\", 5), alpha=0.8)\n",
    "ax.axhline(y=cv_scores.mean(), color=\"red\", linestyle=\"--\", label=f\"Mean = {cv_scores.mean():.1%}\")\n",
    "ax.set_xlabel(\"Fold\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(f\"{MODEL.upper()} — 5-Fold Cross-Validation\")\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. PCA — Embedding Space Visualization\n\nPCA projects the 1,536-dim embedding space down to dimensions we can actually look at. Three views, each answering a different question:\n\n- **Scree plot** (left): How concentrated is the variance? A steep initial drop followed by a long tail means a few directions carry most of the factor structure. A flatter curve means the structure is spread across many dimensions.\n- **PC1 vs PC2** (center): Do the factors form visually distinct clusters, or do they overlap? Well-separated clusters predict high classifier accuracy; overlapping clouds predict confusion matrix off-diagonals.\n- **PC1 vs PC3** (right): Factors that overlap in PC2 sometimes separate along PC3. Always check more than one projection before concluding that factors are inseparable."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10)\n",
    "X_pca = pca.fit_transform(X)\n",
    "cumvar = np.cumsum(pca.explained_variance_ratio_) * 100\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Scree plot\n",
    "axes[0].bar(range(1, 11), pca.explained_variance_ratio_ * 100, alpha=0.6, label=\"Individual\")\n",
    "axes[0].plot(range(1, 11), cumvar, \"r-o\", markersize=4, label=\"Cumulative\")\n",
    "axes[0].set_xlabel(\"PC\")\n",
    "axes[0].set_ylabel(\"Variance Explained (%)\")\n",
    "axes[0].set_title(f\"{MODEL.upper()} — Scree Plot\")\n",
    "axes[0].legend(fontsize=8)\n",
    "\n",
    "# PC1 vs PC2 colored by factor\n",
    "colors = sns.color_palette(\"husl\", df[\"Factor\"].nunique())\n",
    "for i, factor in enumerate(sorted(df[\"Factor\"].unique())):\n",
    "    mask = y == factor\n",
    "    axes[1].scatter(X_pca[mask, 0], X_pca[mask, 1], c=[colors[i]], s=30, alpha=0.7, label=factor)\n",
    "axes[1].set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)\")\n",
    "axes[1].set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)\")\n",
    "axes[1].set_title(f\"{MODEL.upper()} — PC1 vs PC2\")\n",
    "axes[1].legend(fontsize=7, loc=\"best\", ncol=max(1, df['Factor'].nunique() // 8 + 1))\n",
    "\n",
    "# PC1 vs PC3\n",
    "for i, factor in enumerate(sorted(df[\"Factor\"].unique())):\n",
    "    mask = y == factor\n",
    "    axes[2].scatter(X_pca[mask, 0], X_pca[mask, 2], c=[colors[i]], s=30, alpha=0.7, label=factor)\n",
    "axes[2].set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)\")\n",
    "axes[2].set_ylabel(f\"PC3 ({pca.explained_variance_ratio_[2]*100:.1f}%)\")\n",
    "axes[2].set_title(f\"{MODEL.upper()} — PC1 vs PC3\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFirst 10 PCs capture {cumvar[-1]:.1f}% of variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. KMeans Clustering — Do Factors Emerge Without Labels?\n\nIf the embedding space truly captures factor structure, unsupervised clustering should recover the factors without seeing any labels. We run KMeans with *k* equal to the number of factors and compare the resulting clusters (left) against the ground-truth labels (right).\n\n**Cluster purity** measures alignment: for each cluster, what fraction of its members share the same true factor? A purity of 1.0 means KMeans perfectly recovered the labeled structure from geometry alone — the factors are not just classifiable, they are the natural clusters in the space."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_factors = df[\"Factor\"].nunique()\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_factors, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# KMeans clusters in PCA space\n",
    "scatter = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap=\"tab10\", s=30, alpha=0.7)\n",
    "axes[0].set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)\")\n",
    "axes[0].set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)\")\n",
    "axes[0].set_title(f\"{MODEL.upper()} — KMeans ({n_factors} clusters)\")\n",
    "plt.colorbar(scatter, ax=axes[0], ticks=range(n_factors))\n",
    "\n",
    "# Ground-truth factors in PCA space (for comparison)\n",
    "scatter2 = axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=y_encoded, cmap=\"tab10\", s=30, alpha=0.7)\n",
    "axes[1].set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)\")\n",
    "axes[1].set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)\")\n",
    "axes[1].set_title(f\"{MODEL.upper()} — Ground-Truth Factors\")\n",
    "plt.colorbar(scatter2, ax=axes[1], ticks=range(n_factors))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cluster purity: how well do clusters align with factors?\n",
    "from collections import Counter\n",
    "purity_scores = []\n",
    "for c in range(n_factors):\n",
    "    mask = clusters == c\n",
    "    if mask.sum() > 0:\n",
    "        counts = Counter(y[mask])\n",
    "        majority = counts.most_common(1)[0][1]\n",
    "        purity_scores.append(majority / mask.sum())\n",
    "\n",
    "mean_purity = np.mean(purity_scores)\n",
    "print(f\"Mean cluster purity: {mean_purity:.1%} (1.0 = perfect alignment with factors)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Feature Importance — Which Embedding Dimensions Matter?\n\nA Random Forest can report how much each of the 1,536 embedding dimensions contributed to its decision splits. The distribution is typically very skewed: most dimensions contribute almost nothing, and a small handful do most of the work.\n\nThe number of dimensions needed to reach 90% cumulative importance tells you the **effective dimensionality** of the classification problem. If 50 out of 1,536 dimensions carry 90% of the weight, the factor structure lives in a low-dimensional subspace — and those 50 dimensions are the semantic axes that differentiate this model's constructs."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf.feature_importances_\n",
    "top_k = 30\n",
    "top_idx = np.argsort(importances)[-top_k:][::-1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.bar(range(top_k), importances[top_idx], color=\"steelblue\", alpha=0.8)\n",
    "ax.set_xticks(range(top_k))\n",
    "ax.set_xticklabels([f\"d{i}\" for i in top_idx], rotation=45, fontsize=8)\n",
    "ax.set_xlabel(\"Embedding Dimension\")\n",
    "ax.set_ylabel(\"Feature Importance\")\n",
    "ax.set_title(f\"{MODEL.upper()} — Top {top_k} Most Important Embedding Dimensions\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cumulative importance\n",
    "sorted_imp = np.sort(importances)[::-1]\n",
    "cum_imp = np.cumsum(sorted_imp)\n",
    "n_90 = np.searchsorted(cum_imp, 0.9) + 1\n",
    "print(f\"Top {n_90} of {len(importances)} dimensions capture 90% of importance\")\n",
    "print(f\"Top 30 dimensions capture {cum_imp[29]:.1%} of importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 10. Intra-Model Similarity — How Related Are the Factors?\n\nCompute the centroid (mean embedding vector) for each factor and measure pairwise cosine similarity. This reveals the model's internal geometry:\n\n- **High similarity** (cosine > 0.85) between two factors means they share substantial lexical content and may be hard to distinguish empirically. These pairs often correspond to known measurement overlap in the psychometric literature — constructs that researchers have debated splitting or merging.\n- **Low similarity** (cosine < 0.6) means the factors occupy genuinely different regions of the semantic space.\n\nThe most-similar and least-similar pairs printed below are a quick diagnostic: do they match your theoretical expectations for this model?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compute centroid for each factor\nfactors_sorted = sorted(df[\"Factor\"].unique())\ncentroids = np.array([X[y == f].mean(axis=0) for f in factors_sorted])\n\n# Cosine similarity between factor centroids\nnorms = np.linalg.norm(centroids, axis=1, keepdims=True)\ncentroids_norm = centroids / norms\nsim_matrix = centroids_norm @ centroids_norm.T\n\nsim_df = pd.DataFrame(sim_matrix, index=factors_sorted, columns=factors_sorted)\n\nfig, ax = plt.subplots(figsize=(max(8, len(factors_sorted) * 0.8), max(6, len(factors_sorted) * 0.6)))\nsns.heatmap(sim_df, annot=True, fmt=\".2f\", cmap=\"RdYlBu_r\", vmin=0.5, vmax=1.0,\n            ax=ax, square=True)\nax.set_title(f\"{MODEL.upper()} — Inter-Factor Cosine Similarity (centroids)\")\nplt.tight_layout()\nplt.show()\n\n# Most and least similar pairs (excluding self-comparisons)\nmask = np.ones_like(sim_matrix, dtype=bool)\nnp.fill_diagonal(mask, False)\noff_diag = sim_matrix.copy()\noff_diag[~mask] = np.nan\n\nmax_idx = np.unravel_index(np.nanargmax(off_diag), off_diag.shape)\nmin_idx = np.unravel_index(np.nanargmin(off_diag), off_diag.shape)\n\nprint(f\"Most similar pair:  {factors_sorted[max_idx[0]]} <-> {factors_sorted[max_idx[1]]} (cos = {sim_matrix[max_idx]:.3f})\")\nprint(f\"Least similar pair: {factors_sorted[min_idx[0]]} <-> {factors_sorted[min_idx[1]]} (cos = {sim_matrix[min_idx]:.3f})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 11. Cross-Model Search — Find Related Traits Across the Atlas\n\nEverything above stayed within a single model. Now we step outside.\n\nWith all 6,694 traits from all 44 models in a single FAISS index, you can query any factor from the selected model and find its nearest neighbors across the entire atlas. OCEAN Extraversion connects to HEXACO Social Self-Esteem and MBTI Extraversion. Dark Triad Machiavellianism links to MMPI Psychopathic Deviate and OCEAN low Agreeableness. These cross-tradition bridges emerge directly from the shared embedding geometry.\n\nChange `QUERY_FACTOR` below to search for a different factor. Results marked with `*` come from other models."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the full atlas FAISS index\n",
    "CATEGORIES = {\n",
    "    \"Trait-Based\": [\"ocean\", \"hex\", \"mbti\", \"epm\", \"sixteenpf\", \"ftm\"],\n",
    "    \"Narcissism-Based\": [\"npi\", \"pni\", \"hsns\", \"dtm\", \"dt4\", \"ffni\", \"ffni_sf\", \"narq\", \"mcmin\", \"ipn\"],\n",
    "    \"Motivational/Value\": [\"stbv\", \"sdt\", \"rft\", \"aam\", \"mst\", \"cs\"],\n",
    "    \"Cognitive/Learning\": [\"pct\", \"cest\", \"scm\", \"fsls\"],\n",
    "    \"Clinical/Health\": [\"mmpi\", \"scid\", \"bdi\", \"gad7\", \"wais\", \"tci\", \"mcmi\", \"tmp\", \"rit\", \"tat\"],\n",
    "    \"Interpersonal/Conflict\": [\"disc\", \"tki\"],\n",
    "    \"Application-Specific\": [\"riasec\", \"cmoa\", \"tei\", \"bt\", \"em\", \"papc\"]\n",
    "}\n",
    "slug_to_cat = {s: c for c, slugs in CATEGORIES.items() for s in slugs}\n",
    "\n",
    "all_slugs = sorted([f.replace(\".csv\", \"\") for f in os.listdir(\"atlas/datasets\") if f.endswith(\".csv\")])\n",
    "all_vecs, all_labels, all_cats, all_factors, all_adjs = [], [], [], [], []\n",
    "\n",
    "for slug in all_slugs:\n",
    "    d = pd.read_csv(f\"atlas/datasets/{slug}.csv\")\n",
    "    e = pd.read_csv(f\"atlas/Embeddings/{slug}_embeddings.csv\")\n",
    "    vecs = np.array([ast.literal_eval(v) for v in e[\"Embedding\"]])\n",
    "    all_vecs.append(vecs)\n",
    "    all_labels.extend([slug.upper()] * len(d))\n",
    "    all_cats.extend([slug_to_cat.get(slug, \"Unknown\")] * len(d))\n",
    "    all_factors.extend(d[\"Factor\"].values)\n",
    "    all_adjs.extend(d[\"Adjective\"].values)\n",
    "\n",
    "X_all = np.vstack(all_vecs)\n",
    "X_norm = (X_all / np.linalg.norm(X_all, axis=1, keepdims=True)).astype(np.float32)\n",
    "index = faiss.IndexFlatIP(X_norm.shape[1])\n",
    "index.add(X_norm)\n",
    "\n",
    "print(f\"FAISS index: {index.ntotal} vectors from {len(all_slugs)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search: pick a factor from the selected model\n",
    "# Change QUERY_FACTOR to search for a different factor\n",
    "QUERY_FACTOR = df[\"Factor\"].value_counts().index[0]  # default: most common factor\n",
    "\n",
    "query_mask = df[\"Factor\"] == QUERY_FACTOR\n",
    "query_idx = df[query_mask].index[0]\n",
    "q = X[query_idx].reshape(1, -1).astype(np.float32)\n",
    "q = q / np.linalg.norm(q)\n",
    "\n",
    "D, I = index.search(q, 25)\n",
    "\n",
    "query_trait = df.iloc[query_idx]\n",
    "print(f\"Query: {MODEL.upper()} / {QUERY_FACTOR} — \\\"{query_trait['Adjective']}\\\"\\n\")\n",
    "print(f\"{'Rank':<5} {'Model':<12} {'Factor':<30} {'Adjective':<20} {'Category':<22} {'Score':.5}\")\n",
    "print(\"-\" * 95)\n",
    "for rank, (i, score) in enumerate(zip(I[0], D[0]), 1):\n",
    "    marker = \" *\" if all_labels[i] != MODEL.upper() else \"\"\n",
    "    print(f\"{rank:<5} {all_labels[i]:<12} {all_factors[i]:<30} {all_adjs[i]:<20} {all_cats[i]:<22} {score:.4f}{marker}\")\n",
    "\n",
    "# Summary\n",
    "result_cats = set(all_cats[i] for i in I[0])\n",
    "result_models = set(all_labels[i] for i in I[0])\n",
    "cross_model = sum(1 for i in I[0] if all_labels[i] != MODEL.upper())\n",
    "print(f\"\\n{cross_model}/25 results from other models, spanning {len(result_cats)} categories and {len(result_models)} models\")\n",
    "print(\"(* = cross-model match)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 12. Accuracy Leaderboard — All 44 Models\n\nEach model's pre-trained Random Forest is evaluated on the model's own training lexicon (full dataset, no split). **100% is the expected baseline** — if a classifier trained on a model's own traits cannot separate its factors, either the embeddings failed to capture the semantic distinctions or the taxonomy assigned traits to the wrong factors.\n\nMost models hit or approach 100%. The ones that fall short are the scientifically interesting cases: they have many factors with overlapping vocabulary, and the 1,536-dim embedding space does not fully separate them. Clinical instruments (MMPI, SCID) and narcissism measures (FFNI with 15 facets) tend to cluster at the bottom because their constructs share substantial psychometric vocabulary — a known challenge in the assessment literature.\n\nThese lower-accuracy models are where the atlas's 3072-dim embedding upgrade (Section 14) and augmented lexicons add the most value. The leaderboard tells you which models benefit from richer representations and which are already solved at 1,536 dimensions."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for slug in all_slugs:\n",
    "    d = pd.read_csv(f\"atlas/datasets/{slug}.csv\")\n",
    "    m = joblib.load(f\"atlas/models/{slug}_rf_model.pkl\")\n",
    "    enc = joblib.load(f\"atlas/models/{slug}_label_encoder.pkl\")\n",
    "    e = pd.read_csv(f\"atlas/Embeddings/{slug}_embeddings.csv\")\n",
    "    Xm = np.array([ast.literal_eval(v) for v in e[\"Embedding\"]])\n",
    "    preds = enc.inverse_transform(m.predict(Xm))\n",
    "    acc = (preds == d[\"Factor\"].values).mean()\n",
    "    results.append({\n",
    "        \"Model\": slug.upper(), \"Category\": slug_to_cat.get(slug, \"Unknown\"),\n",
    "        \"Traits\": len(d), \"Factors\": d[\"Factor\"].nunique(), \"Accuracy\": acc\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"Accuracy\", ascending=True)\n",
    "\n",
    "# Highlight the selected model\n",
    "colors = [\"#ff6b6b\" if m == MODEL.upper() else \"#4ecdc4\" for m in results_df[\"Model\"]]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, max(8, len(results_df) * 0.25)))\n",
    "ax.barh(range(len(results_df)), results_df[\"Accuracy\"], color=colors, alpha=0.85)\n",
    "ax.set_yticks(range(len(results_df)))\n",
    "ax.set_yticklabels([f\"{m} ({f}F)\" for m, f in zip(results_df[\"Model\"], results_df[\"Factors\"])], fontsize=7)\n",
    "ax.set_xlabel(\"Accuracy\")\n",
    "ax.set_title(f\"Atlas Accuracy Leaderboard — {MODEL.upper()} highlighted in red\")\n",
    "ax.axvline(x=results_df[\"Accuracy\"].mean(), color=\"gray\", linestyle=\"--\", alpha=0.5, label=f\"Mean = {results_df['Accuracy'].mean():.1%}\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Stats\n",
    "selected = results_df[results_df[\"Model\"] == MODEL.upper()].iloc[0]\n",
    "rank = len(results_df) - results_df.index.get_loc(results_df[results_df[\"Model\"] == MODEL.upper()].index[0])\n",
    "print(f\"\\n{MODEL.upper()}: {selected['Accuracy']:.1%} accuracy — rank {rank}/{len(results_df)}\")\n",
    "print(f\"Atlas mean: {results_df['Accuracy'].mean():.1%}, median: {results_df['Accuracy'].median():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 13. Accuracy by Category\n\nDifferent research traditions pose different classification challenges. Narcissism-based models (10 instruments measuring closely related constructs) tend to be harder to separate than trait-based models (where Extraversion and Neuroticism occupy distinct semantic regions). The table below breaks down accuracy by the atlas's 7-category taxonomy."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_stats = pd.DataFrame(results).groupby(\"Category\").agg(\n",
    "    Models=(\"Model\", \"count\"),\n",
    "    Mean_Accuracy=(\"Accuracy\", \"mean\"),\n",
    "    Min_Accuracy=(\"Accuracy\", \"min\"),\n",
    "    Max_Accuracy=(\"Accuracy\", \"max\"),\n",
    "    Total_Traits=(\"Traits\", \"sum\")\n",
    ").sort_values(\"Mean_Accuracy\", ascending=False)\n",
    "\n",
    "cat_stats[\"Mean_Accuracy\"] = cat_stats[\"Mean_Accuracy\"].apply(lambda x: f\"{x:.1%}\")\n",
    "cat_stats[\"Min_Accuracy\"] = cat_stats[\"Min_Accuracy\"].apply(lambda x: f\"{x:.1%}\")\n",
    "cat_stats[\"Max_Accuracy\"] = cat_stats[\"Max_Accuracy\"].apply(lambda x: f\"{x:.1%}\")\n",
    "cat_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 13b. Novel Item Evaluation\n\nSections 4-6 confirmed that each model's classifier separates its own training lexicon — most at or near 100%. The leaderboard in Section 12 showed which models are already solved and which have room to improve. But those results all test on data the classifier was built from.\n\nThe harder question: **can it classify items it has never seen?**\n\nBelow we evaluate on novel items generated independently by GPT-4o from factor definitions alone, without access to the training lexicon. These are the same 5,052 items used in the paper's Experiment 1 (pre-embedded in the repository — no API key needed).\n\nFor OCEAN, expect accuracy to drop from 100% (Section 4) to roughly 70-80%. That gap is the **generalization penalty** — the cost of moving from memorized lexical entries to genuinely new phrasings of the same constructs. The size of that gap across all 44 models is the central finding of the paper's validation experiments.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\n\n# Load pre-computed test items (generated by GPT-4o, embedded offline)\ntest_items = json.load(open(\"atlas/data/test_items/test_items.json\"))\ntest_emb = np.load(\"atlas/data/test_items/test_items_embeddings.npz\")[\"embeddings\"]\n\n# Filter to the selected model\nmodel_idx = [i for i, item in enumerate(test_items) if item[\"slug\"] == MODEL]\n\nif not model_idx:\n    print(f\"No test items available for {MODEL.upper()}\")\nelse:\n    X_novel = test_emb[model_idx]\n    y_novel_true = [test_items[i][\"expected_factor\"] for i in model_idx]\n\n    # Classify using the pre-trained RF from Section 2\n    y_novel_pred = le.inverse_transform(clf.predict(X_novel))\n\n    acc_novel = accuracy_score(y_novel_true, y_novel_pred)\n    print(f\"{MODEL.upper()}: {acc_novel:.1%} accuracy on {len(model_idx)} novel test items\")\n    print(f\"(vs. {acc:.1%} on 80/20 train/test split in Section 4)\\n\")\n    print(classification_report(y_novel_true, y_novel_pred, zero_division=0))\n\n    # Confusion matrix on novel items\n    cm_novel = confusion_matrix(y_novel_true, y_novel_pred, labels=le.classes_)\n    cm_novel_df = pd.DataFrame(cm_novel, index=le.classes_, columns=le.classes_)\n\n    fig, ax = plt.subplots(figsize=(max(8, len(le.classes_) * 0.8), max(6, len(le.classes_) * 0.6)))\n    sns.heatmap(cm_novel_df, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax)\n    ax.set_xlabel(\"Predicted\")\n    ax.set_ylabel(\"Actual (GPT-4o generated)\")\n    ax.set_title(f\"{MODEL.upper()} — Novel Item Confusion Matrix ({len(model_idx)} items, {acc_novel:.1%})\")\n    plt.tight_layout()\n    plt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 14. 3072-dim Embedding Upgrade\n\nOpenAI's `text-embedding-3-large` produces 3,072-dim vectors — twice the resolution. Does the extra dimensionality help separate factors that were borderline in the 1,536-dim space?\n\nThe upgraded embeddings and retrained classifiers are hosted on [Hugging Face Hub](https://huggingface.co/datasets/Wildertrek/personality-atlas-3072). The cells below download the selected model's 3072-dim assets and compare both accuracy and embedding geometry side by side.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "!pip install -q huggingface_hub\nimport os\nos.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"0\"\nos.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", message=\".*HF_TOKEN.*\")\nwarnings.filterwarnings(\"ignore\", message=\".*unauthenticated.*\")\n\nfrom huggingface_hub import hf_hub_download\n\nHF_REPO = \"Wildertrek/personality-atlas-3072\"\n\n# Download 3072-dim assets for the selected model (public dataset — no token needed)\nemb3072_path = hf_hub_download(HF_REPO, f\"Embeddings_3072/{MODEL}_embeddings.csv\", repo_type=\"dataset\")\nmodel3072_path = hf_hub_download(HF_REPO, f\"models_3072/{MODEL}_rf_model.pkl\", repo_type=\"dataset\")\nenc3072_path = hf_hub_download(HF_REPO, f\"models_3072/{MODEL}_label_encoder.pkl\", repo_type=\"dataset\")\n\nemb3072_df = pd.read_csv(emb3072_path)\nX3072 = np.array([ast.literal_eval(e) for e in emb3072_df[\"Embedding\"]])\nclf3072 = joblib.load(model3072_path)\nle3072 = joblib.load(enc3072_path)\n\n# Full-dataset accuracy comparison\ndf_full = pd.read_csv(f\"atlas/datasets/{MODEL}.csv\")\ny_true = df_full[\"Factor\"].values\n\npreds_1536 = le.inverse_transform(clf.predict(X))\npreds_3072 = le3072.inverse_transform(clf3072.predict(X3072))\n\nacc_1536 = (preds_1536 == y_true).mean()\nacc_3072 = (preds_3072 == y_true).mean()\n\nprint(f\"{MODEL.upper()} accuracy comparison:\")\nprint(f\"  1536-dim (text-embedding-3-small): {acc_1536:.1%}\")\nprint(f\"  3072-dim (text-embedding-3-large): {acc_3072:.1%}\")\nprint(f\"  Delta: {(acc_3072 - acc_1536)*100:+.1f} percentage points\")\nprint(f\"  Embedding dimensions: {X.shape[1]} → {X3072.shape[1]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# PCA comparison: 1536 vs 3072 embedding spaces\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\nfor ax, (Xd, dim_label) in zip(axes, [(X, \"1536-dim\"), (X3072, \"3072-dim\")]):\n    pca_d = PCA(n_components=2)\n    Xp = pca_d.fit_transform(Xd)\n    colors_d = sns.color_palette(\"husl\", df_full[\"Factor\"].nunique())\n    for i, factor in enumerate(sorted(df_full[\"Factor\"].unique())):\n        mask = y_true == factor\n        ax.scatter(Xp[mask, 0], Xp[mask, 1], c=[colors_d[i]], s=20, alpha=0.6, label=factor)\n    ax.set_xlabel(f\"PC1 ({pca_d.explained_variance_ratio_[0]*100:.1f}%)\")\n    ax.set_ylabel(f\"PC2 ({pca_d.explained_variance_ratio_[1]*100:.1f}%)\")\n    ax.set_title(f\"{MODEL.upper()} — {dim_label}\")\n    if df_full[\"Factor\"].nunique() <= 12:\n        ax.legend(fontsize=6, loc=\"best\")\n\nplt.suptitle(f\"Embedding Space Comparison: 1536-dim vs 3072-dim\", fontsize=13, y=1.02)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Does the upgrade help on novel items?\n\nTraining-data accuracy is often high for both embedding sizes — the real test is generalization. Below we run both classifiers on the novel GPT-4o test items from Section 13b using pre-computed 3072-dim embeddings from HuggingFace.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Novel item evaluation: 1536-dim vs 3072-dim on truly novel test items\n# Downloads 3072-dim test item embeddings from HuggingFace (no API key needed)\ntest3072_path = hf_hub_download(HF_REPO, \"test_items/test_items_embeddings_3072.npz\", repo_type=\"dataset\")\ntest_emb_3072 = np.load(test3072_path)[\"embeddings\"]\n\nmodel_idx = [i for i, item in enumerate(test_items) if item[\"slug\"] == MODEL]\n\nif model_idx:\n    X_novel_1536 = test_emb[model_idx]\n    X_novel_3072 = test_emb_3072[model_idx]\n    y_novel_true = [test_items[i][\"expected_factor\"] for i in model_idx]\n\n    pred_1536 = le.inverse_transform(clf.predict(X_novel_1536))\n    pred_3072 = le3072.inverse_transform(clf3072.predict(X_novel_3072))\n\n    acc_n1536 = accuracy_score(y_novel_true, pred_1536)\n    acc_n3072 = accuracy_score(y_novel_true, pred_3072)\n    delta = (acc_n3072 - acc_n1536) * 100\n\n    print(f\"{MODEL.upper()} — Novel item accuracy ({len(model_idx)} items):\")\n    print(f\"  1536-dim: {acc_n1536:.1%}\")\n    print(f\"  3072-dim: {acc_n3072:.1%}\")\n    print(f\"  Delta:    {delta:+.1f} percentage points\")\n    print(f\"\\n(Compare with training-data accuracy: 1536={acc_1536:.1%}, 3072={acc_3072:.1%})\")\nelse:\n    print(f\"No test items for {MODEL.upper()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\nEvery analysis in this notebook adapts to any of the 44 models — change `MODEL` in Section 2 and re-run. For the atlas-wide view (cross-model PCA, DSM-5 clinical alignment, full-atlas FAISS search), see the [Quick Start notebook](https://colab.research.google.com/github/Wildertrek/survey/blob/main/notebooks/atlas_quick_start.ipynb).\n\n**Repository:** [github.com/Wildertrek/survey](https://github.com/Wildertrek/survey) | **3072-dim assets:** [Hugging Face Hub](https://huggingface.co/datasets/Wildertrek/personality-atlas-3072)\n**Paper:** Raetano, J., Gregor, J., & Tamang, S. (2026). *A Survey and Computational Atlas of Personality Models.* ACM TIST.\n**License:** MIT"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}