{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personality Atlas — Deep Dive\n",
    "\n",
    "**Explore any of the 44 personality models in depth.** Train classifiers, visualize embeddings, and run cross-model analysis.\n",
    "\n",
    "> Raetano, J., Gregor, J., & Tamang, S. (2026). *A Survey and Computational Atlas of Personality Models.* ACM TIST. Under review.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Wildertrek/survey/blob/main/notebooks/atlas_deep_dive.ipynb)\n",
    "\n",
    "**No API keys required.** All 44 models have pre-computed embeddings and trained classifiers in the repository.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"atlas\"):\n",
    "    !git clone --depth 1 https://github.com/Wildertrek/survey.git atlas\n",
    "else:\n",
    "    print(\"Atlas already cloned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q faiss-cpu\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")\n",
    "\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import faiss\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "print(\"All libraries loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Select a Model\n",
    "\n",
    "Change `SLUG` below to explore any of the 44 models.\n",
    "\n",
    "| Category | Models (slug) |\n",
    "|----------|---------------|\n",
    "| **Trait-Based** | `ocean` (Big Five), `hex` (HEXACO), `mbti`, `epm` (Eysenck), `sixteenpf`, `ftm` (Four Temperaments) |\n",
    "| **Narcissism-Based** | `npi`, `pni`, `ffni`, `ffni_sf`, `narq`, `hsns`, `dtm` (Dark Triad), `dt4` (Dark Tetrad), `mcmin`, `ipn` |\n",
    "| **Motivational/Value** | `stbv` (Schwartz Values), `sdt` (Self-Determination), `rft`, `aam`, `mst`, `cs` (Clifton) |\n",
    "| **Cognitive/Learning** | `pct`, `scm`, `cest`, `fsls` (Felder-Silverman) |\n",
    "| **Clinical/Health** | `mmpi`, `scid` (DSM), `bdi` (Depression), `gad7` (Anxiety), `wais`, `tci`, `mcmi`, `tmp`, `rit` (Rorschach), `tat` |\n",
    "| **Interpersonal/Conflict** | `disc`, `tki` (Thomas-Kilmann) |\n",
    "| **Application-Specific** | `riasec` (Holland Careers), `cmoa`, `tei`, `bt` (Bartle Types), `em` (Enneagram), `papc` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CHANGE THIS to explore a different model\n",
    "SLUG = \"tci\"\n",
    "# ============================================\n",
    "\n",
    "# Load dataset, embeddings, pre-trained model, and label encoder\n",
    "df = pd.read_csv(f\"atlas/datasets/{SLUG}.csv\")\n",
    "emb_df = pd.read_csv(f\"atlas/Embeddings/{SLUG}_embeddings.csv\")\n",
    "clf = joblib.load(f\"atlas/models/{SLUG}_rf_model.pkl\")\n",
    "le = joblib.load(f\"atlas/models/{SLUG}_label_encoder.pkl\")\n",
    "\n",
    "# Parse embeddings from CSV strings to numpy array\n",
    "X = np.array([ast.literal_eval(e) for e in emb_df[\"Embedding\"]])\n",
    "y = df[\"Factor\"].values\n",
    "y_encoded = le.transform(y)\n",
    "\n",
    "print(f\"Model: {SLUG.upper()}\")\n",
    "print(f\"Traits: {len(df)}, Factors: {df['Factor'].nunique()}, Embedding dim: {X.shape[1]}\")\n",
    "print(f\"Factors: {sorted(df['Factor'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factor distribution\n",
    "factor_counts = df[\"Factor\"].value_counts().sort_index()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart\n",
    "factor_counts.plot(kind=\"barh\", ax=axes[0], color=sns.color_palette(\"husl\", len(factor_counts)))\n",
    "axes[0].set_xlabel(\"Trait Count\")\n",
    "axes[0].set_title(f\"{SLUG.upper()} — Factor Distribution ({len(df)} traits)\")\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(factor_counts, labels=factor_counts.index, autopct=\"%1.0f%%\",\n",
    "            colors=sns.color_palette(\"husl\", len(factor_counts)))\n",
    "axes[1].set_title(f\"{SLUG.upper()} — Factor Proportions\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexical schema — sample traits per factor\n",
    "print(f\"{SLUG.upper()}: {len(df)} traits across {df['Factor'].nunique()} factors\\n\")\n",
    "\n",
    "for factor, group in df.groupby(\"Factor\"):\n",
    "    sample = group[\"Adjective\"].unique()[:6]\n",
    "    print(f\"  {factor} ({len(group)} traits): {', '.join(sample)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train a Random Forest Classifier\n",
    "\n",
    "Using the pre-computed 1536-dim embeddings from `text-embedding-3-small`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80/20 stratified split — same parameters as original notebooks\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Train fresh RF (same hyperparameters as the pre-trained models)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test accuracy: {acc:.1%} ({sum(y_pred == y_test)}/{len(y_test)} correct)\\n\")\n",
    "\n",
    "# Compare with pre-trained model from repo\n",
    "y_pred_pretrained = clf.predict(X_test)\n",
    "acc_pretrained = accuracy_score(y_test, y_pred_pretrained)\n",
    "print(f\"Pre-trained model accuracy: {acc_pretrained:.1%}\")\n",
    "print(f\"Freshly trained accuracy:   {acc:.1%}\")\n",
    "if abs(acc - acc_pretrained) < 0.001:\n",
    "    print(\"Identical — reproduced successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full classification report\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_df = pd.DataFrame(cm, index=le.classes_, columns=le.classes_)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Raw counts\n",
    "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[0])\n",
    "axes[0].set_xlabel(\"Predicted\")\n",
    "axes[0].set_ylabel(\"Actual\")\n",
    "axes[0].set_title(f\"{SLUG.upper()} — Confusion Matrix (counts)\")\n",
    "\n",
    "# Normalized (percentages per row)\n",
    "cm_norm = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n",
    "cm_norm_df = pd.DataFrame(cm_norm, index=le.classes_, columns=le.classes_)\n",
    "sns.heatmap(cm_norm_df, annot=True, fmt=\".0%\", cmap=\"Blues\", ax=axes[1])\n",
    "axes[1].set_xlabel(\"Predicted\")\n",
    "axes[1].set_ylabel(\"Actual\")\n",
    "axes[1].set_title(f\"{SLUG.upper()} — Confusion Matrix (row-normalized)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_val_score(\n",
    "    RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    X, y_encoded, cv=5, scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "print(f\"5-Fold Cross-Validation for {SLUG.upper()}:\")\n",
    "print(f\"  Fold scores: {', '.join(f'{s:.1%}' for s in cv_scores)}\")\n",
    "print(f\"  Mean: {cv_scores.mean():.1%} (+/- {cv_scores.std() * 2:.1%})\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.bar(range(1, 6), cv_scores, color=sns.color_palette(\"husl\", 5), alpha=0.8)\n",
    "ax.axhline(y=cv_scores.mean(), color=\"red\", linestyle=\"--\", label=f\"Mean = {cv_scores.mean():.1%}\")\n",
    "ax.set_xlabel(\"Fold\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(f\"{SLUG.upper()} — 5-Fold Cross-Validation\")\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. PCA — Embedding Space Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10)\n",
    "X_pca = pca.fit_transform(X)\n",
    "cumvar = np.cumsum(pca.explained_variance_ratio_) * 100\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Scree plot\n",
    "axes[0].bar(range(1, 11), pca.explained_variance_ratio_ * 100, alpha=0.6, label=\"Individual\")\n",
    "axes[0].plot(range(1, 11), cumvar, \"r-o\", markersize=4, label=\"Cumulative\")\n",
    "axes[0].set_xlabel(\"PC\")\n",
    "axes[0].set_ylabel(\"Variance Explained (%)\")\n",
    "axes[0].set_title(f\"{SLUG.upper()} — Scree Plot\")\n",
    "axes[0].legend(fontsize=8)\n",
    "\n",
    "# PC1 vs PC2 colored by factor\n",
    "colors = sns.color_palette(\"husl\", df[\"Factor\"].nunique())\n",
    "for i, factor in enumerate(sorted(df[\"Factor\"].unique())):\n",
    "    mask = y == factor\n",
    "    axes[1].scatter(X_pca[mask, 0], X_pca[mask, 1], c=[colors[i]], s=30, alpha=0.7, label=factor)\n",
    "axes[1].set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)\")\n",
    "axes[1].set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)\")\n",
    "axes[1].set_title(f\"{SLUG.upper()} — PC1 vs PC2\")\n",
    "axes[1].legend(fontsize=7, loc=\"best\", ncol=max(1, df['Factor'].nunique() // 8 + 1))\n",
    "\n",
    "# PC1 vs PC3\n",
    "for i, factor in enumerate(sorted(df[\"Factor\"].unique())):\n",
    "    mask = y == factor\n",
    "    axes[2].scatter(X_pca[mask, 0], X_pca[mask, 2], c=[colors[i]], s=30, alpha=0.7, label=factor)\n",
    "axes[2].set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)\")\n",
    "axes[2].set_ylabel(f\"PC3 ({pca.explained_variance_ratio_[2]*100:.1f}%)\")\n",
    "axes[2].set_title(f\"{SLUG.upper()} — PC1 vs PC3\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFirst 10 PCs capture {cumvar[-1]:.1f}% of variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_factors = df[\"Factor\"].nunique()\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_factors, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# KMeans clusters in PCA space\n",
    "scatter = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap=\"tab10\", s=30, alpha=0.7)\n",
    "axes[0].set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)\")\n",
    "axes[0].set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)\")\n",
    "axes[0].set_title(f\"{SLUG.upper()} — KMeans ({n_factors} clusters)\")\n",
    "plt.colorbar(scatter, ax=axes[0], ticks=range(n_factors))\n",
    "\n",
    "# Ground-truth factors in PCA space (for comparison)\n",
    "scatter2 = axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=y_encoded, cmap=\"tab10\", s=30, alpha=0.7)\n",
    "axes[1].set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)\")\n",
    "axes[1].set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)\")\n",
    "axes[1].set_title(f\"{SLUG.upper()} — Ground-Truth Factors\")\n",
    "plt.colorbar(scatter2, ax=axes[1], ticks=range(n_factors))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cluster purity: how well do clusters align with factors?\n",
    "from collections import Counter\n",
    "purity_scores = []\n",
    "for c in range(n_factors):\n",
    "    mask = clusters == c\n",
    "    if mask.sum() > 0:\n",
    "        counts = Counter(y[mask])\n",
    "        majority = counts.most_common(1)[0][1]\n",
    "        purity_scores.append(majority / mask.sum())\n",
    "\n",
    "mean_purity = np.mean(purity_scores)\n",
    "print(f\"Mean cluster purity: {mean_purity:.1%} (1.0 = perfect alignment with factors)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Importance — Top Embedding Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf.feature_importances_\n",
    "top_k = 30\n",
    "top_idx = np.argsort(importances)[-top_k:][::-1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.bar(range(top_k), importances[top_idx], color=\"steelblue\", alpha=0.8)\n",
    "ax.set_xticks(range(top_k))\n",
    "ax.set_xticklabels([f\"d{i}\" for i in top_idx], rotation=45, fontsize=8)\n",
    "ax.set_xlabel(\"Embedding Dimension\")\n",
    "ax.set_ylabel(\"Feature Importance\")\n",
    "ax.set_title(f\"{SLUG.upper()} — Top {top_k} Most Important Embedding Dimensions\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cumulative importance\n",
    "sorted_imp = np.sort(importances)[::-1]\n",
    "cum_imp = np.cumsum(sorted_imp)\n",
    "n_90 = np.searchsorted(cum_imp, 0.9) + 1\n",
    "print(f\"Top {n_90} of {len(importances)} dimensions capture 90% of importance\")\n",
    "print(f\"Top 30 dimensions capture {cum_imp[29]:.1%} of importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Intra-Model Similarity — How Similar Are the Factors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute centroid for each factor\n",
    "factors_sorted = sorted(df[\"Factor\"].unique())\n",
    "centroids = np.array([X[y == f].mean(axis=0) for f in factors_sorted])\n",
    "\n",
    "# Cosine similarity between factor centroids\n",
    "norms = np.linalg.norm(centroids, axis=1, keepdims=True)\n",
    "centroids_norm = centroids / norms\n",
    "sim_matrix = centroids_norm @ centroids_norm.T\n",
    "\n",
    "sim_df = pd.DataFrame(sim_matrix, index=factors_sorted, columns=factors_sorted)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(max(8, len(factors_sorted) * 0.8), max(6, len(factors_sorted) * 0.6)))\n",
    "sns.heatmap(sim_df, annot=True, fmt=\".2f\", cmap=\"RdYlBu_r\", vmin=0.5, vmax=1.0,\n",
    "            ax=ax, square=True)\n",
    "ax.set_title(f\"{SLUG.upper()} — Inter-Factor Cosine Similarity (centroids)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Most and least similar pairs\n",
    "np.fill_diagonal(sim_matrix, 0)\n",
    "max_idx = np.unravel_index(sim_matrix.argmax(), sim_matrix.shape)\n",
    "min_idx = np.unravel_index(sim_matrix.argmin(), sim_matrix.shape)\n",
    "np.fill_diagonal(sim_matrix, 1)\n",
    "\n",
    "print(f\"Most similar pair:  {factors_sorted[max_idx[0]]} <-> {factors_sorted[max_idx[1]]} (cos = {sim_matrix[max_idx]:.3f})\")\n",
    "print(f\"Least similar pair: {factors_sorted[min_idx[0]]} <-> {factors_sorted[min_idx[1]]} (cos = {sim_matrix[min_idx]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Cross-Model Search — Find Related Traits Across All 44 Models\n",
    "\n",
    "This is the atlas's core value: a single FAISS index over 6,694 traits from 44 models enables cross-tradition trait retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the full atlas FAISS index\n",
    "CATEGORIES = {\n",
    "    \"Trait-Based\": [\"ocean\", \"hex\", \"mbti\", \"epm\", \"sixteenpf\", \"ftm\"],\n",
    "    \"Narcissism-Based\": [\"npi\", \"pni\", \"hsns\", \"dtm\", \"dt4\", \"ffni\", \"ffni_sf\", \"narq\", \"mcmin\", \"ipn\"],\n",
    "    \"Motivational/Value\": [\"stbv\", \"sdt\", \"rft\", \"aam\", \"mst\", \"cs\"],\n",
    "    \"Cognitive/Learning\": [\"pct\", \"cest\", \"scm\", \"fsls\"],\n",
    "    \"Clinical/Health\": [\"mmpi\", \"scid\", \"bdi\", \"gad7\", \"wais\", \"tci\", \"mcmi\", \"tmp\", \"rit\", \"tat\"],\n",
    "    \"Interpersonal/Conflict\": [\"disc\", \"tki\"],\n",
    "    \"Application-Specific\": [\"riasec\", \"cmoa\", \"tei\", \"bt\", \"em\", \"papc\"]\n",
    "}\n",
    "slug_to_cat = {s: c for c, slugs in CATEGORIES.items() for s in slugs}\n",
    "\n",
    "all_slugs = sorted([f.replace(\".csv\", \"\") for f in os.listdir(\"atlas/datasets\") if f.endswith(\".csv\")])\n",
    "all_vecs, all_labels, all_cats, all_factors, all_adjs = [], [], [], [], []\n",
    "\n",
    "for slug in all_slugs:\n",
    "    d = pd.read_csv(f\"atlas/datasets/{slug}.csv\")\n",
    "    e = pd.read_csv(f\"atlas/Embeddings/{slug}_embeddings.csv\")\n",
    "    vecs = np.array([ast.literal_eval(v) for v in e[\"Embedding\"]])\n",
    "    all_vecs.append(vecs)\n",
    "    all_labels.extend([slug.upper()] * len(d))\n",
    "    all_cats.extend([slug_to_cat.get(slug, \"Unknown\")] * len(d))\n",
    "    all_factors.extend(d[\"Factor\"].values)\n",
    "    all_adjs.extend(d[\"Adjective\"].values)\n",
    "\n",
    "X_all = np.vstack(all_vecs)\n",
    "X_norm = (X_all / np.linalg.norm(X_all, axis=1, keepdims=True)).astype(np.float32)\n",
    "index = faiss.IndexFlatIP(X_norm.shape[1])\n",
    "index.add(X_norm)\n",
    "\n",
    "print(f\"FAISS index: {index.ntotal} vectors from {len(all_slugs)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search: pick a factor from the selected model\n",
    "# Change QUERY_FACTOR to search for a different factor\n",
    "QUERY_FACTOR = df[\"Factor\"].value_counts().index[0]  # default: most common factor\n",
    "\n",
    "query_mask = df[\"Factor\"] == QUERY_FACTOR\n",
    "query_idx = df[query_mask].index[0]\n",
    "q = X[query_idx].reshape(1, -1).astype(np.float32)\n",
    "q = q / np.linalg.norm(q)\n",
    "\n",
    "D, I = index.search(q, 25)\n",
    "\n",
    "query_trait = df.iloc[query_idx]\n",
    "print(f\"Query: {SLUG.upper()} / {QUERY_FACTOR} — \\\"{query_trait['Adjective']}\\\"\\n\")\n",
    "print(f\"{'Rank':<5} {'Model':<12} {'Factor':<30} {'Adjective':<20} {'Category':<22} {'Score':.5}\")\n",
    "print(\"-\" * 95)\n",
    "for rank, (i, score) in enumerate(zip(I[0], D[0]), 1):\n",
    "    marker = \" *\" if all_labels[i] != SLUG.upper() else \"\"\n",
    "    print(f\"{rank:<5} {all_labels[i]:<12} {all_factors[i]:<30} {all_adjs[i]:<20} {all_cats[i]:<22} {score:.4f}{marker}\")\n",
    "\n",
    "# Summary\n",
    "result_cats = set(all_cats[i] for i in I[0])\n",
    "result_models = set(all_labels[i] for i in I[0])\n",
    "cross_model = sum(1 for i in I[0] if all_labels[i] != SLUG.upper())\n",
    "print(f\"\\n{cross_model}/25 results from other models, spanning {len(result_cats)} categories and {len(result_models)} models\")\n",
    "print(\"(* = cross-model match)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Compare All 44 Models — Accuracy Leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for slug in all_slugs:\n",
    "    d = pd.read_csv(f\"atlas/datasets/{slug}.csv\")\n",
    "    m = joblib.load(f\"atlas/models/{slug}_rf_model.pkl\")\n",
    "    enc = joblib.load(f\"atlas/models/{slug}_label_encoder.pkl\")\n",
    "    e = pd.read_csv(f\"atlas/Embeddings/{slug}_embeddings.csv\")\n",
    "    Xm = np.array([ast.literal_eval(v) for v in e[\"Embedding\"]])\n",
    "    preds = enc.inverse_transform(m.predict(Xm))\n",
    "    acc = (preds == d[\"Factor\"].values).mean()\n",
    "    results.append({\n",
    "        \"Model\": slug.upper(), \"Category\": slug_to_cat.get(slug, \"Unknown\"),\n",
    "        \"Traits\": len(d), \"Factors\": d[\"Factor\"].nunique(), \"Accuracy\": acc\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"Accuracy\", ascending=True)\n",
    "\n",
    "# Highlight the selected model\n",
    "colors = [\"#ff6b6b\" if m == SLUG.upper() else \"#4ecdc4\" for m in results_df[\"Model\"]]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, max(8, len(results_df) * 0.25)))\n",
    "ax.barh(range(len(results_df)), results_df[\"Accuracy\"], color=colors, alpha=0.85)\n",
    "ax.set_yticks(range(len(results_df)))\n",
    "ax.set_yticklabels([f\"{m} ({f}F)\" for m, f in zip(results_df[\"Model\"], results_df[\"Factors\"])], fontsize=7)\n",
    "ax.set_xlabel(\"Accuracy\")\n",
    "ax.set_title(f\"Atlas Accuracy Leaderboard — {SLUG.upper()} highlighted in red\")\n",
    "ax.axvline(x=results_df[\"Accuracy\"].mean(), color=\"gray\", linestyle=\"--\", alpha=0.5, label=f\"Mean = {results_df['Accuracy'].mean():.1%}\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Stats\n",
    "selected = results_df[results_df[\"Model\"] == SLUG.upper()].iloc[0]\n",
    "rank = len(results_df) - results_df.index.get_loc(results_df[results_df[\"Model\"] == SLUG.upper()].index[0])\n",
    "print(f\"\\n{SLUG.upper()}: {selected['Accuracy']:.1%} accuracy — rank {rank}/{len(results_df)}\")\n",
    "print(f\"Atlas mean: {results_df['Accuracy'].mean():.1%}, median: {results_df['Accuracy'].median():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Accuracy by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_stats = pd.DataFrame(results).groupby(\"Category\").agg(\n",
    "    Models=(\"Model\", \"count\"),\n",
    "    Mean_Accuracy=(\"Accuracy\", \"mean\"),\n",
    "    Min_Accuracy=(\"Accuracy\", \"min\"),\n",
    "    Max_Accuracy=(\"Accuracy\", \"max\"),\n",
    "    Total_Traits=(\"Traits\", \"sum\")\n",
    ").sort_values(\"Mean_Accuracy\", ascending=False)\n",
    "\n",
    "cat_stats[\"Mean_Accuracy\"] = cat_stats[\"Mean_Accuracy\"].apply(lambda x: f\"{x:.1%}\")\n",
    "cat_stats[\"Min_Accuracy\"] = cat_stats[\"Min_Accuracy\"].apply(lambda x: f\"{x:.1%}\")\n",
    "cat_stats[\"Max_Accuracy\"] = cat_stats[\"Max_Accuracy\"].apply(lambda x: f\"{x:.1%}\")\n",
    "cat_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 14. 3072-dim Embedding Upgrade (Experiment 2)\n\nCompare the selected model's accuracy using 1536-dim vs 3072-dim embeddings. The 3072-dim assets are downloaded from [Hugging Face Hub](https://huggingface.co/datasets/Wildertrek/personality-atlas-3072).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "!pip install -q huggingface_hub\nfrom huggingface_hub import hf_hub_download\n\nHF_REPO = \"Wildertrek/personality-atlas-3072\"\n\n# Download 3072-dim assets for the selected model\nemb3072_path = hf_hub_download(HF_REPO, f\"Embeddings_3072/{SLUG}_embeddings.csv\", repo_type=\"dataset\")\nmodel3072_path = hf_hub_download(HF_REPO, f\"models_3072/{SLUG}_rf_model.pkl\", repo_type=\"dataset\")\nenc3072_path = hf_hub_download(HF_REPO, f\"models_3072/{SLUG}_label_encoder.pkl\", repo_type=\"dataset\")\n\nemb3072_df = pd.read_csv(emb3072_path)\nX3072 = np.array([ast.literal_eval(e) for e in emb3072_df[\"Embedding\"]])\nclf3072 = joblib.load(model3072_path)\nle3072 = joblib.load(enc3072_path)\n\n# Full-dataset accuracy comparison\ndf_full = pd.read_csv(f\"atlas/datasets/{SLUG}.csv\")\ny_true = df_full[\"Factor\"].values\n\npreds_1536 = le.inverse_transform(clf.predict(X))\npreds_3072 = le3072.inverse_transform(clf3072.predict(X3072))\n\nacc_1536 = (preds_1536 == y_true).mean()\nacc_3072 = (preds_3072 == y_true).mean()\n\nprint(f\"{SLUG.upper()} accuracy comparison:\")\nprint(f\"  1536-dim (text-embedding-3-small): {acc_1536:.1%}\")\nprint(f\"  3072-dim (text-embedding-3-large): {acc_3072:.1%}\")\nprint(f\"  Delta: {(acc_3072 - acc_1536)*100:+.1f} percentage points\")\nprint(f\"  Embedding dimensions: {X.shape[1]} → {X3072.shape[1]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# PCA comparison: 1536 vs 3072 embedding spaces\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\nfor ax, (Xd, dim_label) in zip(axes, [(X, \"1536-dim\"), (X3072, \"3072-dim\")]):\n    pca_d = PCA(n_components=2)\n    Xp = pca_d.fit_transform(Xd)\n    colors_d = sns.color_palette(\"husl\", df_full[\"Factor\"].nunique())\n    for i, factor in enumerate(sorted(df_full[\"Factor\"].unique())):\n        mask = y_true == factor\n        ax.scatter(Xp[mask, 0], Xp[mask, 1], c=[colors_d[i]], s=20, alpha=0.6, label=factor)\n    ax.set_xlabel(f\"PC1 ({pca_d.explained_variance_ratio_[0]*100:.1f}%)\")\n    ax.set_ylabel(f\"PC2 ({pca_d.explained_variance_ratio_[1]*100:.1f}%)\")\n    ax.set_title(f\"{SLUG.upper()} — {dim_label}\")\n    if df_full[\"Factor\"].nunique() <= 12:\n        ax.legend(fontsize=6, loc=\"best\")\n\nplt.suptitle(f\"Embedding Space Comparison: 1536-dim vs 3072-dim\", fontsize=13, y=1.02)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n**Repository:** [github.com/Wildertrek/survey](https://github.com/Wildertrek/survey) | **3072-dim assets:** [Hugging Face Hub](https://huggingface.co/datasets/Wildertrek/personality-atlas-3072)  \n**Paper:** Raetano, J., Gregor, J., & Tamang, S. (2026). *A Survey and Computational Atlas of Personality Models.* ACM TIST.  \n**License:** MIT"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}